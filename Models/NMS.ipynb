{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59560fcf-b3af-4fdd-8092-b6f97cad0450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Vstanovlene\\Anaconda Distribution\\envs\\new_gpu_env\\lib\\site-packages\\albumentations\\__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.23). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../\")\n",
    "sys.path.insert(1, \"../Models/\")\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from detection_datasets import *\n",
    "from yolov2 import YOLOv2D19 as YOLOv2\n",
    "from data_preprocessing import get_norms\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations as A\n",
    "from utils import IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfad3ded-33bd-4e5c-a8e1-49779d2f3786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 0.00 MB\n",
      "Allocated memory: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "import gc\n",
    "\n",
    "# Invoke garbage collector\n",
    "gc.collect()\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03223b6b-1fb0-46c2-943c-592ac7af81ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../Models/anchors_VOC0712trainval.pickle', 'rb') as handle:\n",
    "    anchors = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "519b7feb-5c0c-497f-a8e3-0e175720d9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')\n",
    "dtype=torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "915a6a28-2bdd-47fd-8ab1-fe2f0ff6a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = get_norms('../../datasets/VOCdevkit/trainval_norms.json')\n",
    "means = norms['means']\n",
    "stds = norms['stds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a503f2e-0748-492e-91b5-937d817ea475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Me\\PJAIT\\Thesis\\Code\\yolov2-to-yolov8\\Models\\../Models\\yolov2.py:138: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path, map_location=self.device)\n"
     ]
    }
   ],
   "source": [
    "model = YOLOv2(state_dict_path='./darknet19_72.96.pth', device=device, dtype=dtype, num_anchors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73dd8655-630f-4ab3-b024-a47639368029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ../../datasets/VOCdevkit/VOC2012\\ImageSets\\Main\\trainval.txt\n"
     ]
    }
   ],
   "source": [
    "transforms = A.Compose([\n",
    "    A.Resize(width=416, height=416),\n",
    "    # A.VerticalFlip(p=1.0),\n",
    "    A.Normalize(mean=means, std=stds),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc'))\n",
    "train_set = VOCDatasetV2(devkit_path = '../../datasets/VOCdevkit/', \n",
    "                         subsets = [('VOC2012', 'trainval')],\n",
    "                         scales=[13], anchors=anchors, transforms=transforms, \n",
    "                         dtype=dtype, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "300da670-a134-4c68-b1d6-d11d8d137d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_to_expected_outputsV2(out, num_classes, anchors):\n",
    "    # input: (N, objects w/ relative coords, grid_size, grid_size)\n",
    "    # single obj.: (objectness, box, classes) * num_boxes\n",
    "    # [conf, obj_xc, obj_yc, obj_w, obj_h]\n",
    "    obj_stride = num_classes+5\n",
    "    out[:, 0::obj_stride, :, :] = out[:, 0::obj_stride, :, :].sigmoid() # objectness\n",
    "    out[:, 1::obj_stride, :, :] = out[:, 1::obj_stride, :, :].sigmoid() # xc\n",
    "    out[:, 2::obj_stride, :, :] = out[:, 2::obj_stride, :, :].sigmoid() # yc\n",
    "    \n",
    "    grid_size = out.shape[-1]\n",
    "    _anchors = torch.tensor(anchors).to(out.device) * grid_size\n",
    "    pw = _anchors[:, 0]\n",
    "    ph = _anchors[:, 1]\n",
    "    \n",
    "    out[:, 3::obj_stride, :, :] = pw[None, :, None, None] * out[:, 3::obj_stride, :, :].exp() # w\n",
    "    out[:, 4::obj_stride, :, :] = ph[None, :, None, None] * out[:, 4::obj_stride, :, :].exp() # h\n",
    "\n",
    "    for i in range(len(anchors)):\n",
    "        start_i = 5+i*obj_stride\n",
    "        end_i = obj_stride*(i+1)\n",
    "        out[:, start_i:end_i, :, :] = F.softmax(out[:, start_i:end_i, :, :], dim=1, dtype=out.dtype)\n",
    "        \n",
    "    # output: (N, objects w/ absolute coords, grid_size, grid_size)\n",
    "    return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e368d57-9452-40a2-99d8-60d4da731016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_absolute_boxesV2(out, num_classes, num_boxes):\n",
    "    # bringing to outputs relative to the entire image\n",
    "    grid_size = out.shape[-1]\n",
    "    indexed_columns = torch.tensor([range(0,grid_size) for i in range(grid_size)], dtype=out.dtype, device=out.device)\n",
    "    obj_stride = num_classes + 5\n",
    "    out[:, 1::obj_stride, :, :] = (out[:, 1::obj_stride, :, :] + indexed_columns) / grid_size # xc\n",
    "    out[:, 2::obj_stride, :, :] = (out[:, 2::obj_stride, :, :] + indexed_columns.T) / grid_size # yc\n",
    "    \n",
    "    out[:, 3::obj_stride, :, :] = out[:, 3::obj_stride, :, :] / grid_size # w\n",
    "    out[:, 4::obj_stride, :, :] = out[:, 4::obj_stride, :, :] / grid_size # h\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fc90370-ef97-4788-bf81-3469a932826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_eval(out, num_classes, num_boxes):\n",
    "    batch_size = out.shape[0]\n",
    "    obj_stride = num_classes + 5\n",
    "    grid_size = out.shape[-1]\n",
    "    out = out.view(batch_size, obj_stride*num_boxes, grid_size*grid_size)\n",
    "    out = torch.concat(torch.split(out, obj_stride, 1), -1)\n",
    "    out = out.permute(0,2,1) \n",
    "    \n",
    "    # batch_size, number of objects, (classes + coords + objectness) * num_boxes\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d885e57-cac0-4c59-bd7b-ebf8242d1555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_objectness(out):\n",
    "    batch_size = out.shape[0]\n",
    "    indeces = out[:, :, 0].argsort(descending=True)\n",
    "    out = out[torch.arange(batch_size).unsqueeze(1), indeces]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74d10554-8457-49a1-9665-a7f7bafde4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_below_threshold(out, obj_threshold):\n",
    "    batch_size = out.shape[0]\n",
    "    mask = out[:, :, 0] > obj_threshold\n",
    "    return \\\n",
    "    [out[i, mask[i], :] for i in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c12384f-c47f-4e98-b5c3-0380f7ac2012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(box1, box2):\n",
    "    # expects midpoint data\n",
    "    xmin1 = box1[..., 0] - box1[..., 2] / 2 \n",
    "    xmax1 = box1[..., 0] + box1[..., 2] / 2 \n",
    "    ymin1 = box1[..., 1] - box1[..., 3] / 2\n",
    "    ymax1 = box1[..., 1] + box1[..., 3] / 2\n",
    "\n",
    "    xmin2 = box2[..., 0] - box2[..., 2] / 2 \n",
    "    xmax2 = box2[..., 0] + box2[..., 2] / 2 \n",
    "    ymin2 = box2[..., 1] - box2[..., 3] / 2\n",
    "    ymax2 = box2[..., 1] + box2[..., 3] / 2\n",
    "\n",
    "    xmin_i = torch.stack([xmin1, xmin2]).max(dim=0)[0]\n",
    "    xmax_i = torch.stack([xmax1, xmax2]).min(dim=0)[0]\n",
    "    ymin_i = torch.stack([ymin1, ymin2]).max(dim=0)[0]\n",
    "    ymax_i = torch.stack([ymax1, ymax2]).min(dim=0)[0]\n",
    "\n",
    "    intersection = F.relu(xmax_i-xmin_i) * F.relu(ymax_i-ymin_i)\n",
    "\n",
    "    area1 = (xmax1 - xmin1) * (ymax1 - ymin1)\n",
    "    area2 = (xmax2 - xmin2) * (ymax2 - ymin2)\n",
    "\n",
    "    return intersection / (area1 + area2 - intersection + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32cb1e58-ee6e-4377-8d07-4554e2cd6aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMS(pred_boxes, num_classes=20, iou_threshold=0.5):\n",
    "\n",
    "    selected = []\n",
    "    for img in pred_boxes:\n",
    "        pred_class = img[:, 5]\n",
    "        selected_for_img = []\n",
    "        for cls in range(num_classes):\n",
    "            # get objects of that class\n",
    "            indeces = (pred_class == cls).nonzero(as_tuple=True)[0]\n",
    "            if indeces.shape[0] < 1:\n",
    "                continue\n",
    "            objects = img[indeces]\n",
    "            reference = objects[:1][:, 1:5] # highest IoU\n",
    "            compared = objects[1:][:, 1:5]\n",
    "            reference = reference.expand(compared.shape[0], 4)\n",
    "            ious = iou(reference, compared)\n",
    "            ious = torch.concat((torch.tensor([0.0]), ious))\n",
    "            selected_for_img.append(indeces[ious < iou_threshold])\n",
    "        selected_for_img = torch.concat(selected_for_img)\n",
    "        selected.append(selected_for_img)\n",
    "\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1e0f37c-bd44-4bc1-b5e2-4f0f94713167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_labels(eval_out):\n",
    "    return \\\n",
    "        [torch.concat((img[:, :5], img[:, 5:].argmax(dim=1).unsqueeze(-1)), dim=-1) for img in eval_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d28ee978-c156-49d5-ac39-5e0dd0293109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_boxes(out, anchors, num_classes=20, num_boxes=5, obj_threshold=0.5):\n",
    "    out = out.detach()\n",
    "    out = raw_to_expected_outputsV2(out, num_classes, anchors)\n",
    "    out = get_absolute_boxesV2(out, num_classes, num_boxes)\n",
    "    out = reshape_for_eval(out, num_classes, num_boxes)\n",
    "    out = sort_by_objectness(out)\n",
    "    out = remove_below_threshold(out, obj_threshold)\n",
    "    return get_class_labels(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4691a309-4152-4b4f-bd56-05ef19dd9a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(torch.stack([train_set[0][0], train_set[1][0]], dim=0))\n",
    "pred_boxes = get_pred_boxes(out, anchors, obj_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "704b8b58-69a0-495f-bb3e-dc060a802e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "indeces = pred_boxes[0][:, -1]==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbaeae12-208d-497e-9080-700493e7a8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6314, 0.6468, 0.6613, 0.1935, 0.3819, 0.0000],\n",
       "        [0.6164, 0.7331, 0.5818, 0.1386, 0.2208, 0.0000],\n",
       "        [0.6153, 0.8266, 0.5825, 0.0843, 0.3427, 0.0000],\n",
       "        [0.6110, 0.9609, 0.4969, 0.1278, 0.2824, 0.0000],\n",
       "        [0.6004, 0.9631, 0.5711, 0.1471, 0.2422, 0.0000],\n",
       "        [0.5919, 0.2511, 0.9675, 0.3294, 0.8659, 0.0000],\n",
       "        [0.5907, 0.7242, 0.9688, 0.6743, 0.9677, 0.0000],\n",
       "        [0.5897, 0.9582, 0.3501, 0.1758, 0.3197, 0.0000],\n",
       "        [0.5849, 0.8114, 0.9668, 0.2308, 0.4636, 0.0000],\n",
       "        [0.5832, 0.7309, 0.7424, 0.2150, 0.3891, 0.0000],\n",
       "        [0.5735, 0.5816, 0.1088, 0.4373, 0.5547, 0.0000],\n",
       "        [0.5640, 0.7980, 0.9665, 0.4175, 0.6108, 0.0000],\n",
       "        [0.5579, 0.4173, 0.6546, 0.1554, 0.2249, 0.0000],\n",
       "        [0.5566, 0.0452, 0.8089, 0.0998, 0.3293, 0.0000],\n",
       "        [0.5474, 0.7292, 0.8072, 0.3169, 0.2632, 0.0000],\n",
       "        [0.5460, 0.6654, 0.9609, 0.2580, 0.5515, 0.0000],\n",
       "        [0.5447, 0.8152, 0.5049, 0.1005, 0.3775, 0.0000],\n",
       "        [0.5388, 0.5737, 0.1993, 0.4584, 0.3184, 0.0000],\n",
       "        [0.5386, 0.1288, 0.8041, 0.1680, 0.2180, 0.0000],\n",
       "        [0.5386, 0.6594, 0.8845, 0.1242, 0.2881, 0.0000],\n",
       "        [0.5332, 0.5004, 0.7370, 0.1811, 0.2362, 0.0000],\n",
       "        [0.5329, 0.5802, 0.8898, 0.1424, 0.3621, 0.0000],\n",
       "        [0.5298, 0.6469, 0.5878, 0.1924, 0.2391, 0.0000],\n",
       "        [0.5240, 0.1724, 0.9723, 0.5018, 1.0042, 0.0000],\n",
       "        [0.5085, 0.1260, 0.9500, 0.0359, 0.1140, 0.0000],\n",
       "        [0.5022, 0.9621, 0.6538, 0.1623, 0.2681, 0.0000]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_boxes[0][indeces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1356e29-e8a2-4cb5-92fb-af766d048523",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = NMS(pred_boxes, num_classes=20, iou_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22af72a0-9427-4a1e-ae62-23ca8ef32c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6314,  0.6468,  0.6613,  0.1935,  0.3819,  0.0000],\n",
       "        [ 0.6164,  0.7331,  0.5818,  0.1386,  0.2208,  0.0000],\n",
       "        [ 0.6153,  0.8266,  0.5825,  0.0843,  0.3427,  0.0000],\n",
       "        ...,\n",
       "        [ 0.5028,  0.1103,  0.5792,  1.1056,  0.8730, 19.0000],\n",
       "        [ 0.5011,  0.2713,  0.9631,  1.3034,  0.2699, 19.0000],\n",
       "        [ 0.5004,  0.4235,  0.2003,  0.6272,  0.2911, 19.0000]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_boxes[0][sel[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fa0b964-2a43-4bb2-9f60-8205e6434e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8023,  0.8883,  0.2078,  0.0276,  0.0833, 16.0000],\n",
       "        [ 0.7804,  0.8784,  0.1091,  0.1218,  0.1980,  3.0000],\n",
       "        [ 0.7681,  0.7237,  0.6677,  0.0429,  0.0765, 12.0000],\n",
       "        ...,\n",
       "        [ 0.5011,  0.9649,  0.3507,  0.0458,  0.1759, 16.0000],\n",
       "        [ 0.5011,  0.2713,  0.9631,  1.3034,  0.2699, 19.0000],\n",
       "        [ 0.5004,  0.4235,  0.2003,  0.6272,  0.2911, 19.0000]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_boxes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bb0fdc-3a6d-4819-aa6c-0a6730ef4f27",
   "metadata": {},
   "source": [
    "# mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d0ab25f-7dbc-4568-b2fd-7704d8bfde29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt_boxes(gt_out, num_classes=20, num_boxes=5, obj_threshold=0.5):\n",
    "    gt_out = gt_out.detach()\n",
    "    gt_out = get_absolute_boxesV2(gt_out, num_classes, num_boxes)\n",
    "    gt_out = reshape_for_eval(gt_out, num_classes, num_boxes)\n",
    "    gt_out = sort_by_objectness(gt_out)\n",
    "    gt_out = remove_below_threshold(gt_out, obj_threshold)\n",
    "    return get_class_labels(gt_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58891972-569f-4195-a463-1e111bb1fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_out = torch.stack([train_set[0][1], train_set[1][1]], dim=0)\n",
    "gt_boxes = get_gt_boxes(gt_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c7db5b6-efc6-4cbb-bf81-85c064a0e02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 1.0000,  0.4051,  0.4823,  0.8280,  0.7520, 19.0000]]),\n",
       " tensor([[ 1.0000,  0.6835,  0.1117,  0.0420,  0.1592, 14.0000],\n",
       "         [ 1.0000,  0.4691,  0.5934,  0.9080,  0.9670, 18.0000]])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecd85cde-00c4-4fe6-b218-de9c4965756a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 125, 13, 13])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c4632f6b-1446-48b9-a5f1-7117d8ed939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(torch.stack([train_set[0][0], train_set[1][0]], dim=0))\n",
    "pred_boxes = get_pred_boxes(out, anchors, obj_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de81554c-c16b-4bee-8999-8641ac839819",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_out = torch.stack([train_set[0][1], train_set[1][1]], dim=0)\n",
    "gt_out1 = gt_out.detach().clone()\n",
    "gt_boxes = get_gt_boxes(gt_out)\n",
    "gt_boxes1 = get_gt_boxes(gt_out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f1b4a026-d201-4ea2-be3f-74cfd8cb6cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.4051,  0.4823,  0.8280,  0.7520, 19.0000,  1.0000,\n",
       "          1.0000,  1.0000],\n",
       "        [ 1.0000,  1.0000,  0.6835,  0.1117,  0.0420,  0.1592, 14.0000,  1.0000,\n",
       "          1.0000,  1.0000],\n",
       "        [ 1.0000,  1.0000,  0.4691,  0.5934,  0.9080,  0.9670, 18.0000,  1.0000,\n",
       "          1.0000,  1.0000]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = mAP(gt_boxes, gt_boxes1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19b4c997-9005-4283-960e-f36ec20073dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.4051,  0.4823,  0.8280,  0.7520, 19.0000,  1.0000,\n",
       "          1.0000,  1.0000],\n",
       "        [ 1.0000,  1.0000,  0.4691,  0.5934,  0.9080,  0.9670, 18.0000,  1.0000,\n",
       "          1.0000,  1.0000],\n",
       "        [ 1.0000,  1.0000,  0.6835,  0.1117,  0.0420,  0.1592, 14.0000,  1.0000,\n",
       "          1.0000,  1.0000]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[res[:, 6].argsort(descending=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "336c9399-049c-40be-950c-6fbb19b42eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 1.0000,  0.4051,  0.4823,  0.8280,  0.7520, 19.0000]]),\n",
       " tensor([[ 1.0000,  0.6835,  0.1117,  0.0420,  0.1592, 14.0000],\n",
       "         [ 1.0000,  0.4691,  0.5934,  0.9080,  0.9670, 18.0000]])]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c84568-23c2-4c08-afd9-e0fdabd1052b",
   "metadata": {},
   "source": [
    "# Develop test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "37ac4b66-13be-45e1-81fd-07b0d4039a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics.aggregation.auc import AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "da5555d8-9b0d-48ec-8127-48dd8aef8ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mAP(pred_boxes, gt_boxes, num_classes=20, iou_threshold=0.5):\n",
    "\n",
    "    # create one tensor where 0 dim is number of objects and 1 dim is an object.\n",
    "    # [[img_i, objectness_score, xc, yc, w, h, class_label, is_true_positive, precision, recall]]\n",
    "    batch_size = len(gt_boxes)\n",
    "    for img_i in range(batch_size):\n",
    "        img_pred = pred_boxes[img_i]\n",
    "        num_objects = img_pred.shape[0]\n",
    "        img_i_column = torch.full((num_objects, 1), img_i, dtype=img_pred.dtype, device=img_pred.device)\n",
    "        true_positives_column = torch.zeros(num_objects, dtype=img_pred.dtype, device=img_pred.device).unsqueeze(-1)\n",
    "        metrics_columns = torch.full((num_objects, 2), -1, dtype=img_pred.dtype, device=img_pred.device)\n",
    "        new_img_pred = torch.cat((img_i_column, img_pred, true_positives_column, metrics_columns), dim=-1)\n",
    "        pred_boxes[img_i] = new_img_pred\n",
    "    pred_boxes = torch.concat(pred_boxes)\n",
    "\n",
    "    # figure out if predicted objects are true positives\n",
    "    for pred_obj_i in range(pred_boxes.shape[0]):\n",
    "        pred_obj = pred_boxes[pred_obj_i:pred_obj_i+1]\n",
    "        img_i = int(pred_obj[0, 0])\n",
    "        _class = pred_obj[0, 6]\n",
    "        true_objs = gt_boxes[img_i][\n",
    "                            (gt_boxes[img_i][:, 5] == _class).nonzero(as_tuple=True)[0]\n",
    "        ]\n",
    "\n",
    "        if true_objs.numel() == 0:\n",
    "            continue\n",
    "        pred_obj = pred_obj[:, 2:6]\n",
    "        pred_obj = pred_obj.expand(true_objs.shape[0], 4)\n",
    "        true_positive = torch.sum(\n",
    "            iou(pred_obj, true_objs[:, 1:5]) > iou_threshold\n",
    "        ).item() > 0\n",
    "    \n",
    "        if true_positive:\n",
    "            pred_boxes[pred_obj_i, 7] = 1.0\n",
    "\n",
    "    # sort by objectness score\n",
    "    pred_boxes = pred_boxes[pred_boxes[:, 1].argsort(descending=True)]\n",
    "\n",
    "    # calculate precision and recall for every class\n",
    "    pred_class = pred_boxes[:, 6]\n",
    "    results = {}\n",
    "    for cls in range(num_classes):\n",
    "        indeces_of_cls = (pred_class == cls).nonzero(as_tuple=True)[0]\n",
    "        num_objects_of_cls = indeces_of_cls.shape[0]\n",
    "        for obj_i in range(num_objects_of_cls):         \n",
    "            objects = pred_boxes[indeces_of_cls[:obj_i+1]]\n",
    "    \n",
    "            # precision tp / (tp+fp)\n",
    "            tp = objects[:, 7].sum()\n",
    "            tp_plus_fp = objects.shape[0]\n",
    "            precision = tp / tp_plus_fp\n",
    "    \n",
    "            # recall tp / (tp + fn)\n",
    "            recall = tp / num_objects_of_cls\n",
    "\n",
    "            pred_boxes[indeces_of_cls[obj_i], 8] = precision\n",
    "            pred_boxes[indeces_of_cls[obj_i], 9] = recall\n",
    "\n",
    "        if num_objects_of_cls > 0:\n",
    "            metric = AUC()\n",
    "            precision_scores = torch.cat([torch.tensor([1.]), pred_boxes[indeces_of_cls, 8]])\n",
    "            recall_scores = torch.cat([torch.tensor([0.]), pred_boxes[indeces_of_cls, 9]])\n",
    "            metric.update(recall_scores, precision_scores)\n",
    "            print(recall_scores, precision_scores)\n",
    "            ap_score = metric.compute()\n",
    "            metric.reset()\n",
    "            results[cls] = {'ap_score': ap_score, 'num_objects_of_cls': num_objects_of_cls}\n",
    "        else:\n",
    "            results[cls] = {'ap_score': -1, 'num_objects_of_cls': num_objects_of_cls}\n",
    "            continue\n",
    "\n",
    "    N = 0\n",
    "    ap_sum = 0\n",
    "    for cls, value in results.items():\n",
    "        if value['num_objects_of_cls'] < 1:\n",
    "            continue\n",
    "        else:\n",
    "            N += 1\n",
    "            ap_sum += value['ap_score']\n",
    "\n",
    "    results['mAP'] = ap_sum / N\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "08b9c56c-f7ff-4a8d-9f3d-e35f2ffb4373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.1667, 0.3333, 0.5000, 0.6667, 0.8333, 1.0000]) tensor([1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'ap_score': tensor([1.]), 'num_objects_of_cls': 6},\n",
       " 1: {'ap_score': -1, 'num_objects_of_cls': 0},\n",
       " 2: {'ap_score': -1, 'num_objects_of_cls': 0},\n",
       " 3: {'ap_score': -1, 'num_objects_of_cls': 0},\n",
       " 4: {'ap_score': -1, 'num_objects_of_cls': 0},\n",
       " 5: {'ap_score': -1, 'num_objects_of_cls': 0},\n",
       " 6: {'ap_score': -1, 'num_objects_of_cls': 0},\n",
       " 7: {'ap_score': -1, 'num_objects_of_cls': 0},\n",
       " 8: {'ap_score': -1, 'num_objects_of_cls': 0},\n",
       " 9: {'ap_score': -1, 'num_objects_of_cls': 0},\n",
       " 10: {'ap_score': -1, 'num_objects_of_cls': 0},\n",
       " 11: {'ap_score': -1, 'num_objects_of_cls': 0},\n",
       " 12: {'ap_score': -1, 'num_objects_of_cls': 0},\n",
       " 13: {'ap_score': -1, 'num_objects_of_cls': 0},\n",
       " 14: {'ap_score': -1, 'num_objects_of_cls': 0},\n",
       " 15: {'ap_score': -1, 'num_objects_of_cls': 0},\n",
       " 16: {'ap_score': -1, 'num_objects_of_cls': 0},\n",
       " 17: {'ap_score': -1, 'num_objects_of_cls': 0},\n",
       " 18: {'ap_score': -1, 'num_objects_of_cls': 0},\n",
       " 19: {'ap_score': -1, 'num_objects_of_cls': 0},\n",
       " 'mAP': tensor([1.])}"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_boxes = [torch.tensor([[1.0, 0.2, 0.3, 0.2, 0.3, 0.],\n",
    "                          [1.0, 0.6, 0.75, 0.5, 0.3, 0.]]),\n",
    "            torch.tensor([[1.0, 0.8, 0.7, 0.2, 0.3, 0.],\n",
    "                          [1.0, 0.4, 0.25, 0.5, 0.3, 0.]]),\n",
    "            torch.tensor([[1.0, 0.2, 0.7, 0.2, 0.3, 0.],\n",
    "                          [1.0, 0.75, 0.4, 0.3, 0.5, 0.]])]\n",
    "gt_boxes1 = [torch.tensor([[1.0, 0.2, 0.3, 0.2, 0.3, 0.],\n",
    "                          [1.0, 0.6, 0.75, 0.5, 0.3, 0.]]),\n",
    "            torch.tensor([[1.0, 0.8, 0.7, 0.2, 0.3, 0.],\n",
    "                          [1.0, 0.4, 0.25, 0.5, 0.3, 0.]]),\n",
    "            torch.tensor([[1.0, 0.2, 0.7, 0.2, 0.3, 0.],\n",
    "                          [1.0, 0.75, 0.4, 0.3, 0.5, 0.]])]\n",
    "pred_boxes = [torch.tensor([[0.3, 0.7, 0.15, 0.3, 0.1, 0.],\n",
    "                            [0.7, 0.65, 0.7, 0.5, 0.3, 0.],\n",
    "                            [0.6, 0.2, 0.25, 0.3, 0.3, 0.]]),\n",
    "              torch.tensor([[0.3, 0.3, 0.85, 0.3, 0.1, 0.],\n",
    "                            [0.7, 0.35, 0.3, 0.5, 0.3, 0.],\n",
    "                            [0.6, 0.8, 0.75, 0.3, 0.3, 0.]]),\n",
    "              torch.tensor([[0.3, 0.3, 0.15, 0.3, 0.1, 0.],\n",
    "                            [0.7, 0.7, 0.35, 0.3, 0.5, 0.],\n",
    "                            [0.6, 0.2, 0.75, 0.3, 0.3, 0.]])]\n",
    "mAP(gt_boxes1, gt_boxes, num_classes=20, iou_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "22c8a6fa-a771-4a64-be6d-edba656b1fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(torch.stack([train_set[0][0], train_set[1][0]], dim=0))\n",
    "pred_boxes1 = get_pred_boxes(out, anchors, obj_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0486edcc-acf9-442a-bfa5-e1eb157840a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2]), tensor([0, 1, 2]), tensor([0, 1, 2])]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NMS(pred_boxes, iou_threshold=0.51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b2c23eaa-b4aa-44dc-8d16-70ec84718340",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'indeces_of_cls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[176], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m metric \u001b[38;5;241m=\u001b[39m AUC()\n\u001b[1;32m----> 2\u001b[0m precision_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.\u001b[39m]), pred_boxes[\u001b[43mindeces_of_cls\u001b[49m, \u001b[38;5;241m8\u001b[39m]])\n\u001b[0;32m      3\u001b[0m recall_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.\u001b[39m]), pred_boxes[indeces_of_cls, \u001b[38;5;241m9\u001b[39m]])\n\u001b[0;32m      4\u001b[0m metric\u001b[38;5;241m.\u001b[39mupdate(recall_scores, precision_scores)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'indeces_of_cls' is not defined"
     ]
    }
   ],
   "source": [
    "metric = AUC()\n",
    "precision_scores = torch.tensor([1., ])\n",
    "recall_scores = torch.tensor([0., .11, .33, .66, 1.])\n",
    "metric.update(recall_scores, precision_scores)\n",
    "print(recall_scores, precision_scores)\n",
    "ap_score = metric.compute()\n",
    "# metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4c3cdf-19a7-4840-b73b-0530befd6809",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "New Python (GPU)",
   "language": "python",
   "name": "new_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
