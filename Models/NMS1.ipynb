{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cfbba29-f304-436c-b3c9-e8909c7009bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Vstanovlene\\Anaconda Distribution\\envs\\new_gpu_env\\lib\\site-packages\\albumentations\\__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.23). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../\")\n",
    "sys.path.insert(1, \"../Models/\")\n",
    "import torch\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from detection_datasets import *\n",
    "from yolov2 import YOLOv2D19 as YOLOv2\n",
    "from data_preprocessing import get_norms\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations as A\n",
    "from eval import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85a864ce-def4-43c5-836a-cedc2546dc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 0.00 MB\n",
      "Allocated memory: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "import gc\n",
    "\n",
    "# Invoke garbage collector\n",
    "gc.collect()\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ca4bdef-b093-4e5c-b877-c906e02ee829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../Models/anchors_VOC0712trainval.pickle', 'rb') as handle:\n",
    "    anchors = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feae0801-2ec2-475f-b965-e7b556f7dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "# device = torch.device('cpu')\n",
    "dtype=torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb9dd7a0-db44-4032-a7bb-32df4450c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = get_norms('../../datasets/VOCdevkit/trainval_norms.json')\n",
    "means = norms['means']\n",
    "stds = norms['stds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f8c8c8-bbc5-47d1-bf4e-cb6e46eafe64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Me\\PJAIT\\Thesis\\Code\\yolov2-to-yolov8\\Models\\../Models\\yolov2.py:138: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path, map_location=self.device)\n"
     ]
    }
   ],
   "source": [
    "model = YOLOv2(state_dict_path='./darknet19_72.96.pth', device=device, dtype=dtype, num_anchors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a45e53d-fe84-4a33-b090-16b5e95aaa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ../../datasets/VOCdevkit/VOC2012\\ImageSets\\Main\\trainval.txt\n",
      "True ../../datasets/VOCdevkit/VOC2007\\ImageSets\\Main\\val.txt\n"
     ]
    }
   ],
   "source": [
    "transforms = A.Compose([\n",
    "    A.Resize(width=416, height=416),\n",
    "    # A.VerticalFlip(p=1.0),\n",
    "    A.Normalize(mean=means, std=stds),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc'))\n",
    "train_set = VOCDatasetV2(devkit_path = '../../datasets/VOCdevkit/', \n",
    "                         subsets = [('VOC2012', 'trainval')],\n",
    "                         scales=[13], anchors=anchors, transforms=transforms, \n",
    "                         dtype=dtype, device=device)\n",
    "val_set = VOCDatasetV2(devkit_path = '../../datasets/VOCdevkit/', \n",
    "                       subsets = [('VOC2007', 'val')],\n",
    "                       scales=[13], anchors=anchors, transforms=transforms, \n",
    "                       dtype=dtype, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bffd6807-7fa9-4327-a145-88d039bfd3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59f9c612-f5ee-4e64-b369-c6aa9d6bfc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLOv2(state_dict_path='./darknet19_72.96.pth', device=device, dtype=dtype, num_anchors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "897c9d55-8aa3-4aa1-86c8-f24861dbdee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 2025-03-15 23:25:58.831834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Vstanovlene\\Anaconda Distribution\\envs\\new_gpu_env\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: Encountered more than 100 detections in a single image. This means that certain detections with the lowest scores will be ignored, that may have an undesirable impact on performance. Please consider adjusting the `max_detection_threshold` to suit your use case. To disable this warning, set attribute class `warn_on_many_detections=False`, after initializing the metric.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1 2025-03-15 23:26:00.634435\n",
      "batch 2 2025-03-15 23:26:02.169646\n",
      "batch 3 2025-03-15 23:26:03.569033\n",
      "batch 4 2025-03-15 23:26:05.144547\n",
      "batch 5 2025-03-15 23:26:06.584536\n",
      "batch 6 2025-03-15 23:26:08.091271\n",
      "batch 7 2025-03-15 23:26:09.636529\n",
      "batch 8 2025-03-15 23:26:11.230546\n",
      "batch 9 2025-03-15 23:26:12.661851\n",
      "batch 10 2025-03-15 23:26:14.137801\n",
      "batch 11 2025-03-15 23:26:16.036941\n",
      "batch 12 2025-03-15 23:26:17.576725\n",
      "batch 13 2025-03-15 23:26:19.002605\n",
      "batch 14 2025-03-15 23:26:20.546336\n",
      "batch 15 2025-03-15 23:26:22.006161\n",
      "batch 16 2025-03-15 23:26:23.342437\n",
      "batch 17 2025-03-15 23:26:24.881977\n",
      "batch 18 2025-03-15 23:26:26.514172\n",
      "batch 19 2025-03-15 23:26:27.999413\n",
      "batch 20 2025-03-15 23:26:29.652513\n",
      "batch 21 2025-03-15 23:26:31.065484\n",
      "batch 22 2025-03-15 23:26:32.481983\n",
      "batch 23 2025-03-15 23:26:34.052163\n",
      "batch 24 2025-03-15 23:26:35.476258\n",
      "batch 25 2025-03-15 23:26:37.064890\n",
      "batch 26 2025-03-15 23:26:38.502722\n",
      "batch 27 2025-03-15 23:26:40.091970\n",
      "batch 28 2025-03-15 23:26:41.678957\n",
      "batch 29 2025-03-15 23:26:43.723955\n",
      "batch 30 2025-03-15 23:26:45.280129\n",
      "batch 31 2025-03-15 23:26:46.754219\n",
      "batch 32 2025-03-15 23:26:48.240877\n",
      "batch 33 2025-03-15 23:26:49.866557\n",
      "batch 34 2025-03-15 23:26:51.328517\n",
      "batch 35 2025-03-15 23:26:52.876650\n",
      "batch 36 2025-03-15 23:26:54.458443\n",
      "batch 37 2025-03-15 23:26:55.837541\n",
      "batch 38 2025-03-15 23:26:57.357248\n",
      "batch 39 2025-03-15 23:26:59.086844\n",
      "batch 40 2025-03-15 23:27:00.523516\n",
      "batch 41 2025-03-15 23:27:01.870328\n",
      "batch 42 2025-03-15 23:27:03.463370\n",
      "batch 43 2025-03-15 23:27:05.058398\n",
      "batch 44 2025-03-15 23:27:06.652306\n",
      "batch 45 2025-03-15 23:27:08.127869\n",
      "batch 46 2025-03-15 23:27:09.595249\n",
      "batch 47 2025-03-15 23:27:11.033945\n",
      "batch 48 2025-03-15 23:27:12.554422\n",
      "batch 49 2025-03-15 23:27:14.176097\n",
      "batch 50 2025-03-15 23:27:15.676349\n",
      "batch 51 2025-03-15 23:27:17.186673\n",
      "batch 52 2025-03-15 23:27:18.693374\n",
      "batch 53 2025-03-15 23:27:20.089791\n",
      "batch 54 2025-03-15 23:27:21.588115\n",
      "batch 55 2025-03-15 23:27:22.983520\n",
      "batch 56 2025-03-15 23:27:24.533525\n",
      "batch 57 2025-03-15 23:27:25.967309\n",
      "batch 58 2025-03-15 23:27:27.389531\n",
      "batch 59 2025-03-15 23:27:28.824304\n",
      "batch 60 2025-03-15 23:27:30.301244\n",
      "batch 61 2025-03-15 23:27:31.944362\n",
      "batch 62 2025-03-15 23:27:33.440889\n",
      "batch 63 2025-03-15 23:27:34.911929\n",
      "batch 64 2025-03-15 23:27:36.433522\n",
      "batch 65 2025-03-15 23:27:37.985188\n",
      "batch 66 2025-03-15 23:27:39.714273\n",
      "batch 67 2025-03-15 23:27:41.506882\n",
      "batch 68 2025-03-15 23:27:43.419730\n",
      "batch 69 2025-03-15 23:27:45.338908\n",
      "batch 70 2025-03-15 23:27:47.458343\n",
      "batch 71 2025-03-15 23:27:49.602207\n",
      "batch 72 2025-03-15 23:27:51.372241\n",
      "batch 73 2025-03-15 23:27:53.105526\n",
      "batch 74 2025-03-15 23:27:54.906291\n",
      "batch 75 2025-03-15 23:27:56.743527\n",
      "batch 76 2025-03-15 23:27:58.472574\n",
      "batch 77 2025-03-15 23:28:00.330353\n",
      "batch 78 2025-03-15 23:28:01.618436\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manchors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Me\\PJAIT\\Thesis\\Code\\yolov2-to-yolov8\\Models\\../Models\\eval.py:243\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, loader, anchors, mAP, num_classes, num_boxes, obj_threshold, iou_threshold)\u001b[0m\n\u001b[0;32m    239\u001b[0m             targets\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m:boxes,\n\u001b[0;32m    240\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m:labels})\n\u001b[0;32m    242\u001b[0m     mAP\u001b[38;5;241m.\u001b[39mupdate(preds\u001b[38;5;241m=\u001b[39mpreds, target\u001b[38;5;241m=\u001b[39mtargets)\n\u001b[1;32m--> 243\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmAP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m mAP\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\Vstanovlene\\Anaconda Distribution\\envs\\new_gpu_env\\lib\\site-packages\\torchmetrics\\metric.py:699\u001b[0m, in \u001b[0;36mMetric._wrap_compute.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;66;03m# compute relies on the sync context manager to gather the states across processes and apply reduction\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;66;03m# if synchronization happened, the current rank accumulated states will be restored to keep\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;66;03m# accumulation going if ``should_unsync=True``,\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msync_context(\n\u001b[0;32m    695\u001b[0m     dist_sync_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist_sync_fn,\n\u001b[0;32m    696\u001b[0m     should_sync\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_sync,\n\u001b[0;32m    697\u001b[0m     should_unsync\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_unsync,\n\u001b[0;32m    698\u001b[0m ):\n\u001b[1;32m--> 699\u001b[0m     value \u001b[38;5;241m=\u001b[39m _squeeze_if_scalar(compute(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;66;03m# clone tensor to avoid in-place operations after compute, altering already computed results\u001b[39;00m\n\u001b[0;32m    701\u001b[0m     value \u001b[38;5;241m=\u001b[39m apply_to_collection(value, Tensor, \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mclone())\n",
      "File \u001b[1;32mC:\\Vstanovlene\\Anaconda Distribution\\envs\\new_gpu_env\\lib\\site-packages\\torchmetrics\\detection\\mean_ap.py:566\u001b[0m, in \u001b[0;36mMeanAveragePrecision.compute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# if class mode is enabled, evaluate metrics per class\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_metrics:\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;66;03m# regardless of average method, reinitialize dataset to get rid of internal state which can\u001b[39;00m\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;66;03m# lead to wrong results when evaluating per class\u001b[39;00m\n\u001b[1;32m--> 566\u001b[0m     coco_preds, coco_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_coco_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m     coco_eval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcocoeval(coco_target, coco_preds, iouType\u001b[38;5;241m=\u001b[39mi_type)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[0;32m    568\u001b[0m     coco_eval\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39miouThrs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miou_thresholds, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n",
      "File \u001b[1;32mC:\\Vstanovlene\\Anaconda Distribution\\envs\\new_gpu_env\\lib\\site-packages\\torchmetrics\\detection\\mean_ap.py:621\u001b[0m, in \u001b[0;36mMeanAveragePrecision._get_coco_datasets\u001b[1;34m(self, average)\u001b[0m\n\u001b[0;32m    611\u001b[0m coco_target, coco_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoco(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoco()  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[0;32m    613\u001b[0m coco_target\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_coco_format(\n\u001b[0;32m    614\u001b[0m     labels\u001b[38;5;241m=\u001b[39mgroundtruth_labels,\n\u001b[0;32m    615\u001b[0m     boxes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroundtruth_box \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroundtruth_box) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    619\u001b[0m     average\u001b[38;5;241m=\u001b[39maverage,\n\u001b[0;32m    620\u001b[0m )\n\u001b[1;32m--> 621\u001b[0m coco_preds\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_coco_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetection_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetection_box\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetection_box\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetection_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetection_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetection_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mredirect_stdout(io\u001b[38;5;241m.\u001b[39mStringIO()):\n\u001b[0;32m    630\u001b[0m     coco_target\u001b[38;5;241m.\u001b[39mcreateIndex()\n",
      "File \u001b[1;32mC:\\Vstanovlene\\Anaconda Distribution\\envs\\new_gpu_env\\lib\\site-packages\\torchmetrics\\detection\\mean_ap.py:951\u001b[0m, in \u001b[0;36mMeanAveragePrecision._get_coco_format\u001b[1;34m(self, labels, boxes, masks, scores, crowds, area, average)\u001b[0m\n\u001b[0;32m    948\u001b[0m     annotation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegmentation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m image_mask\n\u001b[0;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scores \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 951\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mscores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(score, \u001b[38;5;28mfloat\u001b[39m):\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input score of sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, element \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (expected value of type float, got type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(score)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    956\u001b[0m         )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate(model, loader=val_loader, anchors=anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f5045d1-6637-4110-8fe6-e8e5d66298f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "def evaluate(model, loader, mAP=MeanAveragePrecision(box_format='cxcywh',class_metrics=True),\n",
    "            num_classes=20, num_boxes=5, obj_threshold=0.5, iou_threshold=0.5):\n",
    "    for i, (imgs, labels) in enumerate(loader):\n",
    "        print(f'batch {i} {datetime.datetime.now()}')\n",
    "        preds = []\n",
    "        targets = []\n",
    "        batch_size = labels.shape[0]\n",
    "        with torch.no_grad():\n",
    "            out = model(imgs).detach()\n",
    "            pred_boxes = get_pred_boxes(out, anchors, num_classes=num_classes, num_boxes=num_boxes, obj_threshold=obj_threshold)\n",
    "            gt_boxes = get_gt_boxes(labels.detach(), num_classes=num_classes, num_boxes=num_boxes, obj_threshold=obj_threshold)\n",
    "            keep = NMS(pred_boxes, num_classes=num_classes, iou_threshold=iou_threshold)\n",
    "            for j in range(batch_size):\n",
    "                boxes = pred_boxes[j][keep[j], ..., 1:5]\n",
    "                scores = pred_boxes[j][keep[j], ..., 0]\n",
    "                labels = pred_boxes[j][keep[j], ..., 5].to(torch.uint8)\n",
    "                preds.append({'boxes':boxes,\n",
    "                             'scores':scores,\n",
    "                             'labels':labels})\n",
    "    \n",
    "                boxes = gt_boxes[j][..., 1:5]\n",
    "                labels = gt_boxes[j][..., 5].to(torch.uint8)\n",
    "                targets.append({'boxes':boxes,\n",
    "                               'labels':labels})\n",
    "    \n",
    "        mAP.update(preds=preds, target=targets)\n",
    "    result = mAP.compute()\n",
    "    mAP.reset()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6e818a-1c2d-4a41-adb4-a1d11464bd5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
