{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "6b4b0840-b07e-4fb1-9f71-74ded9239322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import xml.etree.ElementTree as ET\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "cdeb0713-17b1-492c-ac5f-786f89d443c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('anchors_VOC0712trainval.pickle', 'rb') as handle:\n",
    "    anchors = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "5c6248c8-a816-4400-a5fc-6d5e3b4c7844",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, devkit_path, \n",
    "                 subsets = [('VOC2007', 'trainval'), ('VOC2012', 'trainval')], \n",
    "                 anchors = [], scales = [13], \n",
    "                 threshold_ignore_prediction = 0.5,\n",
    "                 transforms = None,\n",
    "                 dtype=None, device=None):\n",
    "        super().__init__()\n",
    "        self.devkit_path = devkit_path\n",
    "        self.subsets = subsets\n",
    "        self.anchors = anchors\n",
    "        self.scales = scales\n",
    "        self.threshold_ignore_prediction = threshold_ignore_prediction\n",
    "        self.transforms = transforms\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "\n",
    "        self.object_placed = 0\n",
    "        self.object_not_placed = 0\n",
    "\n",
    "        self.all_labels = []\n",
    "        for subset in self.subsets:\n",
    "            subset_path = os.path.join(self.devkit_path, subset[0], 'ImageSets', 'Main', '{}.txt'.format(subset[1]))\n",
    "            print(os.path.exists(subset_path), subset_path)\n",
    "            with open(subset_path, 'r') as file:\n",
    "                subset_labels = file.read().splitlines()\n",
    "            self.all_labels.append(subset_labels)\n",
    "\n",
    "        self.classes = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\n",
    "                        'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train',\n",
    "                        'tvmonitor']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ========= TEST ==========\n",
    "        idx_copy = idx\n",
    "        # ========= TEST ==========\n",
    "        # get paths\n",
    "        subset_idx = 0\n",
    "        for subset_labels in self.all_labels:\n",
    "            if idx < len(subset_labels):\n",
    "                break\n",
    "            else:\n",
    "                subset_idx += 1\n",
    "                idx -= len(subset_labels)\n",
    "\n",
    "        if idx < 0 or subset_idx >= len(self.subsets):\n",
    "            raise Exception(\"Index out of range.\")\n",
    "\n",
    "        # print(subset_idx, idx)\n",
    "        image_path = os.path.join(self.devkit_path, self.subsets[subset_idx][0], 'JPEGImages', '{}.jpg'.format(self.all_labels[subset_idx][idx]))\n",
    "        annotation_path = os.path.join(self.devkit_path, self.subsets[subset_idx][0], 'Annotations', '{}.xml'.format(self.all_labels[subset_idx][idx]))\n",
    "\n",
    "        # print(os.path.exists(image_path), image_path)\n",
    "        # print(os.path.exists(annotation_path), annotation_path)\n",
    "\n",
    "        # get PIL image\n",
    "        PIL_img = Image.open(image_path)\n",
    "\n",
    "        # initialize tensors\n",
    "        gt_out = [torch.zeros(len(self.anchors)*(5+len(self.classes)), scale, scale) for scale in self.scales]\n",
    "        \n",
    "        # parse annotations\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        bboxes = []\n",
    "        for item in root.findall('./object'):\n",
    "            bndbox = item.find(\"bndbox\")\n",
    "            xmin = int(bndbox.find(\"xmin\").text)\n",
    "            ymin = int(bndbox.find(\"ymin\").text)\n",
    "            xmax = int(bndbox.find(\"xmax\").text)\n",
    "            ymax = int(bndbox.find(\"ymax\").text)\n",
    "            class_label = self.classes.index(item.find(\"name\").text)\n",
    "\n",
    "            bboxes.append([xmin, ymin, xmax, ymax, class_label])\n",
    "\n",
    "        if self.transforms:\n",
    "            np_img = np.array(PIL_img.convert(\"RGB\"))\n",
    "            transformed = self.transforms(image=np_img, bboxes=bboxes)\n",
    "            image = transformed['image']\n",
    "            img_d, img_h, img_w = image.shape\n",
    "            bboxes = transformed['bboxes']\n",
    "        else:\n",
    "            return PIL_img, bboxes\n",
    "\n",
    "        for box in bboxes:\n",
    "            xmin, ymin, xmax, ymax, class_label = box\n",
    "            class_label = int(class_label)\n",
    "        \n",
    "            obj_w = xmax - xmin\n",
    "            obj_h = ymax - ymin\n",
    "\n",
    "            obj_xc = xmax - obj_w / 2\n",
    "            obj_yc = ymax - obj_h / 2\n",
    "\n",
    "            for scale_idx, scale in enumerate(self.scales):\n",
    "                cell_w = img_w / scale\n",
    "                cell_h = img_h / scale\n",
    "\n",
    "                cell_x = int(obj_xc / cell_w)\n",
    "                cell_y = int(obj_yc / cell_h)\n",
    "                obj_xc = (obj_xc % cell_w) / cell_w\n",
    "                obj_yc = (obj_yc % cell_h) / cell_h\n",
    "\n",
    "                obj_w = obj_w / cell_w\n",
    "                obj_h = obj_h / cell_h\n",
    "\n",
    "                bndbox = torch.tensor([obj_xc, obj_yc, obj_w, obj_h])\n",
    "                \n",
    "                # print((cell_x, cell_y), bndbox, self.classes[class_label])\n",
    "                \n",
    "                IoUs = torch.empty(len(self.anchors))\n",
    "                for i, anchor in enumerate(self.anchors):\n",
    "                    cell_aw = anchor[0] * scale\n",
    "                    cell_ah = anchor[1] * scale\n",
    "                    _anchor = torch.tensor([obj_xc, obj_yc, cell_aw, cell_ah])\n",
    "                    IoUs[i] = IoU(bndbox, _anchor)\n",
    "\n",
    "                anchors_argsort = torch.argsort(IoUs, descending=True)\n",
    "                best_anchor = anchors_argsort[0]\n",
    "                \n",
    "                placement_0 = best_anchor*(5+len(self.classes))\n",
    "                _objectness = (placement_0, cell_x, cell_y)\n",
    "\n",
    "                taken = gt_out[scale_idx][_objectness] == 1                    \n",
    "                if taken:\n",
    "                    # =========== TEST ==========\n",
    "                    self.object_not_placed += 1\n",
    "                    # =========== TEST ==========\n",
    "                    continue\n",
    "                else:\n",
    "                    gt_out[scale_idx][_objectness] = 1\n",
    "                    gt_out[scale_idx][placement_0+1:placement_0+5, cell_x, cell_y] = bndbox\n",
    "                    \n",
    "                    label_placement = placement_0 + 1 + 4 + class_label\n",
    "                    gt_out[scale_idx][label_placement] = 1\n",
    "\n",
    "                    # =========== TEST ==========\n",
    "                    self.object_placed += 1\n",
    "                    # =========== TEST ==========\n",
    "\n",
    "                # not the best anchors\n",
    "                for anchor_idx in anchors_argsort[1:]:\n",
    "                    if IoUs[anchor_idx] > self.threshold_ignore_prediction:\n",
    "                        placement_0 = anchor_idx*(5+len(self.classes))\n",
    "                        _objectness = (placement_0, cell_x, cell_y)\n",
    "                        gt_out[scale_idx][_objectness] = -1\n",
    "                        \n",
    "                    \n",
    "        return (image, gt_out) if len(self.scales) > 1 else (image, gt_out[0]) # that all basically is *just* the label. I also need to write transforms for the input image\n",
    "        \n",
    "    def __len__(self):\n",
    "        summed_len = 0\n",
    "        for _subset in self.all_labels:\n",
    "            summed_len += len(_subset)\n",
    "        return summed_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "eb097bc4-2384-41e3-9f72-b79163e871bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ../../datasets/VOCdevkit/VOC2007\\ImageSets\\Main\\trainval.txt\n",
      "True ../../datasets/VOCdevkit/VOC2012\\ImageSets\\Main\\trainval.txt\n"
     ]
    }
   ],
   "source": [
    "transforms = A.Compose([\n",
    "    A.Resize(width=224, height=224),\n",
    "    A.VerticalFlip(p=1.0),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc'))\n",
    "train_set = VOCDataset(devkit_path = '../../datasets/VOCdevkit/', scales=[13], anchors=anchors, transforms=transforms)\n",
    "image, gt_out = train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "38fd75c0-b92f-4da8-8267-58e3c09724b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6deffd9c-aa4b-4e2f-b9c4-7a5aa1ff1a3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for img in train_loader:\n",
    "    del img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ea2c57e2-c290-4e18-9cc9-8b14d5ba877c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45658, 1570, 47228)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.object_placed, train_set.object_not_placed, train_set.object_placed + train_set.object_not_placed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "New Python (GPU)",
   "language": "python",
   "name": "new_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
