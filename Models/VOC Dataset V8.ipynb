{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed45f5b-ec83-479e-ba7d-47e5e51a2e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Vstanovlene\\Anaconda Distribution\\envs\\new_gpu_env\\lib\\site-packages\\albumentations\\__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.3 (you have 1.4.23). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import xml.etree.ElementTree as ET\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from utils import IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b6b01d6-bbe7-4abc-843a-1eb93bbad1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('anchors_VOC0712trainval.pickle', 'rb') as handle:\n",
    "    anchors = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1f5da94-e733-4a13-98a9-67fa265a6661",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDatasetV2(Dataset):\n",
    "    def __init__(self, devkit_path, \n",
    "                 subsets = [('VOC2007', 'trainval'), ('VOC2012', 'trainval')], \n",
    "                 anchors = [], scales = [13], \n",
    "                 threshold_ignore_prediction = 0.5,\n",
    "                 transforms = None,\n",
    "                 dtype=None, device=None):\n",
    "        super().__init__()\n",
    "        self.devkit_path = devkit_path\n",
    "        self.subsets = subsets\n",
    "        self.anchors = anchors\n",
    "        self.scales = scales\n",
    "        self.threshold_ignore_prediction = threshold_ignore_prediction\n",
    "        self.transforms = transforms\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "\n",
    "        self.object_placed = 0\n",
    "        self.object_not_placed = 0\n",
    "        self.total = 0\n",
    "\n",
    "        self.all_labels = []\n",
    "        for subset in self.subsets:\n",
    "            subset_path = os.path.join(self.devkit_path, subset[0], 'ImageSets', 'Main', '{}.txt'.format(subset[1]))\n",
    "            print(os.path.exists(subset_path), subset_path)\n",
    "            with open(subset_path, 'r') as file:\n",
    "                subset_labels = file.read().splitlines()\n",
    "            self.all_labels.append(subset_labels)\n",
    "\n",
    "        self.classes = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\n",
    "                        'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train',\n",
    "                        'tvmonitor']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get paths\n",
    "        subset_idx = 0\n",
    "        for subset_labels in self.all_labels:\n",
    "            if idx < len(subset_labels):\n",
    "                break\n",
    "            else:\n",
    "                subset_idx += 1\n",
    "                idx -= len(subset_labels)\n",
    "\n",
    "        if idx < 0 or subset_idx >= len(self.subsets):\n",
    "            raise Exception(\"Index out of range.\")\n",
    "\n",
    "        # print(subset_idx, idx)\n",
    "        image_path = os.path.join(self.devkit_path, self.subsets[subset_idx][0], 'JPEGImages', '{}.jpg'.format(self.all_labels[subset_idx][idx]))\n",
    "        annotation_path = os.path.join(self.devkit_path, self.subsets[subset_idx][0], 'Annotations', '{}.xml'.format(self.all_labels[subset_idx][idx]))\n",
    "\n",
    "        # print(os.path.exists(image_path), image_path)\n",
    "        # print(os.path.exists(annotation_path), annotation_path)\n",
    "\n",
    "        # get PIL image\n",
    "        PIL_img = Image.open(image_path)\n",
    "        \n",
    "        # parse annotations\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        bboxes = []\n",
    "        for item in root.findall('./object'):\n",
    "            bndbox = item.find(\"bndbox\")\n",
    "            xmin = int(bndbox.find(\"xmin\").text)\n",
    "            ymin = int(bndbox.find(\"ymin\").text)\n",
    "            xmax = int(bndbox.find(\"xmax\").text)\n",
    "            ymax = int(bndbox.find(\"ymax\").text)\n",
    "            class_label = self.classes.index(item.find(\"name\").text)\n",
    "\n",
    "            bboxes.append([xmin, ymin, xmax, ymax, class_label])\n",
    "\n",
    "            self.total += 1\n",
    "\n",
    "        if self.transforms:\n",
    "            np_img = np.array(PIL_img.convert(\"RGB\"))\n",
    "            transformed = self.transforms(image=np_img, bboxes=bboxes)\n",
    "            image = transformed['image']\n",
    "            if self.dtype is not None:\n",
    "                image = image.type(self.dtype)\n",
    "            if self.device is not None:\n",
    "                image = image.to(self.device)\n",
    "            img_d, img_h, img_w = image.shape\n",
    "            bboxes = transformed['bboxes']\n",
    "        else:\n",
    "            return PIL_img, bboxes\n",
    "\n",
    "        # initialize tensors\n",
    "        gt_out = [torch.zeros(len(self.anchors)*(5+len(self.classes)), scale, scale, dtype=self.dtype, device=self.device) for scale in self.scales]\n",
    "\n",
    "        for box in bboxes:\n",
    "            xmin, ymin, xmax, ymax, class_label = box\n",
    "            class_label = int(class_label)\n",
    "\n",
    "            # =========== TEST ==========\n",
    "            # print('class_label ', class_label)\n",
    "            # =========== TEST ==========\n",
    "        \n",
    "            obj_w = xmax - xmin\n",
    "            obj_h = ymax - ymin\n",
    "\n",
    "            obj_xc = xmax - obj_w / 2\n",
    "            obj_yc = ymax - obj_h / 2\n",
    "\n",
    "            for scale_idx, scale in enumerate(self.scales):\n",
    "                cell_w = img_w / scale\n",
    "                cell_h = img_h / scale\n",
    "\n",
    "                cell_x = int(obj_xc / cell_w)\n",
    "                cell_y = int(obj_yc / cell_h)\n",
    "\n",
    "                # =========== TEST ==========\n",
    "                # print('cell_x ', cell_x)\n",
    "                # print('cell_y ', cell_y)\n",
    "                # =========== TEST ==========\n",
    "                \n",
    "                \n",
    "                obj_xc = (obj_xc % cell_w) / cell_w\n",
    "                obj_yc = (obj_yc % cell_h) / cell_h\n",
    "\n",
    "                obj_w = obj_w / cell_w\n",
    "                obj_h = obj_h / cell_h\n",
    "\n",
    "                bndbox = torch.tensor([obj_xc, obj_yc, obj_w, obj_h])\n",
    "                \n",
    "                # print((cell_x, cell_y), bndbox, self.classes[class_label])\n",
    "                \n",
    "                IoUs = torch.empty(len(self.anchors))\n",
    "                for i, anchor in enumerate(self.anchors):\n",
    "                    cell_aw = anchor[0] * scale\n",
    "                    cell_ah = anchor[1] * scale\n",
    "                    _anchor = torch.tensor([obj_xc, obj_yc, cell_aw, cell_ah])\n",
    "                    IoUs[i] = IoU(bndbox, _anchor)\n",
    "\n",
    "                anchors_argsort = torch.argsort(IoUs, descending=True)\n",
    "                best_anchor = anchors_argsort[0]\n",
    "\n",
    "                # =========== TEST ==========\n",
    "                # print('best_anchor ', best_anchor)\n",
    "                # =========== TEST ==========\n",
    "                \n",
    "                \n",
    "                placement_0 = best_anchor*(5+len(self.classes))\n",
    "                _objectness = (placement_0, cell_x, cell_y)\n",
    "\n",
    "                taken = gt_out[scale_idx][_objectness] == 1                    \n",
    "                if taken:\n",
    "                    # =========== TEST ==========\n",
    "                    self.object_not_placed += 1\n",
    "                    # =========== TEST ==========\n",
    "                    continue\n",
    "                else:\n",
    "                    gt_out[scale_idx][_objectness] = 1\n",
    "                    gt_out[scale_idx][placement_0+1:placement_0+5, cell_x, cell_y] = bndbox\n",
    "                    \n",
    "                    label_placement = (placement_0 + 1 + 4 + class_label, cell_x, cell_y)\n",
    "                    gt_out[scale_idx][label_placement] = 1\n",
    "\n",
    "                    # =========== TEST ==========\n",
    "                    self.object_placed += 1\n",
    "                    # =========== TEST ==========\n",
    "\n",
    "                # not the best anchors\n",
    "                # =========== TEST ==========\n",
    "                # print(anchors_argsort, IoUs)\n",
    "                # =========== TEST ==========\n",
    "                for anchor_idx in anchors_argsort[1:]:\n",
    "                    if IoUs[anchor_idx] > self.threshold_ignore_prediction:\n",
    "                        placement_0 = anchor_idx*(5+len(self.classes))\n",
    "                        _objectness = (placement_0, cell_x, cell_y)\n",
    "                        if gt_out[scale_idx][_objectness] == 0:\n",
    "                            gt_out[scale_idx][_objectness] = -1\n",
    "                        \n",
    "                    \n",
    "        return (image, gt_out) if len(self.scales) > 1 else (image, gt_out[0])\n",
    "        \n",
    "    def __len__(self):\n",
    "        summed_len = 0\n",
    "        for _subset in self.all_labels:\n",
    "            summed_len += len(_subset)\n",
    "        return summed_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a7e8ba0-3288-4eda-b187-f9852a2319a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDatasetV8(Dataset):\n",
    "    def __init__(self, devkit_path, \n",
    "                 subsets = [('VOC2007', 'trainval'), ('VOC2012', 'trainval')], \n",
    "                 scales = [13], num_boxes=16,\n",
    "                 threshold_ignore_prediction = 0.5,\n",
    "                 transforms = None,\n",
    "                 dtype=None, device=None):\n",
    "        super().__init__()\n",
    "        self.devkit_path = devkit_path\n",
    "        self.subsets = subsets\n",
    "        self.scales = scales\n",
    "        self.num_boxes = num_boxes\n",
    "        self.threshold_ignore_prediction = threshold_ignore_prediction\n",
    "        self.transforms = transforms\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "\n",
    "        self.object_placed = 0\n",
    "        self.object_not_placed = 0\n",
    "        self.total = 0\n",
    "\n",
    "        self.all_labels = []\n",
    "        for subset in self.subsets:\n",
    "            subset_path = os.path.join(self.devkit_path, subset[0], 'ImageSets', 'Main', '{}.txt'.format(subset[1]))\n",
    "            print(os.path.exists(subset_path), subset_path)\n",
    "            with open(subset_path, 'r') as file:\n",
    "                subset_labels = file.read().splitlines()\n",
    "            self.all_labels.append(subset_labels)\n",
    "\n",
    "        self.classes = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\n",
    "                        'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train',\n",
    "                        'tvmonitor']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get paths\n",
    "        subset_idx = 0\n",
    "        for subset_labels in self.all_labels:\n",
    "            if idx < len(subset_labels):\n",
    "                break\n",
    "            else:\n",
    "                subset_idx += 1\n",
    "                idx -= len(subset_labels)\n",
    "\n",
    "        if idx < 0 or subset_idx >= len(self.subsets):\n",
    "            raise Exception(\"Index out of range.\")\n",
    "\n",
    "        # print(subset_idx, idx)\n",
    "        image_path = os.path.join(self.devkit_path, self.subsets[subset_idx][0], 'JPEGImages', '{}.jpg'.format(self.all_labels[subset_idx][idx]))\n",
    "        annotation_path = os.path.join(self.devkit_path, self.subsets[subset_idx][0], 'Annotations', '{}.xml'.format(self.all_labels[subset_idx][idx]))\n",
    "\n",
    "        # print(os.path.exists(image_path), image_path)\n",
    "        # print(os.path.exists(annotation_path), annotation_path)\n",
    "\n",
    "        # get PIL image\n",
    "        PIL_img = Image.open(image_path)\n",
    "        \n",
    "        # parse annotations\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        bboxes = []\n",
    "        for item in root.findall('./object'):\n",
    "            bndbox = item.find(\"bndbox\")\n",
    "            xmin = int(bndbox.find(\"xmin\").text)\n",
    "            ymin = int(bndbox.find(\"ymin\").text)\n",
    "            xmax = int(bndbox.find(\"xmax\").text)\n",
    "            ymax = int(bndbox.find(\"ymax\").text)\n",
    "            class_label = self.classes.index(item.find(\"name\").text)\n",
    "\n",
    "            bboxes.append([xmin, ymin, xmax, ymax, class_label])\n",
    "\n",
    "            self.total += 1\n",
    "\n",
    "        if self.transforms:\n",
    "            np_img = np.array(PIL_img.convert(\"RGB\"))\n",
    "            transformed = self.transforms(image=np_img, bboxes=bboxes)\n",
    "            image = transformed['image']\n",
    "            if self.dtype is not None:\n",
    "                image = image.type(self.dtype)\n",
    "            if self.device is not None:\n",
    "                image = image.to(self.device)\n",
    "            img_d, img_h, img_w = image.shape\n",
    "            bboxes = transformed['bboxes']\n",
    "        else:\n",
    "            return PIL_img, bboxes\n",
    "\n",
    "        # initialize tensors\n",
    "        gt_out = [torch.zeros(self.num_boxes*4+len(self.classes), scale, scale, dtype=self.dtype, device=self.device) for scale in self.scales]\n",
    "        \n",
    "        for box in bboxes:\n",
    "            xmin, ymin, xmax, ymax, class_label = box\n",
    "            class_label = int(class_label)\n",
    "        \n",
    "            obj_w = xmax - xmin\n",
    "            obj_h = ymax - ymin\n",
    "\n",
    "            obj_xc = xmax - obj_w / 2\n",
    "            obj_yc = ymax - obj_h / 2\n",
    "\n",
    "            for scale_idx, scale in enumerate(self.scales):\n",
    "                cell_w = img_w / scale\n",
    "                cell_h = img_h / scale\n",
    "\n",
    "                cell_x = int(obj_xc / cell_w)\n",
    "                cell_y = int(obj_yc / cell_h)\n",
    "                \n",
    "                obj_xc = (obj_xc % cell_w) / cell_w\n",
    "                obj_yc = (obj_yc % cell_h) / cell_h\n",
    "\n",
    "                obj_w = obj_w / cell_w\n",
    "                obj_h = obj_h / cell_h\n",
    "\n",
    "                bndbox = torch.tensor([obj_xc, obj_yc, obj_w, obj_h])\n",
    "                \n",
    "                taken_cls = torch.argmax(gt_out[scale_idx][self.num_boxes*4:, cell_x, cell_y])\n",
    "                taken_cls = taken_cls if gt_out[scale_idx][self.num_boxes*4:, cell_x, cell_y].sum() > 0 else None\n",
    "                if taken_cls is None or taken_cls == class_label:\n",
    "                    for i in range(self.num_boxes):\n",
    "                        placement_0 = i*4\n",
    "        \n",
    "                        taken = gt_out[scale_idx][placement_0:placement_0+4, cell_x, cell_y].sum() != 0\n",
    "                        if taken:\n",
    "                            if i == self.num_boxes-1:\n",
    "                                self.object_not_placed += 1\n",
    "                            continue   \n",
    "                        else:\n",
    "                            self.object_placed += 1\n",
    "                            gt_out[scale_idx][placement_0:placement_0+4, cell_x, cell_y] = bndbox\n",
    "                            if taken_cls is None:\n",
    "                                label_placement = (\n",
    "                                    self.num_boxes*4 + class_label, cell_x, cell_y\n",
    "                                )\n",
    "                                gt_out[scale_idx][label_placement] = 1\n",
    "                            break\n",
    "                else:\n",
    "                    self.object_not_placed += 1\n",
    "                        \n",
    "                    \n",
    "        return (image, gt_out) if len(self.scales) > 1 else (image, gt_out[0])\n",
    "        \n",
    "    def __len__(self):\n",
    "        summed_len = 0\n",
    "        for _subset in self.all_labels:\n",
    "            summed_len += len(_subset)\n",
    "        return summed_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f83fc97b-cf3a-46ac-b53f-e6dc384b4252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ../../datasets/VOCdevkit/VOC2007\\ImageSets\\Main\\trainval.txt\n",
      "True ../../datasets/VOCdevkit/VOC2012\\ImageSets\\Main\\trainval.txt\n"
     ]
    }
   ],
   "source": [
    "transforms = A.Compose([\n",
    "    A.Resize(width=224, height=224),\n",
    "    A.VerticalFlip(p=1.0),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc'))\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "dtype=torch.float32\n",
    "\n",
    "# train_set = VOCDatasetV8(devkit_path = '../../datasets/VOCdevkit/', scales=[80, 40, 20], transforms=transforms, device=device, dtype=dtype)\n",
    "train_set = VOCDatasetV2(devkit_path = '../../datasets/VOCdevkit/', scales=[80, 40, 20], anchors=anchors, transforms=transforms, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d595f9d2-3112-49ca-bfa1-f287a023c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a47964e-4ac9-4bde-9cc1-8ba06de70e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in train_loader:\n",
    "    del img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5bb74f8-60c1-4d1d-aa2f-0d03b67b376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximately 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa6923fe-c635-424c-87a4-28bae097c64d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85639, 56030, 141669, 47223)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.object_placed, train_set.object_not_placed, train_set.object_placed + train_set.object_not_placed, train_set.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5db5dbc0-f927-445a-a1b9-a2c5ee91687c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141669"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "47223*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10ed3264-7acf-4a80-a1fa-6a7ad9e9eafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16551"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69e780e9-8f14-49cf-a216-70b59e9b751a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ../../datasets/VOCdevkit/VOC2007\\ImageSets\\Main\\trainval.txt\n",
      "True ../../datasets/VOCdevkit/VOC2012\\ImageSets\\Main\\trainval.txt\n"
     ]
    }
   ],
   "source": [
    "transforms = A.Compose([\n",
    "    A.Resize(width=224, height=224),\n",
    "    A.VerticalFlip(p=1.0),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc'))\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "dtype=torch.float32\n",
    "\n",
    "train_set = VOCDatasetV8(devkit_path = '../../datasets/VOCdevkit/', scales=[80, 40, 20], transforms=transforms, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f981905-a494-4b98-bcfe-248ec80af50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4ccca6a-7601-4734-89e8-7178aa9cafbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in train_loader:\n",
    "    del img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "daaf28cb-6021-4a65-8401-04f121629f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105516, 36153, 141669, 47223)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.object_placed, train_set.object_not_placed, train_set.object_placed + train_set.object_not_placed, train_set.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e1c8d4e-113b-4f6d-b3e8-f74197a1c51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141669"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "47223*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea322f8-4eeb-4329-9e79-ec86c9a49e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "New Python (GPU)",
   "language": "python",
   "name": "new_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
