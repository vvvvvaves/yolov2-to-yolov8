{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c049b8c1-dcc6-4948-b9df-356c778e7983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b1a5950-5470-4fe4-ad9d-12ba35b340e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "from Models.block import Conv\n",
    "from torch.autograd.profiler import record_function\n",
    "from evaluation import count_parameters\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e78180f-56fb-40bd-9345-ceb91548aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv2(torch.nn.Module):\n",
    "    def __init__(self, device=None, dtype=None, num_classes=20, \n",
    "                 anchors=[(1.3221, 1.73145), (3.19275, 4.00944), (5.05587, 8.09892), (9.47112, 4.84053),\n",
    "                          (11.2364, 10.0071)],\n",
    "                 num_boxes=5, act='Leaky'):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.anchors = anchors\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv1 = Conv(3, out_channels=32, kernel_size=(3,3), stride=(1,1), \n",
    "                          padding=(1,1), bias=False, act=act,\n",
    "                          device=device, dtype=dtype)\n",
    "        self.conv2 = Conv(32, out_channels=64, kernel_size=(3,3), stride=(1,1), \n",
    "                          padding=(1,1), bias=False, act=act,\n",
    "                          device=device, dtype=dtype)\n",
    "\n",
    "        self.seq3_5 = nn.Sequential(OrderedDict([\n",
    "            ('conv3', Conv(64, out_channels=128, kernel_size=(3,3), stride=(1,1), \n",
    "                          padding=(1,1), bias=False, act=act,\n",
    "                          device=device, dtype=dtype)),\n",
    "            ('conv4', Conv(128, out_channels=64, kernel_size=(1,1), stride=(1,1), \n",
    "                          padding=(0,0), bias=False, act=act,\n",
    "                          device=device, dtype=dtype)),\n",
    "            ('conv5', Conv(64, out_channels=128, kernel_size=(3,3), stride=(1,1), \n",
    "                          padding=(1,1), bias=False, act=act,\n",
    "                          device=device, dtype=dtype))\n",
    "        ]))\n",
    "        \n",
    "        self.seq6_8 = nn.Sequential(OrderedDict([\n",
    "            ('conv6', Conv(128, out_channels=256, kernel_size=(3,3), stride=(1,1), \n",
    "                          padding=(1,1), bias=False, act=act,\n",
    "                          device=device, dtype=dtype)),\n",
    "            ('conv7', Conv(256, out_channels=128, kernel_size=(1,1), stride=(1,1), \n",
    "                          padding=(0,0), bias=False, act=act,\n",
    "                          device=device, dtype=dtype)),\n",
    "            ('conv8', Conv(128, out_channels=256, kernel_size=(3,3), stride=(1,1), \n",
    "                          padding=(1,1), bias=False, act=act,\n",
    "                          device=device, dtype=dtype))\n",
    "        ]))\n",
    "\n",
    "        self.seq9_13 = nn.Sequential(OrderedDict([\n",
    "            ('conv9', Conv(256, out_channels=512, kernel_size=(3,3), stride=(1,1), \n",
    "                          padding=(1,1), bias=False, act=act,\n",
    "                          device=device, dtype=dtype)),\n",
    "            ('conv10', Conv(512, out_channels=256, kernel_size=(1,1), stride=(1,1), \n",
    "                          padding=(0,0), bias=False, act=act,\n",
    "                          device=device, dtype=dtype)),\n",
    "            ('conv11', Conv(256, out_channels=512, kernel_size=(3,3), stride=(1,1), \n",
    "                          padding=(1,1), bias=False, act=act,\n",
    "                          device=device, dtype=dtype)),\n",
    "            ('conv12', Conv(512, out_channels=256, kernel_size=(1,1), stride=(1,1), \n",
    "                          padding=(0,0), bias=False, act=act,\n",
    "                          device=device, dtype=dtype)),\n",
    "            ('conv13', Conv(256, out_channels=512, kernel_size=(3,3), stride=(1,1), \n",
    "                          padding=(1,1), bias=False, act=act,\n",
    "                          device=device, dtype=dtype))\n",
    "        ]))\n",
    "\n",
    "        # route -->\n",
    "\n",
    "        self.seq14_18 = nn.Sequential(OrderedDict([\n",
    "            ('conv14', Conv(512, out_channels=1024, kernel_size=(3,3), stride=(1,1), \n",
    "                          padding=(1,1), bias=False, act=act,\n",
    "                          device=device, dtype=dtype)),\n",
    "            ('conv15', Conv(1024, out_channels=512, kernel_size=(1,1), stride=(1,1), \n",
    "                          padding=(0,0), bias=False, act=act,\n",
    "                          device=device, dtype=dtype)),\n",
    "            ('conv16', Conv(512, out_channels=1024, kernel_size=(3,3), stride=(1,1), \n",
    "                          padding=(1,1), bias=False, act=act,\n",
    "                          device=device, dtype=dtype)),\n",
    "            ('conv17', Conv(1024, out_channels=512, kernel_size=(1,1), stride=(1,1), \n",
    "                          padding=(0,0), bias=False, act=act,\n",
    "                          device=device, dtype=dtype)),\n",
    "            ('conv18', Conv(512, out_channels=1024, kernel_size=(3,3), stride=(1,1), \n",
    "                          padding=(1,1), bias=False, act=act,\n",
    "                          device=device, dtype=dtype))\n",
    "        ]))\n",
    "\n",
    "        # Detection part\n",
    "\n",
    "        self.seq19_20 = nn.Sequential(OrderedDict([\n",
    "            ('conv19', Conv(1024, out_channels=1024, kernel_size=(3,3), stride=(1,1), \n",
    "                          padding=(1,1), bias=False, act=act,\n",
    "                          device=device, dtype=dtype)),\n",
    "            ('conv20', Conv(1024, out_channels=1024, kernel_size=(3,3), stride=(1,1), \n",
    "                          padding=(1,1), bias=False, act=act,\n",
    "                          device=device, dtype=dtype)),\n",
    "            \n",
    "        ]))\n",
    "\n",
    "        # --> route\n",
    "\n",
    "        self.passthrough_conv = Conv(512, out_channels=64, kernel_size=(1,1), stride=(1,1), \n",
    "                                      padding=(0,0), bias=False, act=act,\n",
    "                                      device=device, dtype=dtype)\n",
    "\n",
    "        self.seq21_22 = nn.Sequential(OrderedDict([\n",
    "            ('conv21', Conv(256+1024, out_channels=1024, kernel_size=(3,3), stride=(1,1), \n",
    "                          padding=(1,1), bias=False, act=act,\n",
    "                          device=device, dtype=dtype)),\n",
    "            ('conv22', Conv(1024, out_channels=num_boxes*(num_classes+5), kernel_size=(1,1), stride=(1,1), \n",
    "                          padding=(0,0), bias=False, act=act,\n",
    "                          device=device, dtype=dtype))\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 416x416, stride: 0\n",
    "        out = self.conv1(x)\n",
    "        with record_function(\"Max pooling\"):\n",
    "            out = self.max_pool(out)\n",
    "\n",
    "        # 208x208, stride: 2\n",
    "        out = self.conv2(out)\n",
    "        with record_function(\"Max pooling\"):\n",
    "            out = self.max_pool(out)\n",
    "\n",
    "        # 104x104, stride: 4\n",
    "        out = self.seq3_5(out)\n",
    "        with record_function(\"Max pooling\"):\n",
    "            out = self.max_pool(out)\n",
    "\n",
    "        # 52x52, stride: 8\n",
    "        out = self.seq6_8(out)\n",
    "        with record_function(\"Max pooling\"):\n",
    "            out = self.max_pool(out)\n",
    "\n",
    "        # 26x26, stride: 16\n",
    "        out = self.seq9_13(out)\n",
    "        passthrough_out = self.passthrough_conv(out).reshape((-1, 256, 13, 13))\n",
    "        \n",
    "        with record_function(\"Max pooling\"):\n",
    "            out = self.max_pool(out)\n",
    "\n",
    "        # 13x13, stride: 32\n",
    "        out = self.seq14_18(out)\n",
    "\n",
    "        # Detection part\n",
    "\n",
    "        out = self.seq19_20(out)\n",
    "        out = torch.cat([passthrough_out, out], 1)\n",
    "        out = self.seq21_22(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a793c793-912c-481f-ad07-71a6e41c8d29",
   "metadata": {},
   "source": [
    "https://github.com/yjh0410/yolov2-yolov3_PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14955e7d-b73d-40fc-b695-1be9470d8d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_BN_LeakyReLU(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ksize, padding=0, stride=1, dilation=1):\n",
    "        super(Conv_BN_LeakyReLU, self).__init__()\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, ksize, padding=padding, stride=stride, dilation=dilation, device=torch.device('cuda:0')),\n",
    "            nn.BatchNorm2d(out_channels, device=torch.device('cuda:0')),\n",
    "            nn.LeakyReLU(0.1, inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.convs(x)\n",
    "\n",
    "class reorg_layer(nn.Module):\n",
    "    def __init__(self, stride):\n",
    "        super(reorg_layer, self).__init__()\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        _height, _width = height // self.stride, width // self.stride\n",
    "        \n",
    "        x = x.view(batch_size, channels, _height, self.stride, _width, self.stride).transpose(3, 4).contiguous()\n",
    "        x = x.view(batch_size, channels, _height * _width, self.stride * self.stride).transpose(2, 3).contiguous()\n",
    "        x = x.view(batch_size, channels, self.stride * self.stride, _height, _width).transpose(1, 2).contiguous()\n",
    "        x = x.view(batch_size, -1, _height, _width)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DarkNet_19(nn.Module):\n",
    "    def __init__(self):        \n",
    "        super(DarkNet_19, self).__init__()\n",
    "        # backbone network : DarkNet-19\n",
    "        # output : stride = 2, c = 32\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            Conv_BN_LeakyReLU(3, 32, 3, 1),\n",
    "            nn.MaxPool2d((2,2), 2),\n",
    "        )\n",
    "\n",
    "        # output : stride = 4, c = 64\n",
    "        self.conv_2 = nn.Sequential(\n",
    "            Conv_BN_LeakyReLU(32, 64, 3, 1),\n",
    "            nn.MaxPool2d((2,2), 2)\n",
    "        )\n",
    "\n",
    "        # output : stride = 8, c = 128\n",
    "        self.conv_3 = nn.Sequential(\n",
    "            Conv_BN_LeakyReLU(64, 128, 3, 1),\n",
    "            Conv_BN_LeakyReLU(128, 64, 1),\n",
    "            Conv_BN_LeakyReLU(64, 128, 3, 1),\n",
    "            nn.MaxPool2d((2,2), 2)\n",
    "        )\n",
    "\n",
    "        # output : stride = 8, c = 256\n",
    "        self.conv_4 = nn.Sequential(\n",
    "            Conv_BN_LeakyReLU(128, 256, 3, 1),\n",
    "            Conv_BN_LeakyReLU(256, 128, 1),\n",
    "            Conv_BN_LeakyReLU(128, 256, 3, 1),\n",
    "        )\n",
    "\n",
    "        # output : stride = 16, c = 512\n",
    "        self.maxpool_4 = nn.MaxPool2d((2, 2), 2)\n",
    "        self.conv_5 = nn.Sequential(\n",
    "            Conv_BN_LeakyReLU(256, 512, 3, 1),\n",
    "            Conv_BN_LeakyReLU(512, 256, 1),\n",
    "            Conv_BN_LeakyReLU(256, 512, 3, 1),\n",
    "            Conv_BN_LeakyReLU(512, 256, 1),\n",
    "            Conv_BN_LeakyReLU(256, 512, 3, 1),\n",
    "        )\n",
    "        \n",
    "        # output : stride = 32, c = 1024\n",
    "        self.maxpool_5 = nn.MaxPool2d((2, 2), 2)\n",
    "        self.conv_6 = nn.Sequential(\n",
    "            Conv_BN_LeakyReLU(512, 1024, 3, 1),\n",
    "            Conv_BN_LeakyReLU(1024, 512, 1),\n",
    "            Conv_BN_LeakyReLU(512, 1024, 3, 1),\n",
    "            Conv_BN_LeakyReLU(1024, 512, 1),\n",
    "            Conv_BN_LeakyReLU(512, 1024, 3, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1 = self.conv_1(x)\n",
    "        c2 = self.conv_2(c1)\n",
    "        c3 = self.conv_3(c2)\n",
    "        c3 = self.conv_4(c3)\n",
    "        c4 = self.conv_5(self.maxpool_4(c3))\n",
    "        c5 = self.conv_6(self.maxpool_5(c4))\n",
    "\n",
    "        output = {\n",
    "            'layer1': c3,\n",
    "            'layer2': c4,\n",
    "            'layer3': c5\n",
    "        }\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "55160252-81a7-41fa-a475-63528a959fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv2D19(nn.Module):\n",
    "    def __init__(self, num_classes=20, num_anchors=5, state_dict_path='./darknet19_72.96.pth', device=None, dtype=None):\n",
    "        super(YOLOv2D19, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = num_anchors\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # Load pretrained backbone\n",
    "        state_dict = torch.load(state_dict_path, map_location='cuda:0')\n",
    "        del state_dict['conv_7.weight']\n",
    "        del state_dict['conv_7.bias']\n",
    "\n",
    "        self.backbone = DarkNet_19()\n",
    "        self.backbone.load_state_dict(state_dict)\n",
    "        \n",
    "        # detection head\n",
    "        self.convsets_1 = nn.Sequential(\n",
    "            Conv_BN_LeakyReLU(1024, 1024, 3, 1),\n",
    "            Conv_BN_LeakyReLU(1024, 1024, 3, 1)\n",
    "        )\n",
    "\n",
    "        self.route_layer = Conv_BN_LeakyReLU(512, 64, 1)\n",
    "        self.reorg = reorg_layer(stride=2)\n",
    "\n",
    "        self.convsets_2 = Conv_BN_LeakyReLU(1280, 1024, 3, 1)\n",
    "        \n",
    "        # prediction layer\n",
    "        self.pred = nn.Conv2d(1024, self.num_anchors*(1 + 4 + self.num_classes), kernel_size=1, device=torch.device('cuda:0'))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # backbone\n",
    "        feats = self.backbone(x)\n",
    "\n",
    "        # reorg layer\n",
    "        p5 = self.convsets_1(feats['layer3'])\n",
    "        p4 = self.reorg(self.route_layer(feats['layer2']))\n",
    "        p5 = torch.cat([p4, p5], dim=1)\n",
    "\n",
    "        # head\n",
    "        p5 = self.convsets_2(p5)\n",
    "\n",
    "        # pred\n",
    "        pred = self.pred(p5)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8a1909f-c58a-48b1-a30f-9c0e9691aad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\conda_tmp\\ipykernel_1656\\1069960794.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path, map_location='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = YOLOv2D19()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23d8c6a3-c89f-465a-8337-f666f62287da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.loss import *\n",
    "from Models.voc_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "95112fcc-42f8-4ac9-ba03-8cc5e36566b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    items = list(zip(*batch))\n",
    "    items[0] = default_collate(items[0])\n",
    "    items[1] = list(items[1])\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a795926b-72ff-46ab-9e0c-ba94d39324fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = VOCDataset(root_path='../../datasets/VOCdevkit', year=\"2007\", mode=\"train\", image_size=416, is_training=True)\n",
    "train_loader = DataLoader(train_set, batch_size=2, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_set = VOCDataset(root_path='../../datasets/VOCdevkit', year=\"2007\", mode=\"val\", image_size=416, is_training=False)\n",
    "test_loader = DataLoader(train_set, batch_size=2, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f29a6918-067e-4931-995a-4732ab45ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = YoloLoss(train_set.num_classes, anchors=[(1.3221, 1.73145), (3.19275, 4.00944), (5.05587, 8.09892), (9.47112, 4.84053),\n",
    "                          (11.2364, 10.0071)], reduction=32, device=torch.device('cuda:0'), dtype=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6bf2aeb2-48f6-4ba3-bff0-bd10df722c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9064472b-92b9-4a43-86e2-87db2b56bbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Loss: 89.18915557861328\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "loss_train = 0.0\n",
    "\n",
    "for i, (imgs, labels) in enumerate(train_loader):\n",
    "    imgs = imgs.to('cuda:0')\n",
    "    #labels = labels.to('cuda:0')\n",
    "    \n",
    "    if (i+1) % 15 == 0:\n",
    "        _datetime = datetime.datetime.now()\n",
    "        print(f\"{_datetime} Batch {i+1} \")\n",
    "    \n",
    "    outputs = model(imgs)\n",
    "\n",
    "    loss, loss_coord, loss_conf, loss_cls, _dict = loss_fn(outputs, labels)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_train += loss.item()\n",
    "    break\n",
    "\n",
    "n_batches = len(train_loader)\n",
    "print(f'[Train] Loss: {loss_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d5e5bc12-8813-4f8a-b068-05a20caf6ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 4, 169]), torch.Size([2, 5, 4, 169]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dict['coord'].shape, _dict['tcoord'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "068c5a36-a716-4f01-a6f5-9fdec13d8d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N, boxes, coords, 13x13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "445834d0-85d9-4d25-932e-dfac48a04e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9, 20]), torch.Size([9]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dict['cls'].shape, _dict['tcls'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c8285898-fbe5-4ed6-a388-bd1ed6b911e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5, 169]), torch.Size([2, 5, 169]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dict['conf'].shape, _dict['tconf'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0a78da3c-6a6a-487f-8179-a606d15d6c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 4, 169])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dict['coord_mask'].shape # zeroes out no_obj ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "af4cfcfa-c014-4086-85fd-4c80f4b7fd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 169])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dict['conf_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "69517608-ccd6-4530-b890-ae713580e55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[141.20325,  88.56023, 147.12195, 327.4398 ,  11.     ],\n",
       "        [  0.     ,   0.     , 211.38211, 435.8095 ,  14.     ]],\n",
       "       dtype=float32),\n",
       " array([[ 65.743866,  46.92408 , 258.4414  , 276.13016 ,   7.      ],\n",
       "        [120.15259 , 112.79826 , 295.8474  , 185.89154 ,  14.      ]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xmin, ymin, xmax, ymax, label\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d5af1d-ad46-4e24-9077-1648aa12530f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
