{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04833633-3c8f-4d84-8869-c36d9a0cd69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a0a4b-7479-40b1-abdd-1227b6940040",
   "metadata": {},
   "source": [
    "https://lernapparat.de/debug-device-assert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5258e5c-2fd1-45d6-bf25-96674d504f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9deb7c5-1256-4fdd-b447-69f25786c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43b63379-9363-448e-b927-2231dbb097b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import *\n",
    "from data_preprocessing import *\n",
    "from data_augmentation import *\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from Models.yolov8cls_path import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbdae16b-4251-46a6-865b-bfa81a9a5ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9a44f5d-8396-4654-b118-82473d81ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_classes=10, \n",
    "              residual_connection=True, \n",
    "              CSP=True, \n",
    "              add_hidden=False,\n",
    "              classifyV8=False,\n",
    "              bottleneck=1.0, \n",
    "              variant='s', \n",
    "              device=device, \n",
    "              dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae3c5dd0-ac5d-4cbb-8463-d9ebe0621f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../../../datasets/imagenette2/'\n",
    "norms_path = os.path.join(data_path, 'norms.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e608e517-6da7-41c7-b62d-81cca6aecf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means are: [0.44969913363456726, 0.44868946075439453, 0.45163223147392273]\n",
      "stds are: [0.28648287057876587, 0.28796446323394775, 0.2865694761276245]\n"
     ]
    }
   ],
   "source": [
    "means = get_means(path=norms_path, train_loader=None)\n",
    "stds = get_stds(path=norms_path, train_loader=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c4a012-f83a-495c-929b-939db6549642",
   "metadata": {},
   "source": [
    "\n",
    "Profiling your personal module \n",
    "https://pytorch.org/tutorials/beginner/profiler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb61344e-ef51-4068-9304-62ac2f686ae7",
   "metadata": {},
   "source": [
    "https://discuss.pytorch.org/t/how-to-prevent-overfitting/1902\n",
    "Right now, with my augmented dataset, at epoch 8, I am getting a testset Top1 accuracy of 45% but a trainset Top1 accuracy of 69%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4a702f-1ae5-4573-a299-a90db5b87b7f",
   "metadata": {},
   "source": [
    "You should strongly consider data augmentation in some meaningful way. If youâ€™re attempting to do classification then think about what augmentations might add useful information and help distinguish classes in your dataset. In one of my cases, introducing background variation increased recognition rate by over 50%. Basically, with small datasets there is too much overfitting so you want the network to learn real-world distinctions vs. irrelevant artifacts like backgrounds / shadows etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d4f81f6-3dfe-4256-8c57-66f92768ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = transforms.Compose([transforms.RandomResizedCrop((224, 224)),\n",
    "                                              Augmentation(),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize(mean=means, std=stds)])\n",
    "transformations_val = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize(mean=means, std=stds)\n",
    "                                                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f555ed4-dfcf-4f5f-9a71-3e40a7ea2521",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageNetSubset(path=data_path, train=True, transform=transformations, half=False, show=False)\n",
    "val_dataset = ImageNetSubset(path=data_path, train=False, transform=transformations_val, half=False, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ae476e6-947b-46a5-bed5-729af9ee6bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab049c64-2683-432e-acd7-e3494df498f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3846504-a142-444a-85cb-d8ab411cc449",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fb7d049-0696-4341-991c-9b3bfbb0ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daee47e0-df3f-4342-b462-ef2104f8a7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-08 20:08:46.531739 Epoch 1 \n",
      "2024-12-08 20:09:00.325095 Batch 15 \n",
      "2024-12-08 20:09:03.125328 Batch 30 \n",
      "2024-12-08 20:09:05.604406 Batch 45 \n",
      "2024-12-08 20:09:07.950189 Batch 60 \n",
      "2024-12-08 20:09:10.235485 Batch 75 \n",
      "2024-12-08 20:09:12.299925 Batch 90 \n",
      "2024-12-08 20:09:14.725019 Batch 105 \n",
      "2024-12-08 20:09:17.279288 Batch 120 \n",
      "2024-12-08 20:09:19.343465 Batch 135 \n",
      "[Train] Accuracy: 24.6911%, Loss per batch: 2.0698\n",
      "2024-12-08 20:09:34.183245 Batch 15 \n",
      "2024-12-08 20:09:35.827190 Batch 30 \n",
      "2024-12-08 20:09:37.632745 Batch 45 \n",
      "2024-12-08 20:09:39.234703 Batch 60 \n",
      "[Val] Accuracy: 33.9363%, loss per batch: 1.8975\n",
      "Epoch 1: SGD lr 0.0100 -> 0.0090\n",
      "2024-12-08 20:09:40.282487 Epoch 2 \n",
      "2024-12-08 20:09:53.168282 Batch 15 \n",
      "2024-12-08 20:09:55.617394 Batch 30 \n",
      "2024-12-08 20:09:58.118986 Batch 45 \n",
      "2024-12-08 20:10:00.410654 Batch 60 \n",
      "2024-12-08 20:10:02.904682 Batch 75 \n",
      "2024-12-08 20:10:05.589794 Batch 90 \n",
      "2024-12-08 20:10:08.007283 Batch 105 \n",
      "2024-12-08 20:10:10.008614 Batch 120 \n",
      "2024-12-08 20:10:12.410717 Batch 135 \n",
      "[Train] Accuracy: 36.9099%, Loss per batch: 1.7823\n",
      "2024-12-08 20:10:27.354476 Batch 15 \n",
      "2024-12-08 20:10:28.862172 Batch 30 \n",
      "2024-12-08 20:10:30.854056 Batch 45 \n",
      "2024-12-08 20:10:32.343803 Batch 60 \n",
      "[Val] Accuracy: 40.4331%, loss per batch: 1.7502\n",
      "Epoch 2: SGD lr 0.0090 -> 0.0081\n",
      "2024-12-08 20:10:33.271784 Epoch 3 \n",
      "2024-12-08 20:10:46.573941 Batch 15 \n",
      "2024-12-08 20:10:48.888851 Batch 30 \n",
      "2024-12-08 20:10:51.270451 Batch 45 \n",
      "2024-12-08 20:10:53.607450 Batch 60 \n",
      "2024-12-08 20:10:56.068185 Batch 75 \n",
      "2024-12-08 20:10:58.523929 Batch 90 \n",
      "2024-12-08 20:11:00.944995 Batch 105 \n",
      "2024-12-08 20:11:03.367639 Batch 120 \n",
      "2024-12-08 20:11:05.883475 Batch 135 \n",
      "[Train] Accuracy: 42.8345%, Loss per batch: 1.6424\n",
      "2024-12-08 20:11:21.008795 Batch 15 \n",
      "2024-12-08 20:11:22.682952 Batch 30 \n",
      "2024-12-08 20:11:24.739123 Batch 45 \n",
      "2024-12-08 20:11:26.448166 Batch 60 \n",
      "[Val] Accuracy: 47.9745%, loss per batch: 1.547\n",
      "Epoch 3: SGD lr 0.0081 -> 0.0073\n",
      "2024-12-08 20:11:27.350613 Epoch 4 \n",
      "2024-12-08 20:11:40.417695 Batch 15 \n",
      "2024-12-08 20:11:43.048481 Batch 30 \n",
      "2024-12-08 20:11:45.667587 Batch 45 \n",
      "2024-12-08 20:11:47.838785 Batch 60 \n",
      "2024-12-08 20:11:50.275913 Batch 75 \n",
      "2024-12-08 20:11:52.839459 Batch 90 \n",
      "2024-12-08 20:11:55.111977 Batch 105 \n",
      "2024-12-08 20:11:57.835936 Batch 120 \n",
      "2024-12-08 20:12:00.293304 Batch 135 \n",
      "[Train] Accuracy: 48.3789%, Loss per batch: 1.5198\n",
      "2024-12-08 20:12:15.755824 Batch 15 \n",
      "2024-12-08 20:12:17.410665 Batch 30 \n",
      "2024-12-08 20:12:19.516727 Batch 45 \n",
      "2024-12-08 20:12:21.118105 Batch 60 \n",
      "[Val] Accuracy: 51.2866%, loss per batch: 1.4184\n",
      "Epoch 4: SGD lr 0.0073 -> 0.0066\n",
      "2024-12-08 20:12:22.035318 Epoch 5 \n",
      "2024-12-08 20:12:35.319658 Batch 15 \n",
      "2024-12-08 20:12:37.735056 Batch 30 \n",
      "2024-12-08 20:12:40.326632 Batch 45 \n",
      "2024-12-08 20:12:42.707430 Batch 60 \n",
      "2024-12-08 20:12:45.696812 Batch 75 \n",
      "2024-12-08 20:12:48.131713 Batch 90 \n",
      "2024-12-08 20:12:50.368155 Batch 105 \n",
      "2024-12-08 20:12:53.199926 Batch 120 \n",
      "2024-12-08 20:12:55.741724 Batch 135 \n",
      "[Train] Accuracy: 50.3221%, Loss per batch: 1.4485\n",
      "2024-12-08 20:13:11.002785 Batch 15 \n",
      "2024-12-08 20:13:12.618094 Batch 30 \n",
      "2024-12-08 20:13:14.657669 Batch 45 \n",
      "2024-12-08 20:13:16.376544 Batch 60 \n",
      "[Val] Accuracy: 56.8662%, loss per batch: 1.3248\n",
      "Epoch 5: SGD lr 0.0066 -> 0.0059\n",
      "2024-12-08 20:13:17.406327 Epoch 6 \n",
      "2024-12-08 20:13:30.581928 Batch 15 \n",
      "2024-12-08 20:13:33.422872 Batch 30 \n",
      "2024-12-08 20:13:35.846217 Batch 45 \n",
      "2024-12-08 20:13:38.201496 Batch 60 \n",
      "2024-12-08 20:13:40.820096 Batch 75 \n",
      "2024-12-08 20:13:43.824323 Batch 90 \n",
      "2024-12-08 20:13:46.480474 Batch 105 \n",
      "2024-12-08 20:13:48.978867 Batch 120 \n",
      "2024-12-08 20:13:52.127115 Batch 135 \n",
      "[Train] Accuracy: 54.1134%, Loss per batch: 1.3539\n",
      "2024-12-08 20:14:09.526128 Batch 15 \n",
      "2024-12-08 20:14:11.315225 Batch 30 \n",
      "2024-12-08 20:14:13.503104 Batch 45 \n",
      "2024-12-08 20:14:15.252330 Batch 60 \n",
      "[Val] Accuracy: 57.3503%, loss per batch: 1.2683\n",
      "Epoch 6: SGD lr 0.0059 -> 0.0053\n",
      "2024-12-08 20:14:16.271744 Epoch 7 \n",
      "2024-12-08 20:14:31.156512 Batch 15 \n",
      "2024-12-08 20:14:33.949000 Batch 30 \n",
      "2024-12-08 20:14:37.103891 Batch 45 \n",
      "2024-12-08 20:14:39.568522 Batch 60 \n",
      "2024-12-08 20:14:42.511394 Batch 75 \n",
      "2024-12-08 20:14:45.459867 Batch 90 \n",
      "2024-12-08 20:14:48.324759 Batch 105 \n",
      "2024-12-08 20:14:50.801776 Batch 120 \n",
      "2024-12-08 20:14:53.848613 Batch 135 \n",
      "[Train] Accuracy: 55.8982%, Loss per batch: 1.3092\n",
      "2024-12-08 20:15:11.250108 Batch 15 \n",
      "2024-12-08 20:15:13.589487 Batch 30 \n",
      "2024-12-08 20:15:16.115267 Batch 45 \n",
      "2024-12-08 20:15:18.192395 Batch 60 \n",
      "[Val] Accuracy: 59.4395%, loss per batch: 1.2209\n",
      "Epoch 7: SGD lr 0.0053 -> 0.0048\n",
      "2024-12-08 20:15:19.222923 Epoch 8 \n",
      "2024-12-08 20:15:35.373138 Batch 15 \n",
      "2024-12-08 20:15:38.101702 Batch 30 \n",
      "2024-12-08 20:15:40.838746 Batch 45 \n",
      "2024-12-08 20:15:43.395733 Batch 60 \n",
      "2024-12-08 20:15:46.092310 Batch 75 \n",
      "2024-12-08 20:15:48.655308 Batch 90 \n",
      "2024-12-08 20:15:51.246755 Batch 105 \n",
      "2024-12-08 20:15:53.582790 Batch 120 \n",
      "2024-12-08 20:15:56.233172 Batch 135 \n",
      "[Train] Accuracy: 57.9259%, Loss per batch: 1.2557\n",
      "2024-12-08 20:16:13.120942 Batch 15 \n",
      "2024-12-08 20:16:15.001399 Batch 30 \n",
      "2024-12-08 20:16:17.214251 Batch 45 \n",
      "2024-12-08 20:16:18.851056 Batch 60 \n",
      "[Val] Accuracy: 57.4268%, loss per batch: 1.2513\n",
      "Epoch 8: SGD lr 0.0048 -> 0.0043\n",
      "2024-12-08 20:16:19.790352 Epoch 9 \n",
      "2024-12-08 20:16:33.307169 Batch 15 \n",
      "2024-12-08 20:16:35.924939 Batch 30 \n",
      "2024-12-08 20:16:38.391257 Batch 45 \n",
      "2024-12-08 20:16:40.853888 Batch 60 \n",
      "2024-12-08 20:16:43.673008 Batch 75 \n",
      "2024-12-08 20:16:46.721383 Batch 90 \n",
      "2024-12-08 20:16:49.442191 Batch 105 \n",
      "2024-12-08 20:16:51.979190 Batch 120 \n",
      "2024-12-08 20:16:54.670373 Batch 135 \n",
      "[Train] Accuracy: 60.3443%, Loss per batch: 1.1998\n",
      "2024-12-08 20:17:10.360391 Batch 15 \n",
      "2024-12-08 20:17:12.062224 Batch 30 \n",
      "2024-12-08 20:17:14.185956 Batch 45 \n",
      "2024-12-08 20:17:15.808979 Batch 60 \n",
      "[Val] Accuracy: 66.5987%, loss per batch: 1.0155\n",
      "Epoch 9: SGD lr 0.0043 -> 0.0039\n",
      "2024-12-08 20:17:16.864992 Epoch 10 \n",
      "2024-12-08 20:17:30.395946 Batch 15 \n",
      "2024-12-08 20:17:33.212943 Batch 30 \n",
      "2024-12-08 20:17:35.994530 Batch 45 \n",
      "2024-12-08 20:17:38.485952 Batch 60 \n",
      "2024-12-08 20:17:41.042983 Batch 75 \n",
      "2024-12-08 20:17:43.599173 Batch 90 \n",
      "2024-12-08 20:17:46.404535 Batch 105 \n",
      "2024-12-08 20:17:48.821332 Batch 120 \n",
      "2024-12-08 20:17:51.659873 Batch 135 \n",
      "[Train] Accuracy: 61.8545%, Loss per batch: 1.1471\n",
      "2024-12-08 20:18:07.731745 Batch 15 \n",
      "2024-12-08 20:18:09.434879 Batch 30 \n",
      "2024-12-08 20:18:11.550731 Batch 45 \n",
      "2024-12-08 20:18:13.139196 Batch 60 \n",
      "[Val] Accuracy: 63.9745%, loss per batch: 1.0761\n",
      "Epoch 10: SGD lr 0.0039 -> 0.0035\n"
     ]
    }
   ],
   "source": [
    "history, gradient_stats = train(epochs, train_loader, val_loader, model, optimizer, loss_fn, scheduler, outputs_path='../../log/YOLOv8cls-version-3/training/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fa7035e-ebd2-42ab-86e4-9e5e198f6f84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Me\\PJAIT\\Thesis\\Code\\yolov2-to-yolov8\\Backbone experiments\\YOLOv8s-cls Path\\Version 3\\../../..\\train.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(os.path.join(outputs_path, f\"state.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-08 20:18:14.249106 Epoch 11 \n",
      "2024-12-08 20:18:27.936833 Batch 15 \n",
      "2024-12-08 20:18:30.695155 Batch 30 \n",
      "2024-12-08 20:18:33.075800 Batch 45 \n",
      "2024-12-08 20:18:35.650416 Batch 60 \n",
      "2024-12-08 20:18:38.705324 Batch 75 \n",
      "2024-12-08 20:18:41.341095 Batch 90 \n",
      "2024-12-08 20:18:43.629238 Batch 105 \n",
      "2024-12-08 20:18:46.432522 Batch 120 \n",
      "2024-12-08 20:18:48.977328 Batch 135 \n",
      "[Train] Accuracy: 63.2168%, Loss per batch: 1.0999\n",
      "2024-12-08 20:19:05.011213 Batch 15 \n",
      "2024-12-08 20:19:06.697591 Batch 30 \n",
      "2024-12-08 20:19:08.983378 Batch 45 \n",
      "2024-12-08 20:19:10.809538 Batch 60 \n",
      "[Val] Accuracy: 67.7452%, loss per batch: 0.9925\n",
      "Epoch 11: SGD lr 0.0035 -> 0.0031\n",
      "2024-12-08 20:19:11.757651 Epoch 12 \n",
      "2024-12-08 20:19:25.381827 Batch 15 \n",
      "2024-12-08 20:19:28.226624 Batch 30 \n",
      "2024-12-08 20:19:30.800758 Batch 45 \n",
      "2024-12-08 20:19:33.106769 Batch 60 \n",
      "2024-12-08 20:19:35.741965 Batch 75 \n",
      "2024-12-08 20:19:38.547286 Batch 90 \n",
      "2024-12-08 20:19:41.206037 Batch 105 \n",
      "2024-12-08 20:19:43.661337 Batch 120 \n",
      "2024-12-08 20:19:46.495676 Batch 135 \n",
      "[Train] Accuracy: 64.9277%, Loss per batch: 1.0681\n",
      "2024-12-08 20:20:02.307305 Batch 15 \n",
      "2024-12-08 20:20:04.010480 Batch 30 \n",
      "2024-12-08 20:20:06.080432 Batch 45 \n",
      "2024-12-08 20:20:07.810890 Batch 60 \n",
      "[Val] Accuracy: 68.8662%, loss per batch: 0.9481\n",
      "Epoch 12: SGD lr 0.0031 -> 0.0028\n",
      "2024-12-08 20:20:08.750878 Epoch 13 \n",
      "2024-12-08 20:20:23.615541 Batch 15 \n",
      "2024-12-08 20:20:26.523584 Batch 30 \n",
      "2024-12-08 20:20:29.851920 Batch 45 \n",
      "2024-12-08 20:20:32.565678 Batch 60 \n",
      "2024-12-08 20:20:35.559814 Batch 75 \n",
      "2024-12-08 20:20:38.778998 Batch 90 \n",
      "2024-12-08 20:20:41.713705 Batch 105 \n",
      "2024-12-08 20:20:44.604911 Batch 120 \n",
      "2024-12-08 20:20:47.619969 Batch 135 \n",
      "[Train] Accuracy: 64.4947%, Loss per batch: 1.0529\n",
      "2024-12-08 20:21:04.717547 Batch 15 \n",
      "2024-12-08 20:21:06.584895 Batch 30 \n",
      "2024-12-08 20:21:08.854474 Batch 45 \n",
      "2024-12-08 20:21:10.681944 Batch 60 \n",
      "[Val] Accuracy: 69.707%, loss per batch: 0.8994\n",
      "Epoch 13: SGD lr 0.0028 -> 0.0025\n",
      "2024-12-08 20:21:11.679458 Epoch 14 \n",
      "2024-12-08 20:21:26.196400 Batch 15 \n",
      "2024-12-08 20:21:29.041421 Batch 30 \n",
      "2024-12-08 20:21:32.033749 Batch 45 \n",
      "2024-12-08 20:21:35.359209 Batch 60 \n",
      "2024-12-08 20:21:38.367582 Batch 75 \n",
      "2024-12-08 20:21:40.888992 Batch 90 \n",
      "2024-12-08 20:21:43.910187 Batch 105 \n",
      "2024-12-08 20:21:47.161754 Batch 120 \n",
      "2024-12-08 20:21:50.505777 Batch 135 \n",
      "[Train] Accuracy: 66.6174%, Loss per batch: 1.0026\n",
      "2024-12-08 20:22:07.599443 Batch 15 \n",
      "2024-12-08 20:22:09.616300 Batch 30 \n",
      "2024-12-08 20:22:12.042284 Batch 45 \n",
      "2024-12-08 20:22:13.981629 Batch 60 \n",
      "[Val] Accuracy: 70.5478%, loss per batch: 0.8963\n",
      "Epoch 14: SGD lr 0.0025 -> 0.0023\n",
      "2024-12-08 20:22:15.005675 Epoch 15 \n",
      "2024-12-08 20:22:29.935253 Batch 15 \n",
      "2024-12-08 20:22:33.290339 Batch 30 \n",
      "2024-12-08 20:22:36.587245 Batch 45 \n",
      "2024-12-08 20:22:39.355350 Batch 60 \n",
      "2024-12-08 20:22:42.810597 Batch 75 \n",
      "2024-12-08 20:22:45.970297 Batch 90 \n",
      "2024-12-08 20:22:49.156027 Batch 105 \n",
      "2024-12-08 20:22:51.971709 Batch 120 \n",
      "2024-12-08 20:22:55.412033 Batch 135 \n",
      "[Train] Accuracy: 66.9764%, Loss per batch: 0.9975\n",
      "2024-12-08 20:23:13.336470 Batch 15 \n",
      "2024-12-08 20:23:15.461822 Batch 30 \n",
      "2024-12-08 20:23:17.593139 Batch 45 \n",
      "2024-12-08 20:23:19.526158 Batch 60 \n",
      "[Val] Accuracy: 70.0637%, loss per batch: 0.8849\n",
      "Epoch 15: SGD lr 0.0023 -> 0.0021\n",
      "2024-12-08 20:23:20.501022 Epoch 16 \n",
      "2024-12-08 20:23:34.821343 Batch 15 \n",
      "2024-12-08 20:23:37.569457 Batch 30 \n",
      "2024-12-08 20:23:40.201477 Batch 45 \n",
      "2024-12-08 20:23:42.967591 Batch 60 \n",
      "2024-12-08 20:23:46.053660 Batch 75 \n",
      "2024-12-08 20:23:48.576387 Batch 90 \n",
      "2024-12-08 20:23:51.302365 Batch 105 \n",
      "2024-12-08 20:23:53.845508 Batch 120 \n",
      "2024-12-08 20:23:56.617464 Batch 135 \n",
      "[Train] Accuracy: 67.7368%, Loss per batch: 0.9656\n",
      "2024-12-08 20:24:12.756523 Batch 15 \n",
      "2024-12-08 20:24:14.651578 Batch 30 \n",
      "2024-12-08 20:24:16.766574 Batch 45 \n",
      "2024-12-08 20:24:18.541038 Batch 60 \n",
      "[Val] Accuracy: 73.4268%, loss per batch: 0.8071\n",
      "Epoch 16: SGD lr 0.0021 -> 0.0019\n",
      "2024-12-08 20:24:19.466508 Epoch 17 \n",
      "2024-12-08 20:24:33.178572 Batch 15 \n",
      "2024-12-08 20:24:35.891510 Batch 30 \n",
      "2024-12-08 20:24:38.537205 Batch 45 \n",
      "2024-12-08 20:24:41.119749 Batch 60 \n",
      "2024-12-08 20:24:43.483558 Batch 75 \n",
      "2024-12-08 20:24:46.789033 Batch 90 \n",
      "2024-12-08 20:24:49.245348 Batch 105 \n",
      "2024-12-08 20:24:51.833130 Batch 120 \n",
      "2024-12-08 20:24:54.789217 Batch 135 \n",
      "[Train] Accuracy: 68.719%, Loss per batch: 0.9482\n",
      "2024-12-08 20:25:10.842846 Batch 15 \n",
      "2024-12-08 20:25:12.595230 Batch 30 \n",
      "2024-12-08 20:25:14.702576 Batch 45 \n",
      "2024-12-08 20:25:16.576026 Batch 60 \n",
      "[Val] Accuracy: 72.3567%, loss per batch: 0.8268\n",
      "Epoch 17: SGD lr 0.0019 -> 0.0017\n",
      "2024-12-08 20:25:17.646906 Epoch 18 \n",
      "2024-12-08 20:25:32.047032 Batch 15 \n",
      "2024-12-08 20:25:35.067284 Batch 30 \n",
      "2024-12-08 20:25:37.623962 Batch 45 \n",
      "2024-12-08 20:25:40.248156 Batch 60 \n",
      "2024-12-08 20:25:43.040944 Batch 75 \n",
      "2024-12-08 20:25:45.890249 Batch 90 \n",
      "2024-12-08 20:25:48.704060 Batch 105 \n",
      "2024-12-08 20:25:51.505866 Batch 120 \n",
      "2024-12-08 20:25:54.247250 Batch 135 \n",
      "[Train] Accuracy: 69.8173%, Loss per batch: 0.9075\n",
      "2024-12-08 20:26:11.110646 Batch 15 \n",
      "2024-12-08 20:26:12.883885 Batch 30 \n",
      "2024-12-08 20:26:14.989601 Batch 45 \n",
      "2024-12-08 20:26:16.725073 Batch 60 \n",
      "[Val] Accuracy: 74.242%, loss per batch: 0.7976\n",
      "Epoch 18: SGD lr 0.0017 -> 0.0015\n",
      "2024-12-08 20:26:17.653702 Epoch 19 \n",
      "2024-12-08 20:26:31.507858 Batch 15 \n",
      "2024-12-08 20:26:34.098559 Batch 30 \n",
      "2024-12-08 20:26:36.822596 Batch 45 \n",
      "2024-12-08 20:26:39.577361 Batch 60 \n",
      "2024-12-08 20:26:42.217662 Batch 75 \n",
      "2024-12-08 20:26:44.802716 Batch 90 \n",
      "2024-12-08 20:26:47.364472 Batch 105 \n",
      "2024-12-08 20:26:50.034474 Batch 120 \n",
      "2024-12-08 20:26:52.691545 Batch 135 \n",
      "[Train] Accuracy: 69.4688%, Loss per batch: 0.9282\n",
      "2024-12-08 20:27:08.765851 Batch 15 \n",
      "2024-12-08 20:27:10.734496 Batch 30 \n",
      "2024-12-08 20:27:12.931972 Batch 45 \n",
      "2024-12-08 20:27:14.574153 Batch 60 \n",
      "[Val] Accuracy: 75.6943%, loss per batch: 0.7641\n",
      "Epoch 19: SGD lr 0.0015 -> 0.0014\n",
      "2024-12-08 20:27:15.513457 Epoch 20 \n",
      "2024-12-08 20:27:29.359682 Batch 15 \n",
      "2024-12-08 20:27:32.020069 Batch 30 \n",
      "2024-12-08 20:27:34.792096 Batch 45 \n",
      "2024-12-08 20:27:37.048030 Batch 60 \n",
      "2024-12-08 20:27:39.759309 Batch 75 \n",
      "2024-12-08 20:27:42.722847 Batch 90 \n",
      "2024-12-08 20:27:45.445629 Batch 105 \n",
      "2024-12-08 20:27:47.728361 Batch 120 \n",
      "2024-12-08 20:27:50.685512 Batch 135 \n",
      "[Train] Accuracy: 70.715%, Loss per batch: 0.8886\n",
      "2024-12-08 20:28:06.689701 Batch 15 \n",
      "2024-12-08 20:28:08.377122 Batch 30 \n",
      "2024-12-08 20:28:10.471541 Batch 45 \n",
      "2024-12-08 20:28:12.110069 Batch 60 \n",
      "[Val] Accuracy: 75.6178%, loss per batch: 0.7526\n",
      "Epoch 20: SGD lr 0.0014 -> 0.0012\n"
     ]
    }
   ],
   "source": [
    "history, gradient_stats = train(epochs, train_loader, val_loader, model, optimizer, \n",
    "                                loss_fn, scheduler, outputs_path='../../log/YOLOv8cls-version-3/training/', resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c08242-eb92-4361-b5dc-90cc842fa29e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "New Python (GPU)",
   "language": "python",
   "name": "new_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
