{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04833633-3c8f-4d84-8869-c36d9a0cd69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a0a4b-7479-40b1-abdd-1227b6940040",
   "metadata": {},
   "source": [
    "https://lernapparat.de/debug-device-assert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5258e5c-2fd1-45d6-bf25-96674d504f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9deb7c5-1256-4fdd-b447-69f25786c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43b63379-9363-448e-b927-2231dbb097b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import *\n",
    "from data_preprocessing import *\n",
    "from data_augmentation import *\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from Models.yolov8cls_path import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbdae16b-4251-46a6-865b-bfa81a9a5ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9a44f5d-8396-4654-b118-82473d81ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_classes=10, \n",
    "              residual_connection=True, \n",
    "              CSP=False, \n",
    "              add_hidden=False,\n",
    "              classifyV8=False,\n",
    "              bottleneck=1.0, \n",
    "              variant='s', \n",
    "              device=device, \n",
    "              dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae3c5dd0-ac5d-4cbb-8463-d9ebe0621f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../../../datasets/imagenette2/'\n",
    "norms_path = os.path.join(data_path, 'norms.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e608e517-6da7-41c7-b62d-81cca6aecf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means are: [0.44969913363456726, 0.44868946075439453, 0.45163223147392273]\n",
      "stds are: [0.28648287057876587, 0.28796446323394775, 0.2865694761276245]\n"
     ]
    }
   ],
   "source": [
    "means = get_means(path=norms_path, train_loader=None)\n",
    "stds = get_stds(path=norms_path, train_loader=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c4a012-f83a-495c-929b-939db6549642",
   "metadata": {},
   "source": [
    "\n",
    "Profiling your personal module \n",
    "https://pytorch.org/tutorials/beginner/profiler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb61344e-ef51-4068-9304-62ac2f686ae7",
   "metadata": {},
   "source": [
    "https://discuss.pytorch.org/t/how-to-prevent-overfitting/1902\n",
    "Right now, with my augmented dataset, at epoch 8, I am getting a testset Top1 accuracy of 45% but a trainset Top1 accuracy of 69%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4a702f-1ae5-4573-a299-a90db5b87b7f",
   "metadata": {},
   "source": [
    "You should strongly consider data augmentation in some meaningful way. If youâ€™re attempting to do classification then think about what augmentations might add useful information and help distinguish classes in your dataset. In one of my cases, introducing background variation increased recognition rate by over 50%. Basically, with small datasets there is too much overfitting so you want the network to learn real-world distinctions vs. irrelevant artifacts like backgrounds / shadows etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d4f81f6-3dfe-4256-8c57-66f92768ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = transforms.Compose([transforms.RandomResizedCrop((224, 224)),\n",
    "                                              Augmentation(),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize(mean=means, std=stds)])\n",
    "transformations_val = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize(mean=means, std=stds)\n",
    "                                                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f555ed4-dfcf-4f5f-9a71-3e40a7ea2521",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageNetSubset(path=data_path, train=True, transform=transformations, half=False, show=False)\n",
    "val_dataset = ImageNetSubset(path=data_path, train=False, transform=transformations_val, half=False, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ae476e6-947b-46a5-bed5-729af9ee6bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab049c64-2683-432e-acd7-e3494df498f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3846504-a142-444a-85cb-d8ab411cc449",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fb7d049-0696-4341-991c-9b3bfbb0ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daee47e0-df3f-4342-b462-ef2104f8a7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-08 19:47:44.882752 Epoch 1 \n",
      "2024-12-08 19:48:00.555886 Batch 15 \n",
      "2024-12-08 19:48:02.792795 Batch 30 \n",
      "2024-12-08 19:48:05.385732 Batch 45 \n",
      "2024-12-08 19:48:08.097029 Batch 60 \n",
      "2024-12-08 19:48:10.427223 Batch 75 \n",
      "2024-12-08 19:48:13.241094 Batch 90 \n",
      "2024-12-08 19:48:15.943053 Batch 105 \n",
      "2024-12-08 19:48:19.039205 Batch 120 \n",
      "2024-12-08 19:48:22.610514 Batch 135 \n",
      "[Train] Accuracy: 26.4231%, Loss per batch: 2.0437\n",
      "2024-12-08 19:48:39.441962 Batch 15 \n",
      "2024-12-08 19:48:41.140857 Batch 30 \n",
      "2024-12-08 19:48:43.184305 Batch 45 \n",
      "2024-12-08 19:48:45.252919 Batch 60 \n",
      "[Val] Accuracy: 34.6752%, loss per batch: 1.857\n",
      "Epoch 1: SGD lr 0.0100 -> 0.0090\n",
      "2024-12-08 19:48:46.435122 Epoch 2 \n",
      "2024-12-08 19:48:59.163487 Batch 15 \n",
      "2024-12-08 19:49:02.054908 Batch 30 \n",
      "2024-12-08 19:49:04.303561 Batch 45 \n",
      "2024-12-08 19:49:06.995991 Batch 60 \n",
      "2024-12-08 19:49:09.554807 Batch 75 \n",
      "2024-12-08 19:49:11.929398 Batch 90 \n",
      "2024-12-08 19:49:14.486892 Batch 105 \n",
      "2024-12-08 19:49:17.062934 Batch 120 \n",
      "2024-12-08 19:49:19.474612 Batch 135 \n",
      "[Train] Accuracy: 37.0366%, Loss per batch: 1.7872\n",
      "2024-12-08 19:49:33.999926 Batch 15 \n",
      "2024-12-08 19:49:35.550675 Batch 30 \n",
      "2024-12-08 19:49:38.071360 Batch 45 \n",
      "2024-12-08 19:49:39.774509 Batch 60 \n",
      "[Val] Accuracy: 42.7771%, loss per batch: 1.6795\n",
      "Epoch 2: SGD lr 0.0090 -> 0.0081\n",
      "2024-12-08 19:49:40.742149 Epoch 3 \n",
      "2024-12-08 19:49:53.523859 Batch 15 \n",
      "2024-12-08 19:49:55.865305 Batch 30 \n",
      "2024-12-08 19:49:58.448997 Batch 45 \n",
      "2024-12-08 19:50:00.752276 Batch 60 \n",
      "2024-12-08 19:50:03.054673 Batch 75 \n",
      "2024-12-08 19:50:05.523592 Batch 90 \n",
      "2024-12-08 19:50:07.933976 Batch 105 \n",
      "2024-12-08 19:50:10.337848 Batch 120 \n",
      "2024-12-08 19:50:13.139698 Batch 135 \n",
      "[Train] Accuracy: 42.8451%, Loss per batch: 1.6339\n",
      "2024-12-08 19:50:29.230730 Batch 15 \n",
      "2024-12-08 19:50:30.810850 Batch 30 \n",
      "2024-12-08 19:50:32.638434 Batch 45 \n",
      "2024-12-08 19:50:34.197301 Batch 60 \n",
      "[Val] Accuracy: 47.6433%, loss per batch: 1.5005\n",
      "Epoch 3: SGD lr 0.0081 -> 0.0073\n",
      "2024-12-08 19:50:35.149341 Epoch 4 \n",
      "2024-12-08 19:50:47.991404 Batch 15 \n",
      "2024-12-08 19:50:50.189747 Batch 30 \n",
      "2024-12-08 19:50:52.457735 Batch 45 \n",
      "2024-12-08 19:50:55.094164 Batch 60 \n",
      "2024-12-08 19:50:58.237338 Batch 75 \n",
      "2024-12-08 19:51:00.839065 Batch 90 \n",
      "2024-12-08 19:51:03.048484 Batch 105 \n",
      "2024-12-08 19:51:05.583757 Batch 120 \n",
      "2024-12-08 19:51:08.062487 Batch 135 \n",
      "[Train] Accuracy: 48.1783%, Loss per batch: 1.5232\n",
      "2024-12-08 19:51:23.113657 Batch 15 \n",
      "2024-12-08 19:51:24.967553 Batch 30 \n",
      "2024-12-08 19:51:27.013018 Batch 45 \n",
      "2024-12-08 19:51:28.678311 Batch 60 \n",
      "[Val] Accuracy: 49.9873%, loss per batch: 1.4686\n",
      "Epoch 4: SGD lr 0.0073 -> 0.0066\n",
      "2024-12-08 19:51:29.657275 Epoch 5 \n",
      "2024-12-08 19:51:42.705179 Batch 15 \n",
      "2024-12-08 19:51:45.248099 Batch 30 \n",
      "2024-12-08 19:51:47.743824 Batch 45 \n",
      "2024-12-08 19:51:50.030784 Batch 60 \n",
      "2024-12-08 19:51:52.627765 Batch 75 \n",
      "2024-12-08 19:51:55.215412 Batch 90 \n",
      "2024-12-08 19:51:57.888394 Batch 105 \n",
      "2024-12-08 19:52:00.345883 Batch 120 \n",
      "2024-12-08 19:52:02.880989 Batch 135 \n",
      "[Train] Accuracy: 50.301%, Loss per batch: 1.4534\n",
      "2024-12-08 19:52:17.975068 Batch 15 \n",
      "2024-12-08 19:52:19.560457 Batch 30 \n",
      "2024-12-08 19:52:21.488594 Batch 45 \n",
      "2024-12-08 19:52:23.054957 Batch 60 \n",
      "[Val] Accuracy: 52.2038%, loss per batch: 1.4585\n",
      "Epoch 5: SGD lr 0.0066 -> 0.0059\n",
      "2024-12-08 19:52:24.004569 Epoch 6 \n",
      "2024-12-08 19:52:36.892959 Batch 15 \n",
      "2024-12-08 19:52:39.523163 Batch 30 \n",
      "2024-12-08 19:52:41.892356 Batch 45 \n",
      "2024-12-08 19:52:44.653791 Batch 60 \n",
      "2024-12-08 19:52:47.191953 Batch 75 \n",
      "2024-12-08 19:52:49.670950 Batch 90 \n",
      "2024-12-08 19:52:51.866739 Batch 105 \n",
      "2024-12-08 19:52:54.525834 Batch 120 \n",
      "2024-12-08 19:52:56.878451 Batch 135 \n",
      "[Train] Accuracy: 52.9729%, Loss per batch: 1.3884\n",
      "2024-12-08 19:53:12.350340 Batch 15 \n",
      "2024-12-08 19:53:14.016856 Batch 30 \n",
      "2024-12-08 19:53:15.909623 Batch 45 \n",
      "2024-12-08 19:53:17.689937 Batch 60 \n",
      "[Val] Accuracy: 53.707%, loss per batch: 1.3539\n",
      "Epoch 6: SGD lr 0.0059 -> 0.0053\n",
      "2024-12-08 19:53:18.657790 Epoch 7 \n",
      "2024-12-08 19:53:31.533609 Batch 15 \n",
      "2024-12-08 19:53:34.221401 Batch 30 \n",
      "2024-12-08 19:53:36.666809 Batch 45 \n",
      "2024-12-08 19:53:39.182688 Batch 60 \n",
      "2024-12-08 19:53:41.680508 Batch 75 \n",
      "2024-12-08 19:53:44.156418 Batch 90 \n",
      "2024-12-08 19:53:46.804938 Batch 105 \n",
      "2024-12-08 19:53:49.269458 Batch 120 \n",
      "2024-12-08 19:53:52.028984 Batch 135 \n",
      "[Train] Accuracy: 56.3946%, Loss per batch: 1.3022\n",
      "2024-12-08 19:54:07.371154 Batch 15 \n",
      "2024-12-08 19:54:08.973601 Batch 30 \n",
      "2024-12-08 19:54:10.892102 Batch 45 \n",
      "2024-12-08 19:54:12.464408 Batch 60 \n",
      "[Val] Accuracy: 59.2102%, loss per batch: 1.2181\n",
      "Epoch 7: SGD lr 0.0053 -> 0.0048\n",
      "2024-12-08 19:54:13.406305 Epoch 8 \n",
      "2024-12-08 19:54:26.498608 Batch 15 \n",
      "2024-12-08 19:54:29.086030 Batch 30 \n",
      "2024-12-08 19:54:31.547137 Batch 45 \n",
      "2024-12-08 19:54:33.920559 Batch 60 \n",
      "2024-12-08 19:54:36.945358 Batch 75 \n",
      "2024-12-08 19:54:39.560191 Batch 90 \n",
      "2024-12-08 19:54:42.069867 Batch 105 \n",
      "2024-12-08 19:54:44.437732 Batch 120 \n",
      "2024-12-08 19:54:47.244232 Batch 135 \n",
      "[Train] Accuracy: 58.4645%, Loss per batch: 1.2601\n",
      "2024-12-08 19:55:02.711826 Batch 15 \n",
      "2024-12-08 19:55:04.347336 Batch 30 \n",
      "2024-12-08 19:55:06.343558 Batch 45 \n",
      "2024-12-08 19:55:08.083831 Batch 60 \n",
      "[Val] Accuracy: 63.4904%, loss per batch: 1.1184\n",
      "Epoch 8: SGD lr 0.0048 -> 0.0043\n",
      "2024-12-08 19:55:09.101023 Epoch 9 \n",
      "2024-12-08 19:55:22.207071 Batch 15 \n",
      "2024-12-08 19:55:25.229794 Batch 30 \n",
      "2024-12-08 19:55:27.808750 Batch 45 \n",
      "2024-12-08 19:55:30.330089 Batch 60 \n",
      "2024-12-08 19:55:33.061059 Batch 75 \n",
      "2024-12-08 19:55:35.720536 Batch 90 \n",
      "2024-12-08 19:55:38.287922 Batch 105 \n",
      "2024-12-08 19:55:40.859008 Batch 120 \n",
      "2024-12-08 19:55:43.680887 Batch 135 \n",
      "[Train] Accuracy: 60.0063%, Loss per batch: 1.2009\n",
      "2024-12-08 19:55:59.511202 Batch 15 \n",
      "2024-12-08 19:56:01.143206 Batch 30 \n",
      "2024-12-08 19:56:03.211159 Batch 45 \n",
      "2024-12-08 19:56:04.818715 Batch 60 \n",
      "[Val] Accuracy: 62.3949%, loss per batch: 1.1726\n",
      "Epoch 9: SGD lr 0.0043 -> 0.0039\n",
      "2024-12-08 19:56:05.745022 Epoch 10 \n",
      "2024-12-08 19:56:19.410554 Batch 15 \n",
      "2024-12-08 19:56:22.098294 Batch 30 \n",
      "2024-12-08 19:56:24.507255 Batch 45 \n",
      "2024-12-08 19:56:27.024467 Batch 60 \n",
      "2024-12-08 19:56:29.865978 Batch 75 \n",
      "2024-12-08 19:56:32.621501 Batch 90 \n",
      "2024-12-08 19:56:35.229275 Batch 105 \n",
      "2024-12-08 19:56:37.782430 Batch 120 \n",
      "2024-12-08 19:56:40.571498 Batch 135 \n",
      "[Train] Accuracy: 61.7277%, Loss per batch: 1.1443\n",
      "2024-12-08 19:56:56.181579 Batch 15 \n",
      "2024-12-08 19:56:57.829127 Batch 30 \n",
      "2024-12-08 19:56:59.965166 Batch 45 \n",
      "2024-12-08 19:57:01.810238 Batch 60 \n",
      "[Val] Accuracy: 64.7898%, loss per batch: 1.0686\n",
      "Epoch 10: SGD lr 0.0039 -> 0.0035\n"
     ]
    }
   ],
   "source": [
    "history, gradient_stats = train(epochs, train_loader, val_loader, model, optimizer, loss_fn, scheduler, outputs_path='../../log/YOLOv8cls-version-2/training/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fa7035e-ebd2-42ab-86e4-9e5e198f6f84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Me\\PJAIT\\Thesis\\Code\\yolov2-to-yolov8\\Backbone experiments\\YOLOv8s-cls Path\\Version 2\\../../..\\train.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(os.path.join(outputs_path, f\"state.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-08 19:57:03.004068 Epoch 11 \n",
      "2024-12-08 19:57:17.117896 Batch 15 \n",
      "2024-12-08 19:57:19.704664 Batch 30 \n",
      "2024-12-08 19:57:22.288480 Batch 45 \n",
      "2024-12-08 19:57:24.663826 Batch 60 \n",
      "2024-12-08 19:57:27.552630 Batch 75 \n",
      "2024-12-08 19:57:30.021543 Batch 90 \n",
      "2024-12-08 19:57:32.771355 Batch 105 \n",
      "2024-12-08 19:57:35.542646 Batch 120 \n",
      "2024-12-08 19:57:38.278372 Batch 135 \n",
      "[Train] Accuracy: 62.7099%, Loss per batch: 1.1255\n",
      "2024-12-08 19:57:54.128577 Batch 15 \n",
      "2024-12-08 19:57:55.726401 Batch 30 \n",
      "2024-12-08 19:57:57.780009 Batch 45 \n",
      "2024-12-08 19:57:59.447201 Batch 60 \n",
      "[Val] Accuracy: 65.4777%, loss per batch: 1.0684\n",
      "Epoch 11: SGD lr 0.0035 -> 0.0031\n",
      "2024-12-08 19:58:00.385356 Epoch 12 \n",
      "2024-12-08 19:58:14.275912 Batch 15 \n",
      "2024-12-08 19:58:17.034217 Batch 30 \n",
      "2024-12-08 19:58:19.815782 Batch 45 \n",
      "2024-12-08 19:58:22.610539 Batch 60 \n",
      "2024-12-08 19:58:25.407594 Batch 75 \n",
      "2024-12-08 19:58:28.218248 Batch 90 \n",
      "2024-12-08 19:58:31.035451 Batch 105 \n",
      "2024-12-08 19:58:33.824120 Batch 120 \n",
      "2024-12-08 19:58:36.631606 Batch 135 \n",
      "[Train] Accuracy: 64.3574%, Loss per batch: 1.0737\n",
      "2024-12-08 19:58:52.379768 Batch 15 \n",
      "2024-12-08 19:58:54.154324 Batch 30 \n",
      "2024-12-08 19:58:56.076138 Batch 45 \n",
      "2024-12-08 19:58:57.707718 Batch 60 \n",
      "[Val] Accuracy: 70.293%, loss per batch: 0.9296\n",
      "Epoch 12: SGD lr 0.0031 -> 0.0028\n",
      "2024-12-08 19:58:58.643836 Epoch 13 \n",
      "2024-12-08 19:59:12.431596 Batch 15 \n",
      "2024-12-08 19:59:15.152707 Batch 30 \n",
      "2024-12-08 19:59:17.887412 Batch 45 \n",
      "2024-12-08 19:59:20.658681 Batch 60 \n",
      "2024-12-08 19:59:23.430519 Batch 75 \n",
      "2024-12-08 19:59:26.224329 Batch 90 \n",
      "2024-12-08 19:59:29.040284 Batch 105 \n",
      "2024-12-08 19:59:31.856749 Batch 120 \n",
      "2024-12-08 19:59:34.633191 Batch 135 \n",
      "[Train] Accuracy: 65.5085%, Loss per batch: 1.0403\n",
      "2024-12-08 19:59:50.334299 Batch 15 \n",
      "2024-12-08 19:59:51.988555 Batch 30 \n",
      "2024-12-08 19:59:53.901246 Batch 45 \n",
      "2024-12-08 19:59:55.508718 Batch 60 \n",
      "[Val] Accuracy: 70.0637%, loss per batch: 0.9285\n",
      "Epoch 13: SGD lr 0.0028 -> 0.0025\n",
      "2024-12-08 19:59:56.563479 Epoch 14 \n",
      "2024-12-08 20:00:09.970146 Batch 15 \n",
      "2024-12-08 20:00:12.754226 Batch 30 \n",
      "2024-12-08 20:00:15.846574 Batch 45 \n",
      "2024-12-08 20:00:18.328352 Batch 60 \n",
      "2024-12-08 20:00:21.187072 Batch 75 \n",
      "2024-12-08 20:00:23.778411 Batch 90 \n",
      "2024-12-08 20:00:26.227198 Batch 105 \n",
      "2024-12-08 20:00:29.003887 Batch 120 \n",
      "2024-12-08 20:00:31.751996 Batch 135 \n",
      "[Train] Accuracy: 65.6669%, Loss per batch: 1.0204\n",
      "2024-12-08 20:00:47.402281 Batch 15 \n",
      "2024-12-08 20:00:49.089847 Batch 30 \n",
      "2024-12-08 20:00:51.083802 Batch 45 \n",
      "2024-12-08 20:00:52.711859 Batch 60 \n",
      "[Val] Accuracy: 67.5414%, loss per batch: 0.9831\n",
      "Epoch 14: SGD lr 0.0025 -> 0.0023\n",
      "2024-12-08 20:00:53.654892 Epoch 15 \n",
      "2024-12-08 20:01:07.394304 Batch 15 \n",
      "2024-12-08 20:01:09.994336 Batch 30 \n",
      "2024-12-08 20:01:12.686890 Batch 45 \n",
      "2024-12-08 20:01:15.235656 Batch 60 \n",
      "2024-12-08 20:01:18.048661 Batch 75 \n",
      "2024-12-08 20:01:20.640682 Batch 90 \n",
      "2024-12-08 20:01:23.260515 Batch 105 \n",
      "2024-12-08 20:01:25.884697 Batch 120 \n",
      "2024-12-08 20:01:28.563541 Batch 135 \n",
      "[Train] Accuracy: 66.9659%, Loss per batch: 0.9949\n",
      "2024-12-08 20:01:44.532037 Batch 15 \n",
      "2024-12-08 20:01:46.237443 Batch 30 \n",
      "2024-12-08 20:01:48.284201 Batch 45 \n",
      "2024-12-08 20:01:50.265597 Batch 60 \n",
      "[Val] Accuracy: 70.879%, loss per batch: 0.9006\n",
      "Epoch 15: SGD lr 0.0023 -> 0.0021\n",
      "2024-12-08 20:01:51.208613 Epoch 16 \n",
      "2024-12-08 20:02:04.996600 Batch 15 \n",
      "2024-12-08 20:02:07.756784 Batch 30 \n",
      "2024-12-08 20:02:10.521183 Batch 45 \n",
      "2024-12-08 20:02:13.333609 Batch 60 \n",
      "2024-12-08 20:02:16.158019 Batch 75 \n",
      "2024-12-08 20:02:18.954057 Batch 90 \n",
      "2024-12-08 20:02:21.778469 Batch 105 \n",
      "2024-12-08 20:02:24.600141 Batch 120 \n",
      "2024-12-08 20:02:27.424465 Batch 135 \n",
      "[Train] Accuracy: 67.8108%, Loss per batch: 0.977\n",
      "2024-12-08 20:02:43.215438 Batch 15 \n",
      "2024-12-08 20:02:44.877458 Batch 30 \n",
      "2024-12-08 20:02:46.865809 Batch 45 \n",
      "2024-12-08 20:02:48.534808 Batch 60 \n",
      "[Val] Accuracy: 71.1847%, loss per batch: 0.8827\n",
      "Epoch 16: SGD lr 0.0021 -> 0.0019\n",
      "2024-12-08 20:02:49.454821 Epoch 17 \n",
      "2024-12-08 20:03:03.352548 Batch 15 \n",
      "2024-12-08 20:03:06.092496 Batch 30 \n",
      "2024-12-08 20:03:08.859851 Batch 45 \n",
      "2024-12-08 20:03:11.656209 Batch 60 \n",
      "2024-12-08 20:03:14.456811 Batch 75 \n",
      "2024-12-08 20:03:17.258127 Batch 90 \n",
      "2024-12-08 20:03:20.037066 Batch 105 \n",
      "2024-12-08 20:03:22.844029 Batch 120 \n",
      "2024-12-08 20:03:25.652275 Batch 135 \n",
      "[Train] Accuracy: 69.3949%, Loss per batch: 0.9318\n",
      "2024-12-08 20:03:41.624223 Batch 15 \n",
      "2024-12-08 20:03:43.362108 Batch 30 \n",
      "2024-12-08 20:03:45.331761 Batch 45 \n",
      "2024-12-08 20:03:47.001027 Batch 60 \n",
      "[Val] Accuracy: 73.121%, loss per batch: 0.8193\n",
      "Epoch 17: SGD lr 0.0019 -> 0.0017\n",
      "2024-12-08 20:03:47.913182 Epoch 18 \n",
      "2024-12-08 20:04:01.731064 Batch 15 \n",
      "2024-12-08 20:04:04.454527 Batch 30 \n",
      "2024-12-08 20:04:07.213249 Batch 45 \n",
      "2024-12-08 20:04:09.979513 Batch 60 \n",
      "2024-12-08 20:04:12.784582 Batch 75 \n",
      "2024-12-08 20:04:15.572616 Batch 90 \n",
      "2024-12-08 20:04:18.385873 Batch 105 \n",
      "2024-12-08 20:04:21.181144 Batch 120 \n",
      "2024-12-08 20:04:23.979390 Batch 135 \n",
      "[Train] Accuracy: 69.6378%, Loss per batch: 0.9218\n",
      "2024-12-08 20:04:39.841816 Batch 15 \n",
      "2024-12-08 20:04:41.502692 Batch 30 \n",
      "2024-12-08 20:04:43.522462 Batch 45 \n",
      "2024-12-08 20:04:45.294837 Batch 60 \n",
      "[Val] Accuracy: 73.9618%, loss per batch: 0.8127\n",
      "Epoch 18: SGD lr 0.0017 -> 0.0015\n",
      "2024-12-08 20:04:46.243217 Epoch 19 \n",
      "2024-12-08 20:04:59.933723 Batch 15 \n",
      "2024-12-08 20:05:02.645776 Batch 30 \n",
      "2024-12-08 20:05:05.388508 Batch 45 \n",
      "2024-12-08 20:05:08.138716 Batch 60 \n",
      "2024-12-08 20:05:10.927949 Batch 75 \n",
      "2024-12-08 20:05:13.715953 Batch 90 \n",
      "2024-12-08 20:05:16.534498 Batch 105 \n",
      "2024-12-08 20:05:19.336494 Batch 120 \n",
      "2024-12-08 20:05:22.137001 Batch 135 \n",
      "[Train] Accuracy: 70.7255%, Loss per batch: 0.887\n",
      "2024-12-08 20:05:37.929384 Batch 15 \n",
      "2024-12-08 20:05:39.496828 Batch 30 \n",
      "2024-12-08 20:05:41.518065 Batch 45 \n",
      "2024-12-08 20:05:43.184304 Batch 60 \n",
      "[Val] Accuracy: 73.9363%, loss per batch: 0.8034\n",
      "Epoch 19: SGD lr 0.0015 -> 0.0014\n",
      "2024-12-08 20:05:44.088415 Epoch 20 \n",
      "2024-12-08 20:05:58.298776 Batch 15 \n",
      "2024-12-08 20:06:00.875074 Batch 30 \n",
      "2024-12-08 20:06:03.229990 Batch 45 \n",
      "2024-12-08 20:06:06.195036 Batch 60 \n",
      "2024-12-08 20:06:08.746504 Batch 75 \n",
      "2024-12-08 20:06:11.512347 Batch 90 \n",
      "2024-12-08 20:06:13.973979 Batch 105 \n",
      "2024-12-08 20:06:16.726588 Batch 120 \n",
      "2024-12-08 20:06:19.563607 Batch 135 \n",
      "[Train] Accuracy: 70.8311%, Loss per batch: 0.878\n",
      "2024-12-08 20:06:35.589945 Batch 15 \n",
      "2024-12-08 20:06:37.523823 Batch 30 \n",
      "2024-12-08 20:06:39.537952 Batch 45 \n",
      "2024-12-08 20:06:41.215628 Batch 60 \n",
      "[Val] Accuracy: 73.9873%, loss per batch: 0.7821\n",
      "Epoch 20: SGD lr 0.0014 -> 0.0012\n"
     ]
    }
   ],
   "source": [
    "history, gradient_stats = train(epochs, train_loader, val_loader, model, optimizer, \n",
    "                                loss_fn, scheduler, outputs_path='../../log/YOLOv8cls-version-2/training/', resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d89177-dc62-4ba6-a1b5-476c88f08ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "New Python (GPU)",
   "language": "python",
   "name": "new_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
