{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04833633-3c8f-4d84-8869-c36d9a0cd69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a0a4b-7479-40b1-abdd-1227b6940040",
   "metadata": {},
   "source": [
    "https://lernapparat.de/debug-device-assert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5258e5c-2fd1-45d6-bf25-96674d504f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9deb7c5-1256-4fdd-b447-69f25786c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43b63379-9363-448e-b927-2231dbb097b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import *\n",
    "from data_preprocessing import *\n",
    "from data_augmentation import *\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from Models.yolov8cls_path import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbdae16b-4251-46a6-865b-bfa81a9a5ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9a44f5d-8396-4654-b118-82473d81ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_classes=10, \n",
    "              residual_connection=True, \n",
    "              CSP=True, \n",
    "              add_hidden=True,\n",
    "              classifyV8=True,\n",
    "              bottleneck=1.0, \n",
    "              variant='s', \n",
    "              device=device, \n",
    "              dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae3c5dd0-ac5d-4cbb-8463-d9ebe0621f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../../../datasets/imagenette2/'\n",
    "norms_path = os.path.join(data_path, 'norms.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e608e517-6da7-41c7-b62d-81cca6aecf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means are: [0.44969913363456726, 0.44868946075439453, 0.45163223147392273]\n",
      "stds are: [0.28648287057876587, 0.28796446323394775, 0.2865694761276245]\n"
     ]
    }
   ],
   "source": [
    "means = get_means(path=norms_path, train_loader=None)\n",
    "stds = get_stds(path=norms_path, train_loader=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c4a012-f83a-495c-929b-939db6549642",
   "metadata": {},
   "source": [
    "\n",
    "Profiling your personal module \n",
    "https://pytorch.org/tutorials/beginner/profiler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb61344e-ef51-4068-9304-62ac2f686ae7",
   "metadata": {},
   "source": [
    "https://discuss.pytorch.org/t/how-to-prevent-overfitting/1902\n",
    "Right now, with my augmented dataset, at epoch 8, I am getting a testset Top1 accuracy of 45% but a trainset Top1 accuracy of 69%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4a702f-1ae5-4573-a299-a90db5b87b7f",
   "metadata": {},
   "source": [
    "You should strongly consider data augmentation in some meaningful way. If youâ€™re attempting to do classification then think about what augmentations might add useful information and help distinguish classes in your dataset. In one of my cases, introducing background variation increased recognition rate by over 50%. Basically, with small datasets there is too much overfitting so you want the network to learn real-world distinctions vs. irrelevant artifacts like backgrounds / shadows etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d4f81f6-3dfe-4256-8c57-66f92768ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = transforms.Compose([transforms.RandomResizedCrop((224, 224)),\n",
    "                                              Augmentation(),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize(mean=means, std=stds)])\n",
    "transformations_val = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize(mean=means, std=stds)\n",
    "                                                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f555ed4-dfcf-4f5f-9a71-3e40a7ea2521",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageNetSubset(path=data_path, train=True, transform=transformations, half=False, show=False)\n",
    "val_dataset = ImageNetSubset(path=data_path, train=False, transform=transformations_val, half=False, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ae476e6-947b-46a5-bed5-729af9ee6bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab049c64-2683-432e-acd7-e3494df498f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3846504-a142-444a-85cb-d8ab411cc449",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fb7d049-0696-4341-991c-9b3bfbb0ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daee47e0-df3f-4342-b462-ef2104f8a7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-08 18:15:17.357163 Epoch 1 \n",
      "2024-12-08 18:15:33.186914 Batch 15 \n",
      "2024-12-08 18:15:35.842873 Batch 30 \n",
      "2024-12-08 18:15:38.385833 Batch 45 \n",
      "2024-12-08 18:15:41.117270 Batch 60 \n",
      "2024-12-08 18:15:43.891518 Batch 75 \n",
      "2024-12-08 18:15:46.636789 Batch 90 \n",
      "2024-12-08 18:15:49.355019 Batch 105 \n",
      "2024-12-08 18:15:51.895369 Batch 120 \n",
      "2024-12-08 18:15:54.716225 Batch 135 \n",
      "[Train] Accuracy: 26.5287%, Loss per batch: 2.0423\n",
      "2024-12-08 18:16:10.717812 Batch 15 \n",
      "2024-12-08 18:16:12.317421 Batch 30 \n",
      "2024-12-08 18:16:14.248104 Batch 45 \n",
      "2024-12-08 18:16:15.859858 Batch 60 \n",
      "[Val] Accuracy: 36.1274%, loss per batch: 1.8331\n",
      "Epoch 1: SGD lr 0.0100 -> 0.0090\n",
      "2024-12-08 18:16:16.990376 Epoch 2 \n",
      "2024-12-08 18:16:30.497077 Batch 15 \n",
      "2024-12-08 18:16:33.163858 Batch 30 \n",
      "2024-12-08 18:16:35.835411 Batch 45 \n",
      "2024-12-08 18:16:38.251176 Batch 60 \n",
      "2024-12-08 18:16:41.044145 Batch 75 \n",
      "2024-12-08 18:16:43.931763 Batch 90 \n",
      "2024-12-08 18:16:46.502935 Batch 105 \n",
      "2024-12-08 18:16:49.067005 Batch 120 \n",
      "2024-12-08 18:16:51.882567 Batch 135 \n",
      "[Train] Accuracy: 40.0676%, Loss per batch: 1.7384\n",
      "2024-12-08 18:17:08.475480 Batch 15 \n",
      "2024-12-08 18:17:10.170694 Batch 30 \n",
      "2024-12-08 18:17:12.499940 Batch 45 \n",
      "2024-12-08 18:17:14.205559 Batch 60 \n",
      "[Val] Accuracy: 43.6433%, loss per batch: 1.6446\n",
      "Epoch 2: SGD lr 0.0090 -> 0.0081\n",
      "2024-12-08 18:17:15.238127 Epoch 3 \n",
      "2024-12-08 18:17:28.858057 Batch 15 \n",
      "2024-12-08 18:17:31.505935 Batch 30 \n",
      "2024-12-08 18:17:34.011398 Batch 45 \n",
      "2024-12-08 18:17:36.310778 Batch 60 \n",
      "2024-12-08 18:17:38.967883 Batch 75 \n",
      "2024-12-08 18:17:41.441288 Batch 90 \n",
      "2024-12-08 18:17:44.375489 Batch 105 \n",
      "2024-12-08 18:17:46.868457 Batch 120 \n",
      "2024-12-08 18:17:49.411244 Batch 135 \n",
      "[Train] Accuracy: 46.0133%, Loss per batch: 1.562\n",
      "2024-12-08 18:18:05.214662 Batch 15 \n",
      "2024-12-08 18:18:06.989097 Batch 30 \n",
      "2024-12-08 18:18:09.192902 Batch 45 \n",
      "2024-12-08 18:18:10.925932 Batch 60 \n",
      "[Val] Accuracy: 32.5096%, loss per batch: 2.5023\n",
      "Epoch 3: SGD lr 0.0081 -> 0.0073\n",
      "2024-12-08 18:18:11.898741 Epoch 4 \n",
      "2024-12-08 18:18:25.531580 Batch 15 \n",
      "2024-12-08 18:18:28.181926 Batch 30 \n",
      "2024-12-08 18:18:30.842950 Batch 45 \n",
      "2024-12-08 18:18:33.432532 Batch 60 \n",
      "2024-12-08 18:18:35.832419 Batch 75 \n",
      "2024-12-08 18:18:38.566123 Batch 90 \n",
      "2024-12-08 18:18:41.262883 Batch 105 \n",
      "2024-12-08 18:18:43.928169 Batch 120 \n",
      "2024-12-08 18:18:46.301497 Batch 135 \n",
      "[Train] Accuracy: 49.7729%, Loss per batch: 1.4753\n",
      "2024-12-08 18:19:02.345330 Batch 15 \n",
      "2024-12-08 18:19:04.194798 Batch 30 \n",
      "2024-12-08 18:19:06.577759 Batch 45 \n",
      "2024-12-08 18:19:08.292936 Batch 60 \n",
      "[Val] Accuracy: 51.3376%, loss per batch: 1.4585\n",
      "Epoch 4: SGD lr 0.0073 -> 0.0066\n",
      "2024-12-08 18:19:09.306744 Epoch 5 \n",
      "2024-12-08 18:19:22.951100 Batch 15 \n",
      "2024-12-08 18:19:25.702464 Batch 30 \n",
      "2024-12-08 18:19:28.840963 Batch 45 \n",
      "2024-12-08 18:19:31.658541 Batch 60 \n",
      "2024-12-08 18:19:34.368258 Batch 75 \n",
      "2024-12-08 18:19:37.167798 Batch 90 \n",
      "2024-12-08 18:19:39.977498 Batch 105 \n",
      "2024-12-08 18:19:42.365613 Batch 120 \n",
      "2024-12-08 18:19:45.348733 Batch 135 \n",
      "[Train] Accuracy: 53.9339%, Loss per batch: 1.3724\n",
      "2024-12-08 18:20:01.843633 Batch 15 \n",
      "2024-12-08 18:20:03.655355 Batch 30 \n",
      "2024-12-08 18:20:05.801699 Batch 45 \n",
      "2024-12-08 18:20:07.605343 Batch 60 \n",
      "[Val] Accuracy: 53.5541%, loss per batch: 1.3881\n",
      "Epoch 5: SGD lr 0.0066 -> 0.0059\n",
      "2024-12-08 18:20:08.625805 Epoch 6 \n",
      "2024-12-08 18:20:22.916920 Batch 15 \n",
      "2024-12-08 18:20:25.585368 Batch 30 \n",
      "2024-12-08 18:20:28.308535 Batch 45 \n",
      "2024-12-08 18:20:30.686605 Batch 60 \n",
      "2024-12-08 18:20:33.278264 Batch 75 \n",
      "2024-12-08 18:20:36.084592 Batch 90 \n",
      "2024-12-08 18:20:39.049141 Batch 105 \n",
      "2024-12-08 18:20:41.539685 Batch 120 \n",
      "2024-12-08 18:20:44.491892 Batch 135 \n",
      "[Train] Accuracy: 56.8698%, Loss per batch: 1.2972\n",
      "2024-12-08 18:21:00.867148 Batch 15 \n",
      "2024-12-08 18:21:02.744356 Batch 30 \n",
      "2024-12-08 18:21:04.854003 Batch 45 \n",
      "2024-12-08 18:21:06.649866 Batch 60 \n",
      "[Val] Accuracy: 54.9554%, loss per batch: 1.371\n",
      "Epoch 6: SGD lr 0.0059 -> 0.0053\n",
      "2024-12-08 18:21:07.639755 Epoch 7 \n",
      "2024-12-08 18:21:21.335556 Batch 15 \n",
      "2024-12-08 18:21:24.054674 Batch 30 \n",
      "2024-12-08 18:21:26.726930 Batch 45 \n",
      "2024-12-08 18:21:29.091338 Batch 60 \n",
      "2024-12-08 18:21:31.890265 Batch 75 \n",
      "2024-12-08 18:21:34.441843 Batch 90 \n",
      "2024-12-08 18:21:37.498128 Batch 105 \n",
      "2024-12-08 18:21:40.181343 Batch 120 \n",
      "2024-12-08 18:21:43.080307 Batch 135 \n",
      "[Train] Accuracy: 58.4328%, Loss per batch: 1.2446\n",
      "2024-12-08 18:21:59.488124 Batch 15 \n",
      "2024-12-08 18:22:01.322241 Batch 30 \n",
      "2024-12-08 18:22:03.489115 Batch 45 \n",
      "2024-12-08 18:22:05.211354 Batch 60 \n",
      "[Val] Accuracy: 56.5096%, loss per batch: 1.2958\n",
      "Epoch 7: SGD lr 0.0053 -> 0.0048\n",
      "2024-12-08 18:22:06.191232 Epoch 8 \n",
      "2024-12-08 18:22:20.391248 Batch 15 \n",
      "2024-12-08 18:22:23.093196 Batch 30 \n",
      "2024-12-08 18:22:25.796688 Batch 45 \n",
      "2024-12-08 18:22:28.154424 Batch 60 \n",
      "2024-12-08 18:22:31.064997 Batch 75 \n",
      "2024-12-08 18:22:33.861809 Batch 90 \n",
      "2024-12-08 18:22:36.475166 Batch 105 \n",
      "2024-12-08 18:22:39.088719 Batch 120 \n",
      "2024-12-08 18:22:41.900615 Batch 135 \n",
      "[Train] Accuracy: 60.2704%, Loss per batch: 1.1858\n",
      "2024-12-08 18:22:58.252679 Batch 15 \n",
      "2024-12-08 18:23:00.135633 Batch 30 \n",
      "2024-12-08 18:23:02.397909 Batch 45 \n",
      "2024-12-08 18:23:04.129026 Batch 60 \n",
      "[Val] Accuracy: 60.5605%, loss per batch: 1.1956\n",
      "Epoch 8: SGD lr 0.0048 -> 0.0043\n",
      "2024-12-08 18:23:05.143312 Epoch 9 \n",
      "2024-12-08 18:23:19.457192 Batch 15 \n",
      "2024-12-08 18:23:22.149367 Batch 30 \n",
      "2024-12-08 18:23:25.185931 Batch 45 \n",
      "2024-12-08 18:23:27.781185 Batch 60 \n",
      "2024-12-08 18:23:30.708642 Batch 75 \n",
      "2024-12-08 18:23:33.461592 Batch 90 \n",
      "2024-12-08 18:23:36.224767 Batch 105 \n",
      "2024-12-08 18:23:38.890760 Batch 120 \n",
      "2024-12-08 18:23:41.866396 Batch 135 \n",
      "[Train] Accuracy: 61.9601%, Loss per batch: 1.1488\n",
      "2024-12-08 18:23:58.906629 Batch 15 \n",
      "2024-12-08 18:24:00.773240 Batch 30 \n",
      "2024-12-08 18:24:03.137752 Batch 45 \n",
      "2024-12-08 18:24:05.061472 Batch 60 \n",
      "[Val] Accuracy: 67.949%, loss per batch: 0.9862\n",
      "Epoch 9: SGD lr 0.0043 -> 0.0039\n",
      "2024-12-08 18:24:06.039831 Epoch 10 \n",
      "2024-12-08 18:24:20.231811 Batch 15 \n",
      "2024-12-08 18:24:23.329095 Batch 30 \n",
      "2024-12-08 18:24:26.136671 Batch 45 \n",
      "2024-12-08 18:24:29.029455 Batch 60 \n",
      "2024-12-08 18:24:31.915305 Batch 75 \n",
      "2024-12-08 18:24:34.344511 Batch 90 \n",
      "2024-12-08 18:24:37.193463 Batch 105 \n",
      "2024-12-08 18:24:40.037921 Batch 120 \n",
      "2024-12-08 18:24:42.868608 Batch 135 \n",
      "[Train] Accuracy: 63.7871%, Loss per batch: 1.0815\n",
      "2024-12-08 18:24:59.251838 Batch 15 \n",
      "2024-12-08 18:25:01.037886 Batch 30 \n",
      "2024-12-08 18:25:03.260330 Batch 45 \n",
      "2024-12-08 18:25:05.020897 Batch 60 \n",
      "[Val] Accuracy: 69.2739%, loss per batch: 0.9569\n",
      "Epoch 10: SGD lr 0.0039 -> 0.0035\n"
     ]
    }
   ],
   "source": [
    "history, gradient_stats = train(epochs, train_loader, val_loader, model, optimizer, loss_fn, scheduler, outputs_path='../../log/YOLOv8cls-version-5/training/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fa7035e-ebd2-42ab-86e4-9e5e198f6f84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Me\\PJAIT\\Thesis\\Code\\yolov2-to-yolov8\\Backbone experiments\\YOLOv8s-cls Path\\Version 5\\../../..\\train.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(os.path.join(outputs_path, f\"state.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-08 18:25:06.175147 Epoch 11 \n",
      "2024-12-08 18:25:20.231139 Batch 15 \n",
      "2024-12-08 18:25:23.228820 Batch 30 \n",
      "2024-12-08 18:25:26.090275 Batch 45 \n",
      "2024-12-08 18:25:28.843335 Batch 60 \n",
      "2024-12-08 18:25:31.737618 Batch 75 \n",
      "2024-12-08 18:25:34.630933 Batch 90 \n",
      "2024-12-08 18:25:37.573393 Batch 105 \n",
      "2024-12-08 18:25:40.197501 Batch 120 \n",
      "2024-12-08 18:25:43.024126 Batch 135 \n",
      "[Train] Accuracy: 64.4102%, Loss per batch: 1.0779\n",
      "2024-12-08 18:25:59.763312 Batch 15 \n",
      "2024-12-08 18:26:01.588000 Batch 30 \n",
      "2024-12-08 18:26:03.866676 Batch 45 \n",
      "2024-12-08 18:26:05.626523 Batch 60 \n",
      "[Val] Accuracy: 68.3057%, loss per batch: 0.9768\n",
      "Epoch 11: SGD lr 0.0035 -> 0.0031\n",
      "2024-12-08 18:26:06.611558 Epoch 12 \n",
      "2024-12-08 18:26:20.661377 Batch 15 \n",
      "2024-12-08 18:26:23.528063 Batch 30 \n",
      "2024-12-08 18:26:26.280272 Batch 45 \n",
      "2024-12-08 18:26:28.848591 Batch 60 \n",
      "2024-12-08 18:26:31.917600 Batch 75 \n",
      "2024-12-08 18:26:35.015758 Batch 90 \n",
      "2024-12-08 18:26:37.886838 Batch 105 \n",
      "2024-12-08 18:26:40.235771 Batch 120 \n",
      "2024-12-08 18:26:43.000526 Batch 135 \n",
      "[Train] Accuracy: 66.3639%, Loss per batch: 1.0148\n",
      "2024-12-08 18:26:59.376378 Batch 15 \n",
      "2024-12-08 18:27:01.164706 Batch 30 \n",
      "2024-12-08 18:27:03.613171 Batch 45 \n",
      "2024-12-08 18:27:05.386622 Batch 60 \n",
      "[Val] Accuracy: 71.6943%, loss per batch: 0.9199\n",
      "Epoch 12: SGD lr 0.0031 -> 0.0028\n",
      "2024-12-08 18:27:06.375788 Epoch 13 \n",
      "2024-12-08 18:27:20.544216 Batch 15 \n",
      "2024-12-08 18:27:23.072197 Batch 30 \n",
      "2024-12-08 18:27:26.010710 Batch 45 \n",
      "2024-12-08 18:27:28.372797 Batch 60 \n",
      "2024-12-08 18:27:31.031422 Batch 75 \n",
      "2024-12-08 18:27:34.033345 Batch 90 \n",
      "2024-12-08 18:27:36.975615 Batch 105 \n",
      "2024-12-08 18:27:39.629320 Batch 120 \n",
      "2024-12-08 18:27:42.442621 Batch 135 \n",
      "[Train] Accuracy: 66.9659%, Loss per batch: 0.9937\n",
      "2024-12-08 18:27:59.228270 Batch 15 \n",
      "2024-12-08 18:28:01.356054 Batch 30 \n",
      "2024-12-08 18:28:04.110520 Batch 45 \n",
      "2024-12-08 18:28:06.021785 Batch 60 \n",
      "[Val] Accuracy: 72.3312%, loss per batch: 0.8463\n",
      "Epoch 13: SGD lr 0.0028 -> 0.0025\n",
      "2024-12-08 18:28:07.029982 Epoch 14 \n",
      "2024-12-08 18:28:21.362952 Batch 15 \n",
      "2024-12-08 18:28:24.100979 Batch 30 \n",
      "2024-12-08 18:28:27.381730 Batch 45 \n",
      "2024-12-08 18:28:30.656562 Batch 60 \n",
      "2024-12-08 18:28:33.257341 Batch 75 \n",
      "2024-12-08 18:28:36.151087 Batch 90 \n",
      "2024-12-08 18:28:39.293396 Batch 105 \n",
      "2024-12-08 18:28:42.077794 Batch 120 \n",
      "2024-12-08 18:28:44.720042 Batch 135 \n",
      "[Train] Accuracy: 67.4939%, Loss per batch: 0.9761\n",
      "2024-12-08 18:29:01.713764 Batch 15 \n",
      "2024-12-08 18:29:03.783135 Batch 30 \n",
      "2024-12-08 18:29:06.157114 Batch 45 \n",
      "2024-12-08 18:29:07.998481 Batch 60 \n",
      "[Val] Accuracy: 73.3248%, loss per batch: 0.826\n",
      "Epoch 14: SGD lr 0.0025 -> 0.0023\n",
      "2024-12-08 18:29:09.064411 Epoch 15 \n",
      "2024-12-08 18:29:23.276993 Batch 15 \n",
      "2024-12-08 18:29:26.109550 Batch 30 \n",
      "2024-12-08 18:29:28.893722 Batch 45 \n",
      "2024-12-08 18:29:31.883797 Batch 60 \n",
      "2024-12-08 18:29:34.612297 Batch 75 \n",
      "2024-12-08 18:29:37.232569 Batch 90 \n",
      "2024-12-08 18:29:40.063117 Batch 105 \n",
      "2024-12-08 18:29:42.939162 Batch 120 \n",
      "2024-12-08 18:29:45.835207 Batch 135 \n",
      "[Train] Accuracy: 68.5394%, Loss per batch: 0.9498\n",
      "2024-12-08 18:30:02.623474 Batch 15 \n",
      "2024-12-08 18:30:04.381939 Batch 30 \n",
      "2024-12-08 18:30:06.526678 Batch 45 \n",
      "2024-12-08 18:30:08.236211 Batch 60 \n",
      "[Val] Accuracy: 73.0191%, loss per batch: 0.8386\n",
      "Epoch 15: SGD lr 0.0023 -> 0.0021\n",
      "2024-12-08 18:30:09.222804 Epoch 16 \n",
      "2024-12-08 18:30:23.741323 Batch 15 \n",
      "2024-12-08 18:30:26.580902 Batch 30 \n",
      "2024-12-08 18:30:29.531222 Batch 45 \n",
      "2024-12-08 18:30:32.059438 Batch 60 \n",
      "2024-12-08 18:30:35.167216 Batch 75 \n",
      "2024-12-08 18:30:38.138455 Batch 90 \n",
      "2024-12-08 18:30:40.855411 Batch 105 \n",
      "2024-12-08 18:30:43.648722 Batch 120 \n",
      "2024-12-08 18:30:46.767316 Batch 135 \n",
      "[Train] Accuracy: 69.6061%, Loss per batch: 0.9226\n",
      "2024-12-08 18:31:03.228642 Batch 15 \n",
      "2024-12-08 18:31:05.258927 Batch 30 \n",
      "2024-12-08 18:31:07.316839 Batch 45 \n",
      "2024-12-08 18:31:09.275732 Batch 60 \n",
      "[Val] Accuracy: 73.2994%, loss per batch: 0.8231\n",
      "Epoch 16: SGD lr 0.0021 -> 0.0019\n",
      "2024-12-08 18:31:10.268749 Epoch 17 \n",
      "2024-12-08 18:31:26.100298 Batch 15 \n",
      "2024-12-08 18:31:28.640512 Batch 30 \n",
      "2024-12-08 18:31:32.125161 Batch 45 \n",
      "2024-12-08 18:31:35.569339 Batch 60 \n",
      "2024-12-08 18:31:38.850030 Batch 75 \n",
      "2024-12-08 18:31:41.552137 Batch 90 \n",
      "2024-12-08 18:31:44.483764 Batch 105 \n",
      "2024-12-08 18:31:48.037180 Batch 120 \n",
      "2024-12-08 18:31:50.913905 Batch 135 \n",
      "[Train] Accuracy: 69.6166%, Loss per batch: 0.9078\n",
      "2024-12-08 18:32:07.223700 Batch 15 \n",
      "2024-12-08 18:32:09.051284 Batch 30 \n",
      "2024-12-08 18:32:11.274921 Batch 45 \n",
      "2024-12-08 18:32:13.437167 Batch 60 \n",
      "[Val] Accuracy: 75.0318%, loss per batch: 0.7695\n",
      "Epoch 17: SGD lr 0.0019 -> 0.0017\n",
      "2024-12-08 18:32:14.459105 Epoch 18 \n",
      "2024-12-08 18:32:28.693268 Batch 15 \n",
      "2024-12-08 18:32:31.435905 Batch 30 \n",
      "2024-12-08 18:32:33.729973 Batch 45 \n",
      "2024-12-08 18:32:36.204537 Batch 60 \n",
      "2024-12-08 18:32:38.924891 Batch 75 \n",
      "2024-12-08 18:32:41.599342 Batch 90 \n",
      "2024-12-08 18:32:44.236952 Batch 105 \n",
      "2024-12-08 18:32:46.655204 Batch 120 \n",
      "2024-12-08 18:32:49.817808 Batch 135 \n",
      "[Train] Accuracy: 70.9579%, Loss per batch: 0.8757\n",
      "2024-12-08 18:33:05.955522 Batch 15 \n",
      "2024-12-08 18:33:07.840638 Batch 30 \n",
      "2024-12-08 18:33:10.104869 Batch 45 \n",
      "2024-12-08 18:33:11.862516 Batch 60 \n",
      "[Val] Accuracy: 75.3376%, loss per batch: 0.7679\n",
      "Epoch 18: SGD lr 0.0017 -> 0.0015\n",
      "2024-12-08 18:33:12.860999 Epoch 19 \n",
      "2024-12-08 18:33:27.087979 Batch 15 \n",
      "2024-12-08 18:33:29.776337 Batch 30 \n",
      "2024-12-08 18:33:32.552306 Batch 45 \n",
      "2024-12-08 18:33:35.296597 Batch 60 \n",
      "2024-12-08 18:33:38.483621 Batch 75 \n",
      "2024-12-08 18:33:40.982609 Batch 90 \n",
      "2024-12-08 18:33:44.422339 Batch 105 \n",
      "2024-12-08 18:33:47.330534 Batch 120 \n",
      "2024-12-08 18:33:50.238327 Batch 135 \n",
      "[Train] Accuracy: 71.7394%, Loss per batch: 0.8693\n",
      "2024-12-08 18:34:06.964658 Batch 15 \n",
      "2024-12-08 18:34:08.868511 Batch 30 \n",
      "2024-12-08 18:34:11.126191 Batch 45 \n",
      "2024-12-08 18:34:12.884405 Batch 60 \n",
      "[Val] Accuracy: 75.7962%, loss per batch: 0.7473\n",
      "Epoch 19: SGD lr 0.0015 -> 0.0014\n",
      "2024-12-08 18:34:13.869649 Epoch 20 \n",
      "2024-12-08 18:34:27.871437 Batch 15 \n",
      "2024-12-08 18:34:30.615226 Batch 30 \n",
      "2024-12-08 18:34:33.218833 Batch 45 \n",
      "2024-12-08 18:34:35.687182 Batch 60 \n",
      "2024-12-08 18:34:39.077321 Batch 75 \n",
      "2024-12-08 18:34:44.034966 Batch 90 \n",
      "2024-12-08 18:34:49.003606 Batch 105 \n",
      "2024-12-08 18:34:52.179946 Batch 120 \n",
      "2024-12-08 18:34:56.455463 Batch 135 \n",
      "[Train] Accuracy: 71.6866%, Loss per batch: 0.8598\n",
      "2024-12-08 18:35:14.013091 Batch 15 \n",
      "2024-12-08 18:35:15.783269 Batch 30 \n",
      "2024-12-08 18:35:17.520546 Batch 45 \n",
      "2024-12-08 18:35:19.235405 Batch 60 \n",
      "[Val] Accuracy: 76.535%, loss per batch: 0.7396\n",
      "Epoch 20: SGD lr 0.0014 -> 0.0012\n"
     ]
    }
   ],
   "source": [
    "history, gradient_stats = train(epochs, train_loader, val_loader, model, optimizer, \n",
    "                                loss_fn, scheduler, outputs_path='../../log/YOLOv8cls-version-5/training/', resume=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "New Python (GPU)",
   "language": "python",
   "name": "new_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
