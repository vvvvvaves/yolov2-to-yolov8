{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04833633-3c8f-4d84-8869-c36d9a0cd69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a0a4b-7479-40b1-abdd-1227b6940040",
   "metadata": {},
   "source": [
    "https://lernapparat.de/debug-device-assert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5258e5c-2fd1-45d6-bf25-96674d504f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9deb7c5-1256-4fdd-b447-69f25786c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43b63379-9363-448e-b927-2231dbb097b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import *\n",
    "from data_preprocessing import *\n",
    "from data_augmentation import *\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from Models.yolov8cls_path import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbdae16b-4251-46a6-865b-bfa81a9a5ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35628af8-5f33-4d45-8a3c-3ed52eac7ae3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n': {'d': 0.34, 'w': 0.25, 'mc': 1024},\n",
       " 's': {'d': 0.34, 'w': 0.5, 'mc': 1024},\n",
       " 'm': {'d': 0.67, 'w': 0.75, 'mc': 768},\n",
       " 'l': {'d': 1.0, 'w': 1.0, 'mc': 512},\n",
       " 'xl': {'d': 1.0, 'w': 1.25, 'mc': 512}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model.variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9a44f5d-8396-4654-b118-82473d81ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_classes=10, \n",
    "              residual_connection=True, \n",
    "              CSP=True, \n",
    "              add_hidden=True,\n",
    "              classifyV8=True,\n",
    "              bottleneck=0.5, \n",
    "              variant='s', \n",
    "              device=device, \n",
    "              dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b7a55ab-c490-4ed6-9c46-df61b5dee02a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (conv1): Conv(\n",
       "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act): SiLU()\n",
       "  )\n",
       "  (conv2): Conv(\n",
       "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act): SiLU()\n",
       "  )\n",
       "  (c2f1): C2f(\n",
       "    (conv1): Conv(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (conv2): Conv(\n",
       "      (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (n_blocks): ModuleList(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv(\n",
       "          (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (conv2): Conv(\n",
       "          (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv3): Conv(\n",
       "    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act): SiLU()\n",
       "  )\n",
       "  (c2f2): C2f(\n",
       "    (conv1): Conv(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (conv2): Conv(\n",
       "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (n_blocks): ModuleList(\n",
       "      (0-1): 2 x Bottleneck(\n",
       "        (conv1): Conv(\n",
       "          (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (conv2): Conv(\n",
       "          (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv4): Conv(\n",
       "    (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act): SiLU()\n",
       "  )\n",
       "  (c2f3): C2f(\n",
       "    (conv1): Conv(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (conv2): Conv(\n",
       "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (n_blocks): ModuleList(\n",
       "      (0-1): 2 x Bottleneck(\n",
       "        (conv1): Conv(\n",
       "          (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (conv2): Conv(\n",
       "          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv5): Conv(\n",
       "    (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act): SiLU()\n",
       "  )\n",
       "  (c2f4): C2f(\n",
       "    (conv1): Conv(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (conv2): Conv(\n",
       "      (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (n_blocks): ModuleList(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (conv2): Conv(\n",
       "          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classify): ClassifyV8(\n",
       "    (conv): Conv(\n",
       "      (conv): Conv2d(512, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act): SiLU()\n",
       "    )\n",
       "    (pool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (linear): Linear(in_features=1280, out_features=10, bias=True)\n",
       "    (softmax): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae3c5dd0-ac5d-4cbb-8463-d9ebe0621f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../../../datasets/imagenette2/'\n",
    "norms_path = os.path.join(data_path, 'norms.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e608e517-6da7-41c7-b62d-81cca6aecf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# means = get_means(path=norms_path, train_loader=None)\n",
    "# stds = get_stds(path=norms_path, train_loader=None)\n",
    "norms = get_norms(path=norms_path, train_loader=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1567c205-e99f-4a93-ae7a-20a457c2eb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "means, stds = norms['means'], norms['stds']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c4a012-f83a-495c-929b-939db6549642",
   "metadata": {},
   "source": [
    "\n",
    "Profiling your personal module \n",
    "https://pytorch.org/tutorials/beginner/profiler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb61344e-ef51-4068-9304-62ac2f686ae7",
   "metadata": {},
   "source": [
    "https://discuss.pytorch.org/t/how-to-prevent-overfitting/1902\n",
    "Right now, with my augmented dataset, at epoch 8, I am getting a testset Top1 accuracy of 45% but a trainset Top1 accuracy of 69%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4a702f-1ae5-4573-a299-a90db5b87b7f",
   "metadata": {},
   "source": [
    "You should strongly consider data augmentation in some meaningful way. If you’re attempting to do classification then think about what augmentations might add useful information and help distinguish classes in your dataset. In one of my cases, introducing background variation increased recognition rate by over 50%. Basically, with small datasets there is too much overfitting so you want the network to learn real-world distinctions vs. irrelevant artifacts like backgrounds / shadows etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d4f81f6-3dfe-4256-8c57-66f92768ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = transforms.Compose([transforms.RandomResizedCrop((224, 224)),\n",
    "                                              Augmentation(),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize(mean=means, std=stds)])\n",
    "transformations_val = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize(mean=means, std=stds)\n",
    "                                                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f555ed4-dfcf-4f5f-9a71-3e40a7ea2521",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageNetSubset(path=data_path, train=True, transform=transformations, half=False, show=False)\n",
    "val_dataset = ImageNetSubset(path=data_path, train=False, transform=transformations_val, half=False, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ae476e6-947b-46a5-bed5-729af9ee6bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab049c64-2683-432e-acd7-e3494df498f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3846504-a142-444a-85cb-d8ab411cc449",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fb7d049-0696-4341-991c-9b3bfbb0ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "daee47e0-df3f-4342-b462-ef2104f8a7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-06 16:58:56.602764 Epoch 1 \n",
      "2025-04-06 16:59:16.970678 Batch 15 \n",
      "2025-04-06 16:59:19.695843 Batch 30 \n",
      "2025-04-06 16:59:22.497966 Batch 45 \n",
      "2025-04-06 16:59:25.964383 Batch 60 \n",
      "2025-04-06 16:59:28.925614 Batch 75 \n",
      "2025-04-06 16:59:31.789236 Batch 90 \n",
      "2025-04-06 16:59:34.538711 Batch 105 \n",
      "2025-04-06 16:59:37.783342 Batch 120 \n",
      "2025-04-06 16:59:40.667622 Batch 135 \n",
      "[Train] Accuracy: 26.1907%, Loss per batch: 2.0574\n",
      "2025-04-06 16:59:59.849662 Batch 15 \n",
      "2025-04-06 17:00:01.800225 Batch 30 \n",
      "2025-04-06 17:00:04.260619 Batch 45 \n",
      "2025-04-06 17:00:06.320098 Batch 60 \n",
      "[Val] Accuracy: 31.8217%, loss per batch: 1.9032\n",
      "Epoch 1: SGD lr 0.0100 -> 0.0090\n",
      "2025-04-06 17:00:07.855354 Epoch 2 \n",
      "2025-04-06 17:00:24.899691 Batch 15 \n",
      "2025-04-06 17:00:27.418362 Batch 30 \n",
      "2025-04-06 17:00:30.168551 Batch 45 \n",
      "2025-04-06 17:00:33.009977 Batch 60 \n",
      "2025-04-06 17:00:35.619893 Batch 75 \n",
      "2025-04-06 17:00:38.154100 Batch 90 \n",
      "2025-04-06 17:00:41.056731 Batch 105 \n",
      "2025-04-06 17:00:43.784622 Batch 120 \n",
      "2025-04-06 17:00:46.603703 Batch 135 \n",
      "[Train] Accuracy: 37.2901%, Loss per batch: 1.7764\n",
      "2025-04-06 17:01:05.402568 Batch 15 \n",
      "2025-04-06 17:01:07.291717 Batch 30 \n",
      "2025-04-06 17:01:09.519627 Batch 45 \n",
      "2025-04-06 17:01:11.405404 Batch 60 \n",
      "[Val] Accuracy: 43.2611%, loss per batch: 1.6863\n",
      "Epoch 2: SGD lr 0.0090 -> 0.0081\n",
      "2025-04-06 17:01:12.474375 Epoch 3 \n",
      "2025-04-06 17:01:29.007398 Batch 15 \n",
      "2025-04-06 17:01:31.415547 Batch 30 \n",
      "2025-04-06 17:01:34.556878 Batch 45 \n",
      "2025-04-06 17:01:37.077696 Batch 60 \n",
      "2025-04-06 17:01:39.952777 Batch 75 \n",
      "2025-04-06 17:01:42.505704 Batch 90 \n",
      "2025-04-06 17:01:45.546592 Batch 105 \n",
      "2025-04-06 17:01:48.352501 Batch 120 \n",
      "2025-04-06 17:01:51.243088 Batch 135 \n",
      "[Train] Accuracy: 44.3025%, Loss per batch: 1.617\n",
      "2025-04-06 17:02:09.668460 Batch 15 \n",
      "2025-04-06 17:02:11.496305 Batch 30 \n",
      "2025-04-06 17:02:13.842046 Batch 45 \n",
      "2025-04-06 17:02:15.777099 Batch 60 \n",
      "[Val] Accuracy: 48.8153%, loss per batch: 1.5526\n",
      "Epoch 3: SGD lr 0.0081 -> 0.0073\n",
      "2025-04-06 17:02:16.852189 Epoch 4 \n",
      "2025-04-06 17:02:32.747109 Batch 15 \n",
      "2025-04-06 17:02:35.326136 Batch 30 \n",
      "2025-04-06 17:02:38.021396 Batch 45 \n",
      "2025-04-06 17:02:40.289415 Batch 60 \n",
      "2025-04-06 17:02:42.909693 Batch 75 \n",
      "2025-04-06 17:02:46.040275 Batch 90 \n",
      "2025-04-06 17:02:49.518814 Batch 105 \n",
      "2025-04-06 17:02:52.223657 Batch 120 \n",
      "2025-04-06 17:02:55.491376 Batch 135 \n",
      "[Train] Accuracy: 48.2416%, Loss per batch: 1.5207\n",
      "2025-04-06 17:03:19.393607 Batch 15 \n",
      "2025-04-06 17:03:21.261216 Batch 30 \n",
      "2025-04-06 17:03:23.371913 Batch 45 \n",
      "2025-04-06 17:03:25.112840 Batch 60 \n",
      "[Val] Accuracy: 48.9936%, loss per batch: 1.4678\n",
      "Epoch 4: SGD lr 0.0073 -> 0.0066\n",
      "2025-04-06 17:03:26.147986 Epoch 5 \n",
      "2025-04-06 17:03:41.555188 Batch 15 \n",
      "2025-04-06 17:03:44.216193 Batch 30 \n",
      "2025-04-06 17:03:47.043449 Batch 45 \n",
      "2025-04-06 17:03:49.357486 Batch 60 \n",
      "2025-04-06 17:03:52.346281 Batch 75 \n",
      "2025-04-06 17:03:55.091154 Batch 90 \n",
      "2025-04-06 17:03:57.460550 Batch 105 \n",
      "2025-04-06 17:04:00.068701 Batch 120 \n",
      "2025-04-06 17:04:02.802434 Batch 135 \n",
      "[Train] Accuracy: 51.4521%, Loss per batch: 1.4242\n",
      "2025-04-06 17:04:21.172013 Batch 15 \n",
      "2025-04-06 17:04:23.479755 Batch 30 \n",
      "2025-04-06 17:04:26.061323 Batch 45 \n",
      "2025-04-06 17:04:28.073573 Batch 60 \n",
      "[Val] Accuracy: 48.3057%, loss per batch: 1.5162\n",
      "Epoch 5: SGD lr 0.0066 -> 0.0059\n",
      "2025-04-06 17:04:29.168545 Epoch 6 \n",
      "2025-04-06 17:04:47.341763 Batch 15 \n",
      "2025-04-06 17:04:50.182488 Batch 30 \n",
      "2025-04-06 17:04:52.712718 Batch 45 \n",
      "2025-04-06 17:04:55.764160 Batch 60 \n",
      "2025-04-06 17:04:58.432638 Batch 75 \n",
      "2025-04-06 17:05:01.156355 Batch 90 \n",
      "2025-04-06 17:05:03.662602 Batch 105 \n",
      "2025-04-06 17:05:06.492432 Batch 120 \n",
      "2025-04-06 17:05:09.002755 Batch 135 \n",
      "[Train] Accuracy: 54.1345%, Loss per batch: 1.3443\n",
      "2025-04-06 17:05:27.720828 Batch 15 \n",
      "2025-04-06 17:05:29.529627 Batch 30 \n",
      "2025-04-06 17:05:31.751058 Batch 45 \n",
      "2025-04-06 17:05:33.455788 Batch 60 \n",
      "[Val] Accuracy: 56.051%, loss per batch: 1.2939\n",
      "Epoch 6: SGD lr 0.0059 -> 0.0053\n",
      "2025-04-06 17:05:34.448809 Epoch 7 \n",
      "2025-04-06 17:05:50.039392 Batch 15 \n",
      "2025-04-06 17:05:52.642249 Batch 30 \n",
      "2025-04-06 17:05:55.521896 Batch 45 \n",
      "2025-04-06 17:05:57.834318 Batch 60 \n",
      "2025-04-06 17:06:00.610162 Batch 75 \n",
      "2025-04-06 17:06:03.311679 Batch 90 \n",
      "2025-04-06 17:06:06.216547 Batch 105 \n",
      "2025-04-06 17:06:08.710402 Batch 120 \n",
      "2025-04-06 17:06:11.492693 Batch 135 \n",
      "[Train] Accuracy: 56.3206%, Loss per batch: 1.2889\n",
      "2025-04-06 17:06:29.283415 Batch 15 \n",
      "2025-04-06 17:06:31.311357 Batch 30 \n",
      "2025-04-06 17:06:33.484749 Batch 45 \n",
      "2025-04-06 17:06:35.223433 Batch 60 \n",
      "[Val] Accuracy: 63.4904%, loss per batch: 1.1357\n",
      "Epoch 7: SGD lr 0.0053 -> 0.0048\n",
      "2025-04-06 17:06:36.228661 Epoch 8 \n",
      "2025-04-06 17:06:51.910030 Batch 15 \n",
      "2025-04-06 17:06:54.392041 Batch 30 \n",
      "2025-04-06 17:06:56.987563 Batch 45 \n",
      "2025-04-06 17:06:59.367578 Batch 60 \n",
      "2025-04-06 17:07:02.000034 Batch 75 \n",
      "2025-04-06 17:07:04.745307 Batch 90 \n",
      "2025-04-06 17:07:07.445522 Batch 105 \n",
      "2025-04-06 17:07:09.924696 Batch 120 \n",
      "2025-04-06 17:07:12.657408 Batch 135 \n",
      "[Train] Accuracy: 58.9608%, Loss per batch: 1.2186\n",
      "2025-04-06 17:07:30.352659 Batch 15 \n",
      "2025-04-06 17:07:32.026548 Batch 30 \n",
      "2025-04-06 17:07:34.226685 Batch 45 \n",
      "2025-04-06 17:07:35.959080 Batch 60 \n",
      "[Val] Accuracy: 63.0573%, loss per batch: 1.118\n",
      "Epoch 8: SGD lr 0.0048 -> 0.0043\n",
      "2025-04-06 17:07:36.940606 Epoch 9 \n",
      "2025-04-06 17:07:52.539621 Batch 15 \n",
      "2025-04-06 17:07:55.251665 Batch 30 \n",
      "2025-04-06 17:07:57.527715 Batch 45 \n",
      "2025-04-06 17:08:00.076563 Batch 60 \n",
      "2025-04-06 17:08:02.908470 Batch 75 \n",
      "2025-04-06 17:08:05.286665 Batch 90 \n",
      "2025-04-06 17:08:07.962831 Batch 105 \n",
      "2025-04-06 17:08:10.597315 Batch 120 \n",
      "2025-04-06 17:08:13.426517 Batch 135 \n",
      "[Train] Accuracy: 60.3654%, Loss per batch: 1.1717\n",
      "2025-04-06 17:08:30.866666 Batch 15 \n",
      "2025-04-06 17:08:32.570360 Batch 30 \n",
      "2025-04-06 17:08:34.766357 Batch 45 \n",
      "2025-04-06 17:08:36.480187 Batch 60 \n",
      "[Val] Accuracy: 65.2229%, loss per batch: 1.0416\n",
      "Epoch 9: SGD lr 0.0043 -> 0.0039\n",
      "2025-04-06 17:08:37.545380 Epoch 10 \n",
      "2025-04-06 17:08:53.018054 Batch 15 \n",
      "2025-04-06 17:08:55.813705 Batch 30 \n",
      "2025-04-06 17:08:58.315185 Batch 45 \n",
      "2025-04-06 17:09:00.950574 Batch 60 \n",
      "2025-04-06 17:09:03.268143 Batch 75 \n",
      "2025-04-06 17:09:05.908507 Batch 90 \n",
      "2025-04-06 17:09:08.294550 Batch 105 \n",
      "2025-04-06 17:09:11.249299 Batch 120 \n",
      "2025-04-06 17:09:13.679490 Batch 135 \n",
      "[Train] Accuracy: 61.2103%, Loss per batch: 1.1516\n",
      "2025-04-06 17:09:31.236477 Batch 15 \n",
      "2025-04-06 17:09:32.963293 Batch 30 \n",
      "2025-04-06 17:09:35.047336 Batch 45 \n",
      "2025-04-06 17:09:36.798560 Batch 60 \n",
      "[Val] Accuracy: 61.4013%, loss per batch: 1.1709\n",
      "Epoch 10: SGD lr 0.0039 -> 0.0035\n"
     ]
    }
   ],
   "source": [
    "history, gradient_stats = train(epochs, train_loader, val_loader, model, optimizer, loss_fn, scheduler, outputs_path='../../log/YOLOv8cls-version-6/training/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fa7035e-ebd2-42ab-86e4-9e5e198f6f84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Me\\PJAIT\\Thesis\\Code\\yolov2-to-yolov8\\Backbone experiments\\YOLOv8s-cls Path\\Version 7\\../../..\\train.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(os.path.join(outputs_path, f\"state.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-06 17:09:38.002728 Epoch 11 \n",
      "2025-04-06 17:09:53.528253 Batch 15 \n",
      "2025-04-06 17:09:56.216705 Batch 30 \n",
      "2025-04-06 17:09:58.748998 Batch 45 \n",
      "2025-04-06 17:10:01.482702 Batch 60 \n",
      "2025-04-06 17:10:04.460796 Batch 75 \n",
      "2025-04-06 17:10:07.175423 Batch 90 \n",
      "2025-04-06 17:10:09.881386 Batch 105 \n",
      "2025-04-06 17:10:12.104509 Batch 120 \n",
      "2025-04-06 17:10:14.927322 Batch 135 \n",
      "[Train] Accuracy: 62.9422%, Loss per batch: 1.0984\n",
      "2025-04-06 17:10:32.777246 Batch 15 \n",
      "2025-04-06 17:10:34.488695 Batch 30 \n",
      "2025-04-06 17:10:36.675732 Batch 45 \n",
      "2025-04-06 17:10:38.328622 Batch 60 \n",
      "[Val] Accuracy: 68.6369%, loss per batch: 0.9784\n",
      "Epoch 11: SGD lr 0.0035 -> 0.0031\n",
      "2025-04-06 17:10:39.317540 Epoch 12 \n",
      "2025-04-06 17:10:54.676498 Batch 15 \n",
      "2025-04-06 17:10:57.140910 Batch 30 \n",
      "2025-04-06 17:11:00.123207 Batch 45 \n",
      "2025-04-06 17:11:02.659192 Batch 60 \n",
      "2025-04-06 17:11:05.356490 Batch 75 \n",
      "2025-04-06 17:11:07.950648 Batch 90 \n",
      "2025-04-06 17:11:10.440509 Batch 105 \n",
      "2025-04-06 17:11:12.700677 Batch 120 \n",
      "2025-04-06 17:11:15.346479 Batch 135 \n",
      "[Train] Accuracy: 64.0511%, Loss per batch: 1.073\n",
      "2025-04-06 17:11:32.548977 Batch 15 \n",
      "2025-04-06 17:11:34.360372 Batch 30 \n",
      "2025-04-06 17:11:36.484440 Batch 45 \n",
      "2025-04-06 17:11:38.082427 Batch 60 \n",
      "[Val] Accuracy: 65.7834%, loss per batch: 1.0758\n",
      "Epoch 12: SGD lr 0.0031 -> 0.0028\n",
      "2025-04-06 17:11:39.044837 Epoch 13 \n",
      "2025-04-06 17:11:54.391017 Batch 15 \n",
      "2025-04-06 17:11:57.195876 Batch 30 \n",
      "2025-04-06 17:11:59.333370 Batch 45 \n",
      "2025-04-06 17:12:02.032381 Batch 60 \n",
      "2025-04-06 17:12:04.756207 Batch 75 \n",
      "2025-04-06 17:12:07.501842 Batch 90 \n",
      "2025-04-06 17:12:09.724527 Batch 105 \n",
      "2025-04-06 17:12:12.281590 Batch 120 \n",
      "2025-04-06 17:12:14.900497 Batch 135 \n",
      "[Train] Accuracy: 64.8537%, Loss per batch: 1.0493\n",
      "2025-04-06 17:12:32.820878 Batch 15 \n",
      "2025-04-06 17:12:34.617524 Batch 30 \n",
      "2025-04-06 17:12:36.710461 Batch 45 \n",
      "2025-04-06 17:12:38.675183 Batch 60 \n",
      "[Val] Accuracy: 70.1656%, loss per batch: 0.9074\n",
      "Epoch 13: SGD lr 0.0028 -> 0.0025\n",
      "2025-04-06 17:12:39.754525 Epoch 14 \n",
      "2025-04-06 17:12:55.472578 Batch 15 \n",
      "2025-04-06 17:12:58.064876 Batch 30 \n",
      "2025-04-06 17:13:00.555493 Batch 45 \n",
      "2025-04-06 17:13:03.113633 Batch 60 \n",
      "2025-04-06 17:13:05.304742 Batch 75 \n",
      "2025-04-06 17:13:07.939064 Batch 90 \n",
      "2025-04-06 17:13:10.703988 Batch 105 \n",
      "2025-04-06 17:13:13.318299 Batch 120 \n",
      "2025-04-06 17:13:15.675291 Batch 135 \n",
      "[Train] Accuracy: 67.1454%, Loss per batch: 0.9961\n",
      "2025-04-06 17:13:33.661263 Batch 15 \n",
      "2025-04-06 17:13:35.527315 Batch 30 \n",
      "2025-04-06 17:13:37.757075 Batch 45 \n",
      "2025-04-06 17:13:39.496516 Batch 60 \n",
      "[Val] Accuracy: 68.8662%, loss per batch: 0.9561\n",
      "Epoch 14: SGD lr 0.0025 -> 0.0023\n",
      "2025-04-06 17:13:40.533060 Epoch 15 \n",
      "2025-04-06 17:13:56.126453 Batch 15 \n",
      "2025-04-06 17:13:59.118711 Batch 30 \n",
      "2025-04-06 17:14:02.209541 Batch 45 \n",
      "2025-04-06 17:14:04.406736 Batch 60 \n",
      "2025-04-06 17:14:07.060664 Batch 75 \n",
      "2025-04-06 17:14:09.823952 Batch 90 \n",
      "2025-04-06 17:14:12.504255 Batch 105 \n",
      "2025-04-06 17:14:15.063231 Batch 120 \n",
      "2025-04-06 17:14:17.856314 Batch 135 \n",
      "[Train] Accuracy: 67.325%, Loss per batch: 0.9895\n",
      "2025-04-06 17:14:35.464924 Batch 15 \n",
      "2025-04-06 17:14:37.236892 Batch 30 \n",
      "2025-04-06 17:14:39.414337 Batch 45 \n",
      "2025-04-06 17:14:41.061484 Batch 60 \n",
      "[Val] Accuracy: 70.4713%, loss per batch: 0.9112\n",
      "Epoch 15: SGD lr 0.0023 -> 0.0021\n",
      "2025-04-06 17:14:42.101562 Epoch 16 \n",
      "2025-04-06 17:14:57.662628 Batch 15 \n",
      "2025-04-06 17:15:00.205476 Batch 30 \n",
      "2025-04-06 17:15:03.465457 Batch 45 \n",
      "2025-04-06 17:15:06.355249 Batch 60 \n",
      "2025-04-06 17:15:08.663476 Batch 75 \n",
      "2025-04-06 17:15:11.372654 Batch 90 \n",
      "2025-04-06 17:15:13.639433 Batch 105 \n",
      "2025-04-06 17:15:16.213266 Batch 120 \n",
      "2025-04-06 17:15:18.819292 Batch 135 \n",
      "[Train] Accuracy: 67.9375%, Loss per batch: 0.9735\n",
      "2025-04-06 17:15:38.546663 Batch 15 \n",
      "2025-04-06 17:15:40.385670 Batch 30 \n",
      "2025-04-06 17:15:42.523191 Batch 45 \n",
      "2025-04-06 17:15:44.252564 Batch 60 \n",
      "[Val] Accuracy: 72.4841%, loss per batch: 0.8567\n",
      "Epoch 16: SGD lr 0.0021 -> 0.0019\n",
      "2025-04-06 17:15:45.273312 Epoch 17 \n",
      "2025-04-06 17:16:01.238147 Batch 15 \n",
      "2025-04-06 17:16:03.838107 Batch 30 \n",
      "2025-04-06 17:16:06.567204 Batch 45 \n",
      "2025-04-06 17:16:09.169264 Batch 60 \n",
      "2025-04-06 17:16:11.575434 Batch 75 \n",
      "2025-04-06 17:16:14.074534 Batch 90 \n",
      "2025-04-06 17:16:16.461080 Batch 105 \n",
      "2025-04-06 17:16:19.076531 Batch 120 \n",
      "2025-04-06 17:16:21.598824 Batch 135 \n",
      "[Train] Accuracy: 68.6239%, Loss per batch: 0.9498\n",
      "2025-04-06 17:16:40.937659 Batch 15 \n",
      "2025-04-06 17:16:42.716195 Batch 30 \n",
      "2025-04-06 17:16:44.973247 Batch 45 \n",
      "2025-04-06 17:16:46.754473 Batch 60 \n",
      "[Val] Accuracy: 74.0127%, loss per batch: 0.8093\n",
      "Epoch 17: SGD lr 0.0019 -> 0.0017\n",
      "2025-04-06 17:16:47.803619 Epoch 18 \n",
      "2025-04-06 17:17:03.493928 Batch 15 \n",
      "2025-04-06 17:17:06.184559 Batch 30 \n",
      "2025-04-06 17:17:08.792887 Batch 45 \n",
      "2025-04-06 17:17:11.288686 Batch 60 \n",
      "2025-04-06 17:17:13.977448 Batch 75 \n",
      "2025-04-06 17:17:16.441539 Batch 90 \n",
      "2025-04-06 17:17:18.913595 Batch 105 \n",
      "2025-04-06 17:17:21.391313 Batch 120 \n",
      "2025-04-06 17:17:23.893559 Batch 135 \n",
      "[Train] Accuracy: 68.6979%, Loss per batch: 0.9394\n",
      "2025-04-06 17:17:42.479624 Batch 15 \n",
      "2025-04-06 17:17:44.440234 Batch 30 \n",
      "2025-04-06 17:17:46.534249 Batch 45 \n",
      "2025-04-06 17:17:48.232231 Batch 60 \n",
      "[Val] Accuracy: 74.6242%, loss per batch: 0.7947\n",
      "Epoch 18: SGD lr 0.0017 -> 0.0015\n",
      "2025-04-06 17:17:49.264347 Epoch 19 \n",
      "2025-04-06 17:18:04.899530 Batch 15 \n",
      "2025-04-06 17:18:07.524875 Batch 30 \n",
      "2025-04-06 17:18:09.933134 Batch 45 \n",
      "2025-04-06 17:18:12.487109 Batch 60 \n",
      "2025-04-06 17:18:15.237720 Batch 75 \n",
      "2025-04-06 17:18:17.838085 Batch 90 \n",
      "2025-04-06 17:18:20.034619 Batch 105 \n",
      "2025-04-06 17:18:22.669590 Batch 120 \n",
      "2025-04-06 17:18:25.927495 Batch 135 \n",
      "[Train] Accuracy: 69.2893%, Loss per batch: 0.9144\n",
      "2025-04-06 17:18:44.326681 Batch 15 \n",
      "2025-04-06 17:18:46.224521 Batch 30 \n",
      "2025-04-06 17:18:48.566168 Batch 45 \n",
      "2025-04-06 17:18:50.282186 Batch 60 \n",
      "[Val] Accuracy: 73.5541%, loss per batch: 0.8084\n",
      "Epoch 19: SGD lr 0.0015 -> 0.0014\n",
      "2025-04-06 17:18:51.345959 Epoch 20 \n",
      "2025-04-06 17:19:06.765639 Batch 15 \n",
      "2025-04-06 17:19:09.213230 Batch 30 \n",
      "2025-04-06 17:19:11.691049 Batch 45 \n",
      "2025-04-06 17:19:14.169845 Batch 60 \n",
      "2025-04-06 17:19:16.685345 Batch 75 \n",
      "2025-04-06 17:19:19.244627 Batch 90 \n",
      "2025-04-06 17:19:21.772547 Batch 105 \n",
      "2025-04-06 17:19:24.114612 Batch 120 \n",
      "2025-04-06 17:19:26.942539 Batch 135 \n",
      "[Train] Accuracy: 69.8912%, Loss per batch: 0.9006\n",
      "2025-04-06 17:19:45.261531 Batch 15 \n",
      "2025-04-06 17:19:47.143215 Batch 30 \n",
      "2025-04-06 17:19:49.258809 Batch 45 \n",
      "2025-04-06 17:19:51.189578 Batch 60 \n",
      "[Val] Accuracy: 74.4459%, loss per batch: 0.8038\n",
      "Epoch 20: SGD lr 0.0014 -> 0.0012\n"
     ]
    }
   ],
   "source": [
    "history, gradient_stats = train(epochs, train_loader, val_loader, model, optimizer, \n",
    "                                loss_fn, scheduler, outputs_path='../../log/YOLOv8cls-version-6/training/', resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbcfe3f-53c8-4faf-bdad-2e1d34940199",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "New Python (GPU)",
   "language": "python",
   "name": "new_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
