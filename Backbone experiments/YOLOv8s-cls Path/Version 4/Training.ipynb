{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04833633-3c8f-4d84-8869-c36d9a0cd69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a0a4b-7479-40b1-abdd-1227b6940040",
   "metadata": {},
   "source": [
    "https://lernapparat.de/debug-device-assert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5258e5c-2fd1-45d6-bf25-96674d504f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9deb7c5-1256-4fdd-b447-69f25786c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43b63379-9363-448e-b927-2231dbb097b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import *\n",
    "from data_preprocessing import *\n",
    "from data_augmentation import *\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from Models.yolov8cls_path import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbdae16b-4251-46a6-865b-bfa81a9a5ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9a44f5d-8396-4654-b118-82473d81ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_classes=10, \n",
    "              residual_connection=True, \n",
    "              CSP=True, \n",
    "              add_hidden=True,\n",
    "              classifyV8=False,\n",
    "              bottleneck=1.0, \n",
    "              variant='s', \n",
    "              device=device, \n",
    "              dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae3c5dd0-ac5d-4cbb-8463-d9ebe0621f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../../../datasets/imagenette2/'\n",
    "norms_path = os.path.join(data_path, 'norms.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e608e517-6da7-41c7-b62d-81cca6aecf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means are: [0.44969913363456726, 0.44868946075439453, 0.45163223147392273]\n",
      "stds are: [0.28648287057876587, 0.28796446323394775, 0.2865694761276245]\n"
     ]
    }
   ],
   "source": [
    "means = get_means(path=norms_path, train_loader=None)\n",
    "stds = get_stds(path=norms_path, train_loader=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c4a012-f83a-495c-929b-939db6549642",
   "metadata": {},
   "source": [
    "\n",
    "Profiling your personal module \n",
    "https://pytorch.org/tutorials/beginner/profiler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb61344e-ef51-4068-9304-62ac2f686ae7",
   "metadata": {},
   "source": [
    "https://discuss.pytorch.org/t/how-to-prevent-overfitting/1902\n",
    "Right now, with my augmented dataset, at epoch 8, I am getting a testset Top1 accuracy of 45% but a trainset Top1 accuracy of 69%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4a702f-1ae5-4573-a299-a90db5b87b7f",
   "metadata": {},
   "source": [
    "You should strongly consider data augmentation in some meaningful way. If youâ€™re attempting to do classification then think about what augmentations might add useful information and help distinguish classes in your dataset. In one of my cases, introducing background variation increased recognition rate by over 50%. Basically, with small datasets there is too much overfitting so you want the network to learn real-world distinctions vs. irrelevant artifacts like backgrounds / shadows etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d4f81f6-3dfe-4256-8c57-66f92768ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = transforms.Compose([transforms.RandomResizedCrop((224, 224)),\n",
    "                                              Augmentation(),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize(mean=means, std=stds)])\n",
    "transformations_val = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize(mean=means, std=stds)\n",
    "                                                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f555ed4-dfcf-4f5f-9a71-3e40a7ea2521",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageNetSubset(path=data_path, train=True, transform=transformations, half=False, show=False)\n",
    "val_dataset = ImageNetSubset(path=data_path, train=False, transform=transformations_val, half=False, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ae476e6-947b-46a5-bed5-729af9ee6bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab049c64-2683-432e-acd7-e3494df498f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3846504-a142-444a-85cb-d8ab411cc449",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fb7d049-0696-4341-991c-9b3bfbb0ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daee47e0-df3f-4342-b462-ef2104f8a7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-08 20:35:38.517374 Epoch 1 \n",
      "2024-12-08 20:35:52.222891 Batch 15 \n",
      "2024-12-08 20:35:54.778926 Batch 30 \n",
      "2024-12-08 20:35:57.260733 Batch 45 \n",
      "2024-12-08 20:35:59.702554 Batch 60 \n",
      "2024-12-08 20:36:02.184153 Batch 75 \n",
      "2024-12-08 20:36:04.588491 Batch 90 \n",
      "2024-12-08 20:36:06.576683 Batch 105 \n",
      "2024-12-08 20:36:08.834560 Batch 120 \n",
      "2024-12-08 20:36:11.474636 Batch 135 \n",
      "[Train] Accuracy: 26.0112%, Loss per batch: 2.0539\n",
      "2024-12-08 20:36:25.885564 Batch 15 \n",
      "2024-12-08 20:36:27.614711 Batch 30 \n",
      "2024-12-08 20:36:29.525386 Batch 45 \n",
      "2024-12-08 20:36:31.091705 Batch 60 \n",
      "[Val] Accuracy: 35.8981%, loss per batch: 1.8608\n",
      "Epoch 1: SGD lr 0.0100 -> 0.0090\n",
      "2024-12-08 20:36:32.215248 Epoch 2 \n",
      "2024-12-08 20:36:44.867413 Batch 15 \n",
      "2024-12-08 20:36:47.236158 Batch 30 \n",
      "2024-12-08 20:36:49.551075 Batch 45 \n",
      "2024-12-08 20:36:51.778730 Batch 60 \n",
      "2024-12-08 20:36:54.335864 Batch 75 \n",
      "2024-12-08 20:36:56.372106 Batch 90 \n",
      "2024-12-08 20:36:58.869974 Batch 105 \n",
      "2024-12-08 20:37:01.371510 Batch 120 \n",
      "2024-12-08 20:37:03.662510 Batch 135 \n",
      "[Train] Accuracy: 38.4518%, Loss per batch: 1.7779\n",
      "2024-12-08 20:37:18.360955 Batch 15 \n",
      "2024-12-08 20:37:19.899662 Batch 30 \n",
      "2024-12-08 20:37:21.834089 Batch 45 \n",
      "2024-12-08 20:37:23.292138 Batch 60 \n",
      "[Val] Accuracy: 29.6815%, loss per batch: 2.212\n",
      "Epoch 2: SGD lr 0.0090 -> 0.0081\n",
      "2024-12-08 20:37:24.201158 Epoch 3 \n",
      "2024-12-08 20:37:36.835208 Batch 15 \n",
      "2024-12-08 20:37:39.251791 Batch 30 \n",
      "2024-12-08 20:37:41.915926 Batch 45 \n",
      "2024-12-08 20:37:44.057596 Batch 60 \n",
      "2024-12-08 20:37:46.508334 Batch 75 \n",
      "2024-12-08 20:37:48.980274 Batch 90 \n",
      "2024-12-08 20:37:51.241169 Batch 105 \n",
      "2024-12-08 20:37:53.450365 Batch 120 \n",
      "2024-12-08 20:37:55.746513 Batch 135 \n",
      "[Train] Accuracy: 43.0774%, Loss per batch: 1.6454\n",
      "2024-12-08 20:38:10.748281 Batch 15 \n",
      "2024-12-08 20:38:12.465292 Batch 30 \n",
      "2024-12-08 20:38:14.370181 Batch 45 \n",
      "2024-12-08 20:38:15.854032 Batch 60 \n",
      "[Val] Accuracy: 39.4395%, loss per batch: 1.8231\n",
      "Epoch 3: SGD lr 0.0081 -> 0.0073\n",
      "2024-12-08 20:38:16.767817 Epoch 4 \n",
      "2024-12-08 20:38:29.778837 Batch 15 \n",
      "2024-12-08 20:38:32.994155 Batch 30 \n",
      "2024-12-08 20:38:35.749315 Batch 45 \n",
      "2024-12-08 20:38:38.281084 Batch 60 \n",
      "2024-12-08 20:38:41.167101 Batch 75 \n",
      "2024-12-08 20:38:43.771241 Batch 90 \n",
      "2024-12-08 20:38:46.005011 Batch 105 \n",
      "2024-12-08 20:38:48.499026 Batch 120 \n",
      "2024-12-08 20:38:50.903900 Batch 135 \n",
      "[Train] Accuracy: 47.3334%, Loss per batch: 1.5365\n",
      "2024-12-08 20:39:06.772239 Batch 15 \n",
      "2024-12-08 20:39:08.585009 Batch 30 \n",
      "2024-12-08 20:39:10.528439 Batch 45 \n",
      "2024-12-08 20:39:12.059394 Batch 60 \n",
      "[Val] Accuracy: 52.9936%, loss per batch: 1.4281\n",
      "Epoch 4: SGD lr 0.0073 -> 0.0066\n",
      "2024-12-08 20:39:12.990803 Epoch 5 \n",
      "2024-12-08 20:39:25.948005 Batch 15 \n",
      "2024-12-08 20:39:28.469541 Batch 30 \n",
      "2024-12-08 20:39:30.652978 Batch 45 \n",
      "2024-12-08 20:39:33.371203 Batch 60 \n",
      "2024-12-08 20:39:35.852039 Batch 75 \n",
      "2024-12-08 20:39:38.530472 Batch 90 \n",
      "2024-12-08 20:39:40.710682 Batch 105 \n",
      "2024-12-08 20:39:43.230707 Batch 120 \n",
      "2024-12-08 20:39:45.726519 Batch 135 \n",
      "[Train] Accuracy: 51.4732%, Loss per batch: 1.4373\n",
      "2024-12-08 20:40:00.868946 Batch 15 \n",
      "2024-12-08 20:40:02.548566 Batch 30 \n",
      "2024-12-08 20:40:04.490950 Batch 45 \n",
      "2024-12-08 20:40:06.132682 Batch 60 \n",
      "[Val] Accuracy: 53.5796%, loss per batch: 1.3563\n",
      "Epoch 5: SGD lr 0.0066 -> 0.0059\n",
      "2024-12-08 20:40:07.085089 Epoch 6 \n",
      "2024-12-08 20:40:20.281913 Batch 15 \n",
      "2024-12-08 20:40:22.498713 Batch 30 \n",
      "2024-12-08 20:40:24.799083 Batch 45 \n",
      "2024-12-08 20:40:27.381087 Batch 60 \n",
      "2024-12-08 20:40:29.929619 Batch 75 \n",
      "2024-12-08 20:40:32.154421 Batch 90 \n",
      "2024-12-08 20:40:34.652598 Batch 105 \n",
      "2024-12-08 20:40:37.113979 Batch 120 \n",
      "2024-12-08 20:40:39.667160 Batch 135 \n",
      "[Train] Accuracy: 54.6626%, Loss per batch: 1.3508\n",
      "2024-12-08 20:40:54.899836 Batch 15 \n",
      "2024-12-08 20:40:56.496125 Batch 30 \n",
      "2024-12-08 20:40:58.491927 Batch 45 \n",
      "2024-12-08 20:41:00.044066 Batch 60 \n",
      "[Val] Accuracy: 56.2548%, loss per batch: 1.2896\n",
      "Epoch 6: SGD lr 0.0059 -> 0.0053\n",
      "2024-12-08 20:41:00.989633 Epoch 7 \n",
      "2024-12-08 20:41:14.662266 Batch 15 \n",
      "2024-12-08 20:41:17.258241 Batch 30 \n",
      "2024-12-08 20:41:19.899743 Batch 45 \n",
      "2024-12-08 20:41:22.667608 Batch 60 \n",
      "2024-12-08 20:41:25.185835 Batch 75 \n",
      "2024-12-08 20:41:27.501375 Batch 90 \n",
      "2024-12-08 20:41:30.049977 Batch 105 \n",
      "2024-12-08 20:41:32.635347 Batch 120 \n",
      "2024-12-08 20:41:35.367477 Batch 135 \n",
      "[Train] Accuracy: 56.8064%, Loss per batch: 1.2836\n",
      "2024-12-08 20:41:51.425109 Batch 15 \n",
      "2024-12-08 20:41:53.149282 Batch 30 \n",
      "2024-12-08 20:41:55.282677 Batch 45 \n",
      "2024-12-08 20:41:56.885145 Batch 60 \n",
      "[Val] Accuracy: 60.0%, loss per batch: 1.2263\n",
      "Epoch 7: SGD lr 0.0053 -> 0.0048\n",
      "2024-12-08 20:41:57.853882 Epoch 8 \n",
      "2024-12-08 20:42:12.319928 Batch 15 \n",
      "2024-12-08 20:42:15.364336 Batch 30 \n",
      "2024-12-08 20:42:18.066019 Batch 45 \n",
      "2024-12-08 20:42:20.720954 Batch 60 \n",
      "2024-12-08 20:42:23.583761 Batch 75 \n",
      "2024-12-08 20:42:26.783512 Batch 90 \n",
      "2024-12-08 20:42:29.388939 Batch 105 \n",
      "2024-12-08 20:42:32.086734 Batch 120 \n",
      "2024-12-08 20:42:34.896193 Batch 135 \n",
      "[Train] Accuracy: 59.1615%, Loss per batch: 1.2296\n",
      "2024-12-08 20:42:51.107380 Batch 15 \n",
      "2024-12-08 20:42:52.981648 Batch 30 \n",
      "2024-12-08 20:42:55.162834 Batch 45 \n",
      "2024-12-08 20:42:56.970099 Batch 60 \n",
      "[Val] Accuracy: 63.3885%, loss per batch: 1.1037\n",
      "Epoch 8: SGD lr 0.0048 -> 0.0043\n",
      "2024-12-08 20:42:57.934535 Epoch 9 \n",
      "2024-12-08 20:43:12.033793 Batch 15 \n",
      "2024-12-08 20:43:14.935583 Batch 30 \n",
      "2024-12-08 20:43:17.741435 Batch 45 \n",
      "2024-12-08 20:43:20.705717 Batch 60 \n",
      "2024-12-08 20:43:23.452893 Batch 75 \n",
      "2024-12-08 20:43:26.541908 Batch 90 \n",
      "2024-12-08 20:43:29.169242 Batch 105 \n",
      "2024-12-08 20:43:32.151574 Batch 120 \n",
      "2024-12-08 20:43:35.365235 Batch 135 \n",
      "[Train] Accuracy: 61.1152%, Loss per batch: 1.1748\n",
      "2024-12-08 20:43:52.668082 Batch 15 \n",
      "2024-12-08 20:43:54.668940 Batch 30 \n",
      "2024-12-08 20:43:57.194182 Batch 45 \n",
      "2024-12-08 20:43:59.017241 Batch 60 \n",
      "[Val] Accuracy: 66.828%, loss per batch: 1.0218\n",
      "Epoch 9: SGD lr 0.0043 -> 0.0039\n",
      "2024-12-08 20:44:00.036850 Epoch 10 \n",
      "2024-12-08 20:44:14.950739 Batch 15 \n",
      "2024-12-08 20:44:17.697515 Batch 30 \n",
      "2024-12-08 20:44:20.343380 Batch 45 \n",
      "2024-12-08 20:44:22.918171 Batch 60 \n",
      "2024-12-08 20:44:25.903127 Batch 75 \n",
      "2024-12-08 20:44:28.793055 Batch 90 \n",
      "2024-12-08 20:44:31.609169 Batch 105 \n",
      "2024-12-08 20:44:34.793961 Batch 120 \n",
      "2024-12-08 20:44:37.712904 Batch 135 \n",
      "[Train] Accuracy: 62.372%, Loss per batch: 1.131\n",
      "2024-12-08 20:44:57.728073 Batch 15 \n",
      "2024-12-08 20:45:00.006898 Batch 30 \n",
      "2024-12-08 20:45:02.762598 Batch 45 \n",
      "2024-12-08 20:45:04.822806 Batch 60 \n",
      "[Val] Accuracy: 65.6561%, loss per batch: 1.0423\n",
      "Epoch 10: SGD lr 0.0039 -> 0.0035\n"
     ]
    }
   ],
   "source": [
    "history, gradient_stats = train(epochs, train_loader, val_loader, model, optimizer, loss_fn, scheduler, outputs_path='../../log/YOLOv8cls-version-4/training/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fa7035e-ebd2-42ab-86e4-9e5e198f6f84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Me\\PJAIT\\Thesis\\Code\\yolov2-to-yolov8\\Backbone experiments\\YOLOv8s-cls Path\\Version 4\\../../..\\train.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(os.path.join(outputs_path, f\"state.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-08 20:45:06.088592 Epoch 11 \n",
      "2024-12-08 20:45:21.565594 Batch 15 \n",
      "2024-12-08 20:45:24.235178 Batch 30 \n",
      "2024-12-08 20:45:26.630657 Batch 45 \n",
      "2024-12-08 20:45:29.131698 Batch 60 \n",
      "2024-12-08 20:45:32.084178 Batch 75 \n",
      "2024-12-08 20:45:34.868948 Batch 90 \n",
      "2024-12-08 20:45:37.636044 Batch 105 \n",
      "2024-12-08 20:45:40.018844 Batch 120 \n",
      "2024-12-08 20:45:42.519738 Batch 135 \n",
      "[Train] Accuracy: 63.3119%, Loss per batch: 1.0988\n",
      "2024-12-08 20:45:58.098159 Batch 15 \n",
      "2024-12-08 20:45:59.808384 Batch 30 \n",
      "2024-12-08 20:46:02.171153 Batch 45 \n",
      "2024-12-08 20:46:03.867263 Batch 60 \n",
      "[Val] Accuracy: 68.7643%, loss per batch: 0.9659\n",
      "Epoch 11: SGD lr 0.0035 -> 0.0031\n",
      "2024-12-08 20:46:04.822173 Epoch 12 \n",
      "2024-12-08 20:46:18.771436 Batch 15 \n",
      "2024-12-08 20:46:21.634684 Batch 30 \n",
      "2024-12-08 20:46:24.489995 Batch 45 \n",
      "2024-12-08 20:46:27.291009 Batch 60 \n",
      "2024-12-08 20:46:30.199909 Batch 75 \n",
      "2024-12-08 20:46:32.879122 Batch 90 \n",
      "2024-12-08 20:46:35.404403 Batch 105 \n",
      "2024-12-08 20:46:38.376872 Batch 120 \n",
      "2024-12-08 20:46:41.092813 Batch 135 \n",
      "[Train] Accuracy: 64.3996%, Loss per batch: 1.0653\n",
      "2024-12-08 20:46:57.412902 Batch 15 \n",
      "2024-12-08 20:46:59.323858 Batch 30 \n",
      "2024-12-08 20:47:01.467522 Batch 45 \n",
      "2024-12-08 20:47:03.268152 Batch 60 \n",
      "[Val] Accuracy: 69.7834%, loss per batch: 0.947\n",
      "Epoch 12: SGD lr 0.0031 -> 0.0028\n",
      "2024-12-08 20:47:04.252058 Epoch 13 \n",
      "2024-12-08 20:47:18.300805 Batch 15 \n",
      "2024-12-08 20:47:21.158267 Batch 30 \n",
      "2024-12-08 20:47:23.971243 Batch 45 \n",
      "2024-12-08 20:47:26.508223 Batch 60 \n",
      "2024-12-08 20:47:29.640876 Batch 75 \n",
      "2024-12-08 20:47:32.456103 Batch 90 \n",
      "2024-12-08 20:47:35.066408 Batch 105 \n",
      "2024-12-08 20:47:37.877080 Batch 120 \n",
      "2024-12-08 20:47:40.881346 Batch 135 \n",
      "[Train] Accuracy: 65.8993%, Loss per batch: 1.0296\n",
      "2024-12-08 20:47:57.467773 Batch 15 \n",
      "2024-12-08 20:47:59.383066 Batch 30 \n",
      "2024-12-08 20:48:01.746226 Batch 45 \n",
      "2024-12-08 20:48:03.649719 Batch 60 \n",
      "[Val] Accuracy: 67.0318%, loss per batch: 0.9815\n",
      "Epoch 13: SGD lr 0.0028 -> 0.0025\n",
      "2024-12-08 20:48:04.776671 Epoch 14 \n",
      "2024-12-08 20:48:20.830371 Batch 15 \n",
      "2024-12-08 20:48:23.789859 Batch 30 \n",
      "2024-12-08 20:48:26.748664 Batch 45 \n",
      "2024-12-08 20:48:30.090266 Batch 60 \n",
      "2024-12-08 20:48:32.961589 Batch 75 \n",
      "2024-12-08 20:48:35.303008 Batch 90 \n",
      "2024-12-08 20:48:37.964013 Batch 105 \n",
      "2024-12-08 20:48:40.681660 Batch 120 \n",
      "2024-12-08 20:48:43.529755 Batch 135 \n",
      "[Train] Accuracy: 67.4939%, Loss per batch: 0.9961\n",
      "2024-12-08 20:48:58.843260 Batch 15 \n",
      "2024-12-08 20:49:00.531792 Batch 30 \n",
      "2024-12-08 20:49:02.613599 Batch 45 \n",
      "2024-12-08 20:49:04.326009 Batch 60 \n",
      "[Val] Accuracy: 71.2102%, loss per batch: 0.8892\n",
      "Epoch 14: SGD lr 0.0025 -> 0.0023\n",
      "2024-12-08 20:49:05.230487 Epoch 15 \n",
      "2024-12-08 20:49:18.742332 Batch 15 \n",
      "2024-12-08 20:49:21.330318 Batch 30 \n",
      "2024-12-08 20:49:24.025893 Batch 45 \n",
      "2024-12-08 20:49:26.316578 Batch 60 \n",
      "2024-12-08 20:49:29.023690 Batch 75 \n",
      "2024-12-08 20:49:31.873686 Batch 90 \n",
      "2024-12-08 20:49:34.482457 Batch 105 \n",
      "2024-12-08 20:49:36.894043 Batch 120 \n",
      "2024-12-08 20:49:39.691357 Batch 135 \n",
      "[Train] Accuracy: 68.4022%, Loss per batch: 0.9624\n",
      "2024-12-08 20:49:55.299295 Batch 15 \n",
      "2024-12-08 20:49:56.962388 Batch 30 \n",
      "2024-12-08 20:49:59.021391 Batch 45 \n",
      "2024-12-08 20:50:00.633769 Batch 60 \n",
      "[Val] Accuracy: 72.7134%, loss per batch: 0.8496\n",
      "Epoch 15: SGD lr 0.0023 -> 0.0021\n",
      "2024-12-08 20:50:01.587029 Epoch 16 \n",
      "2024-12-08 20:50:15.390005 Batch 15 \n",
      "2024-12-08 20:50:17.930930 Batch 30 \n",
      "2024-12-08 20:50:20.618037 Batch 45 \n",
      "2024-12-08 20:50:23.053792 Batch 60 \n",
      "2024-12-08 20:50:25.581640 Batch 75 \n",
      "2024-12-08 20:50:28.206762 Batch 90 \n",
      "2024-12-08 20:50:30.817333 Batch 105 \n",
      "2024-12-08 20:50:33.224533 Batch 120 \n",
      "2024-12-08 20:50:35.722961 Batch 135 \n",
      "[Train] Accuracy: 68.5394%, Loss per batch: 0.9494\n",
      "2024-12-08 20:50:51.166014 Batch 15 \n",
      "2024-12-08 20:50:52.784903 Batch 30 \n",
      "2024-12-08 20:50:54.722922 Batch 45 \n",
      "2024-12-08 20:50:56.386353 Batch 60 \n",
      "[Val] Accuracy: 72.3312%, loss per batch: 0.8521\n",
      "Epoch 16: SGD lr 0.0021 -> 0.0019\n",
      "2024-12-08 20:50:57.268334 Epoch 17 \n",
      "2024-12-08 20:51:10.537421 Batch 15 \n",
      "2024-12-08 20:51:13.405425 Batch 30 \n",
      "2024-12-08 20:51:15.880592 Batch 45 \n",
      "2024-12-08 20:51:18.330410 Batch 60 \n",
      "2024-12-08 20:51:20.899998 Batch 75 \n",
      "2024-12-08 20:51:23.418219 Batch 90 \n",
      "2024-12-08 20:51:26.042558 Batch 105 \n",
      "2024-12-08 20:51:28.527988 Batch 120 \n",
      "2024-12-08 20:51:31.185315 Batch 135 \n",
      "[Train] Accuracy: 68.814%, Loss per batch: 0.9319\n",
      "2024-12-08 20:51:46.716325 Batch 15 \n",
      "2024-12-08 20:51:48.390680 Batch 30 \n",
      "2024-12-08 20:51:50.314565 Batch 45 \n",
      "2024-12-08 20:51:51.912274 Batch 60 \n",
      "[Val] Accuracy: 71.0828%, loss per batch: 0.859\n",
      "Epoch 17: SGD lr 0.0019 -> 0.0017\n",
      "2024-12-08 20:51:52.840271 Epoch 18 \n",
      "2024-12-08 20:52:06.322422 Batch 15 \n",
      "2024-12-08 20:52:08.751635 Batch 30 \n",
      "2024-12-08 20:52:11.008219 Batch 45 \n",
      "2024-12-08 20:52:13.501200 Batch 60 \n",
      "2024-12-08 20:52:15.976744 Batch 75 \n",
      "2024-12-08 20:52:18.400682 Batch 90 \n",
      "2024-12-08 20:52:21.072714 Batch 105 \n",
      "2024-12-08 20:52:23.713432 Batch 120 \n",
      "2024-12-08 20:52:26.117500 Batch 135 \n",
      "[Train] Accuracy: 69.9018%, Loss per batch: 0.9164\n",
      "2024-12-08 20:52:41.308300 Batch 15 \n",
      "2024-12-08 20:52:43.046993 Batch 30 \n",
      "2024-12-08 20:52:45.167112 Batch 45 \n",
      "2024-12-08 20:52:46.724819 Batch 60 \n",
      "[Val] Accuracy: 74.1656%, loss per batch: 0.7954\n",
      "Epoch 18: SGD lr 0.0017 -> 0.0015\n",
      "2024-12-08 20:52:47.649612 Epoch 19 \n",
      "2024-12-08 20:53:01.203019 Batch 15 \n",
      "2024-12-08 20:53:03.808632 Batch 30 \n",
      "2024-12-08 20:53:06.230060 Batch 45 \n",
      "2024-12-08 20:53:08.449681 Batch 60 \n",
      "2024-12-08 20:53:11.210751 Batch 75 \n",
      "2024-12-08 20:53:13.694613 Batch 90 \n",
      "2024-12-08 20:53:16.143360 Batch 105 \n",
      "2024-12-08 20:53:18.588789 Batch 120 \n",
      "2024-12-08 20:53:21.174423 Batch 135 \n",
      "[Train] Accuracy: 70.3665%, Loss per batch: 0.9025\n",
      "2024-12-08 20:53:36.486092 Batch 15 \n",
      "2024-12-08 20:53:38.190229 Batch 30 \n",
      "2024-12-08 20:53:40.192022 Batch 45 \n",
      "2024-12-08 20:53:41.799967 Batch 60 \n",
      "[Val] Accuracy: 74.9299%, loss per batch: 0.7702\n",
      "Epoch 19: SGD lr 0.0015 -> 0.0014\n",
      "2024-12-08 20:53:42.676087 Epoch 20 \n",
      "2024-12-08 20:53:55.812332 Batch 15 \n",
      "2024-12-08 20:53:58.535338 Batch 30 \n",
      "2024-12-08 20:54:00.996629 Batch 45 \n",
      "2024-12-08 20:54:03.438008 Batch 60 \n",
      "2024-12-08 20:54:06.048273 Batch 75 \n",
      "2024-12-08 20:54:08.352872 Batch 90 \n",
      "2024-12-08 20:54:11.091859 Batch 105 \n",
      "2024-12-08 20:54:13.629491 Batch 120 \n",
      "2024-12-08 20:54:16.391828 Batch 135 \n",
      "[Train] Accuracy: 70.546%, Loss per batch: 0.8902\n",
      "2024-12-08 20:54:31.736557 Batch 15 \n",
      "2024-12-08 20:54:33.456871 Batch 30 \n",
      "2024-12-08 20:54:35.391838 Batch 45 \n",
      "2024-12-08 20:54:37.059084 Batch 60 \n",
      "[Val] Accuracy: 74.4459%, loss per batch: 0.7927\n",
      "Epoch 20: SGD lr 0.0014 -> 0.0012\n"
     ]
    }
   ],
   "source": [
    "history, gradient_stats = train(epochs, train_loader, val_loader, model, optimizer, \n",
    "                                loss_fn, scheduler, outputs_path='../../log/YOLOv8cls-version-4/training/', resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f46c65-da36-4b5e-b16a-2c2cea786863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "New Python (GPU)",
   "language": "python",
   "name": "new_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
