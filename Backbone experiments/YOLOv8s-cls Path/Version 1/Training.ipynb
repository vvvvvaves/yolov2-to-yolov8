{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04833633-3c8f-4d84-8869-c36d9a0cd69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a0a4b-7479-40b1-abdd-1227b6940040",
   "metadata": {},
   "source": [
    "https://lernapparat.de/debug-device-assert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5258e5c-2fd1-45d6-bf25-96674d504f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9deb7c5-1256-4fdd-b447-69f25786c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43b63379-9363-448e-b927-2231dbb097b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import *\n",
    "from data_preprocessing import *\n",
    "from data_augmentation import *\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from Models.yolov8cls_path import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbdae16b-4251-46a6-865b-bfa81a9a5ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9a44f5d-8396-4654-b118-82473d81ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_classes=10, \n",
    "              residual_connection=False, \n",
    "              CSP=False, \n",
    "              add_hidden=False,\n",
    "              classifyV8=False,\n",
    "              bottleneck=1.0, \n",
    "              variant='s', \n",
    "              device=device, \n",
    "              dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae3c5dd0-ac5d-4cbb-8463-d9ebe0621f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../../../datasets/imagenette2/'\n",
    "norms_path = os.path.join(data_path, 'norms.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e608e517-6da7-41c7-b62d-81cca6aecf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means are: [0.44969913363456726, 0.44868946075439453, 0.45163223147392273]\n",
      "stds are: [0.28648287057876587, 0.28796446323394775, 0.2865694761276245]\n"
     ]
    }
   ],
   "source": [
    "means = get_means(path=norms_path, train_loader=None)\n",
    "stds = get_stds(path=norms_path, train_loader=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c4a012-f83a-495c-929b-939db6549642",
   "metadata": {},
   "source": [
    "\n",
    "Profiling your personal module \n",
    "https://pytorch.org/tutorials/beginner/profiler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb61344e-ef51-4068-9304-62ac2f686ae7",
   "metadata": {},
   "source": [
    "https://discuss.pytorch.org/t/how-to-prevent-overfitting/1902\n",
    "Right now, with my augmented dataset, at epoch 8, I am getting a testset Top1 accuracy of 45% but a trainset Top1 accuracy of 69%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4a702f-1ae5-4573-a299-a90db5b87b7f",
   "metadata": {},
   "source": [
    "You should strongly consider data augmentation in some meaningful way. If youâ€™re attempting to do classification then think about what augmentations might add useful information and help distinguish classes in your dataset. In one of my cases, introducing background variation increased recognition rate by over 50%. Basically, with small datasets there is too much overfitting so you want the network to learn real-world distinctions vs. irrelevant artifacts like backgrounds / shadows etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d4f81f6-3dfe-4256-8c57-66f92768ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = transforms.Compose([transforms.RandomResizedCrop((224, 224)),\n",
    "                                              Augmentation(),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize(mean=means, std=stds)])\n",
    "transformations_val = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize(mean=means, std=stds)\n",
    "                                                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f555ed4-dfcf-4f5f-9a71-3e40a7ea2521",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageNetSubset(path=data_path, train=True, transform=transformations, half=False, show=False)\n",
    "val_dataset = ImageNetSubset(path=data_path, train=False, transform=transformations_val, half=False, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ae476e6-947b-46a5-bed5-729af9ee6bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab049c64-2683-432e-acd7-e3494df498f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3846504-a142-444a-85cb-d8ab411cc449",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fb7d049-0696-4341-991c-9b3bfbb0ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daee47e0-df3f-4342-b462-ef2104f8a7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-08 18:38:13.984091 Epoch 1 \n",
      "2024-12-08 18:38:31.409380 Batch 15 \n",
      "2024-12-08 18:38:33.779508 Batch 30 \n",
      "2024-12-08 18:38:36.263418 Batch 45 \n",
      "2024-12-08 18:38:38.974709 Batch 60 \n",
      "2024-12-08 18:38:41.445733 Batch 75 \n",
      "2024-12-08 18:38:44.086240 Batch 90 \n",
      "2024-12-08 18:38:47.385274 Batch 105 \n",
      "2024-12-08 18:38:50.267189 Batch 120 \n",
      "2024-12-08 18:38:53.175568 Batch 135 \n",
      "[Train] Accuracy: 16.7494%, Loss per batch: 2.2321\n",
      "2024-12-08 18:39:10.443350 Batch 15 \n",
      "2024-12-08 18:39:12.373760 Batch 30 \n",
      "2024-12-08 18:39:14.468293 Batch 45 \n",
      "2024-12-08 18:39:16.240795 Batch 60 \n",
      "[Val] Accuracy: 21.0955%, loss per batch: 2.204\n",
      "Epoch 1: SGD lr 0.0100 -> 0.0090\n",
      "2024-12-08 18:39:17.451157 Epoch 2 \n",
      "2024-12-08 18:39:31.262484 Batch 15 \n",
      "2024-12-08 18:39:34.131033 Batch 30 \n",
      "2024-12-08 18:39:36.758500 Batch 45 \n",
      "2024-12-08 18:39:39.515135 Batch 60 \n",
      "2024-12-08 18:39:42.466658 Batch 75 \n",
      "2024-12-08 18:39:45.147063 Batch 90 \n",
      "2024-12-08 18:39:48.078739 Batch 105 \n",
      "2024-12-08 18:39:51.141962 Batch 120 \n",
      "2024-12-08 18:39:54.626742 Batch 135 \n",
      "[Train] Accuracy: 23.9624%, Loss per batch: 2.1034\n",
      "2024-12-08 18:40:11.647161 Batch 15 \n",
      "2024-12-08 18:40:13.401143 Batch 30 \n",
      "2024-12-08 18:40:15.496042 Batch 45 \n",
      "2024-12-08 18:40:17.253639 Batch 60 \n",
      "[Val] Accuracy: 24.9427%, loss per batch: 2.1702\n",
      "Epoch 2: SGD lr 0.0090 -> 0.0081\n",
      "2024-12-08 18:40:18.325362 Epoch 3 \n",
      "2024-12-08 18:40:32.049080 Batch 15 \n",
      "2024-12-08 18:40:34.976584 Batch 30 \n",
      "2024-12-08 18:40:37.615907 Batch 45 \n",
      "2024-12-08 18:40:40.523280 Batch 60 \n",
      "2024-12-08 18:40:43.668910 Batch 75 \n",
      "2024-12-08 18:40:46.353673 Batch 90 \n",
      "2024-12-08 18:40:49.363391 Batch 105 \n",
      "2024-12-08 18:40:51.856218 Batch 120 \n",
      "2024-12-08 18:40:54.844332 Batch 135 \n",
      "[Train] Accuracy: 27.9966%, Loss per batch: 2.0169\n",
      "2024-12-08 18:41:11.988603 Batch 15 \n",
      "2024-12-08 18:41:13.894024 Batch 30 \n",
      "2024-12-08 18:41:16.260496 Batch 45 \n",
      "2024-12-08 18:41:18.055375 Batch 60 \n",
      "[Val] Accuracy: 31.8217%, loss per batch: 1.9284\n",
      "Epoch 3: SGD lr 0.0081 -> 0.0073\n",
      "2024-12-08 18:41:19.114563 Epoch 4 \n",
      "2024-12-08 18:41:33.820359 Batch 15 \n",
      "2024-12-08 18:41:36.860676 Batch 30 \n",
      "2024-12-08 18:41:39.956400 Batch 45 \n",
      "2024-12-08 18:41:42.747812 Batch 60 \n",
      "2024-12-08 18:41:45.798296 Batch 75 \n",
      "2024-12-08 18:41:49.004101 Batch 90 \n",
      "2024-12-08 18:41:52.080587 Batch 105 \n",
      "2024-12-08 18:41:55.125652 Batch 120 \n",
      "2024-12-08 18:41:57.794375 Batch 135 \n",
      "[Train] Accuracy: 30.3728%, Loss per batch: 1.9617\n",
      "2024-12-08 18:42:14.602501 Batch 15 \n",
      "2024-12-08 18:42:16.465557 Batch 30 \n",
      "2024-12-08 18:42:18.621305 Batch 45 \n",
      "2024-12-08 18:42:20.522290 Batch 60 \n",
      "[Val] Accuracy: 37.9873%, loss per batch: 1.8127\n",
      "Epoch 4: SGD lr 0.0073 -> 0.0066\n",
      "2024-12-08 18:42:21.642111 Epoch 5 \n",
      "2024-12-08 18:42:35.481917 Batch 15 \n",
      "2024-12-08 18:42:38.611180 Batch 30 \n",
      "2024-12-08 18:42:41.423871 Batch 45 \n",
      "2024-12-08 18:42:44.057913 Batch 60 \n",
      "2024-12-08 18:42:47.203615 Batch 75 \n",
      "2024-12-08 18:42:50.149657 Batch 90 \n",
      "2024-12-08 18:42:53.051829 Batch 105 \n",
      "2024-12-08 18:42:55.789764 Batch 120 \n",
      "2024-12-08 18:42:58.680766 Batch 135 \n",
      "[Train] Accuracy: 33.4037%, Loss per batch: 1.8948\n",
      "2024-12-08 18:43:16.169113 Batch 15 \n",
      "2024-12-08 18:43:18.497157 Batch 30 \n",
      "2024-12-08 18:43:21.032084 Batch 45 \n",
      "2024-12-08 18:43:22.876013 Batch 60 \n",
      "[Val] Accuracy: 37.4013%, loss per batch: 1.7976\n",
      "Epoch 5: SGD lr 0.0066 -> 0.0059\n",
      "2024-12-08 18:43:23.962442 Epoch 6 \n",
      "2024-12-08 18:43:39.043900 Batch 15 \n",
      "2024-12-08 18:43:42.265143 Batch 30 \n",
      "2024-12-08 18:43:45.131356 Batch 45 \n",
      "2024-12-08 18:43:48.113247 Batch 60 \n",
      "2024-12-08 18:43:51.374978 Batch 75 \n",
      "2024-12-08 18:43:54.307905 Batch 90 \n",
      "2024-12-08 18:43:57.153369 Batch 105 \n",
      "2024-12-08 18:44:00.224207 Batch 120 \n",
      "2024-12-08 18:44:03.256714 Batch 135 \n",
      "[Train] Accuracy: 37.0789%, Loss per batch: 1.801\n",
      "2024-12-08 18:44:20.737258 Batch 15 \n",
      "2024-12-08 18:44:22.485106 Batch 30 \n",
      "2024-12-08 18:44:24.673676 Batch 45 \n",
      "2024-12-08 18:44:26.616521 Batch 60 \n",
      "[Val] Accuracy: 39.3631%, loss per batch: 1.7755\n",
      "Epoch 6: SGD lr 0.0059 -> 0.0053\n",
      "2024-12-08 18:44:27.576872 Epoch 7 \n",
      "2024-12-08 18:44:41.193172 Batch 15 \n",
      "2024-12-08 18:44:43.706088 Batch 30 \n",
      "2024-12-08 18:44:46.296943 Batch 45 \n",
      "2024-12-08 18:44:49.013978 Batch 60 \n",
      "2024-12-08 18:44:52.027358 Batch 75 \n",
      "2024-12-08 18:44:54.753093 Batch 90 \n",
      "2024-12-08 18:44:57.743533 Batch 105 \n",
      "2024-12-08 18:45:00.557258 Batch 120 \n",
      "2024-12-08 18:45:03.948901 Batch 135 \n",
      "[Train] Accuracy: 40.1204%, Loss per batch: 1.6965\n",
      "2024-12-08 18:45:22.191884 Batch 15 \n",
      "2024-12-08 18:45:24.052992 Batch 30 \n",
      "2024-12-08 18:45:26.198108 Batch 45 \n",
      "2024-12-08 18:45:27.911250 Batch 60 \n",
      "[Val] Accuracy: 40.4331%, loss per batch: 1.7513\n",
      "Epoch 7: SGD lr 0.0053 -> 0.0048\n",
      "2024-12-08 18:45:28.934229 Epoch 8 \n",
      "2024-12-08 18:45:43.425580 Batch 15 \n",
      "2024-12-08 18:45:46.091774 Batch 30 \n",
      "2024-12-08 18:45:48.874053 Batch 45 \n",
      "2024-12-08 18:45:51.885285 Batch 60 \n",
      "2024-12-08 18:45:54.496296 Batch 75 \n",
      "2024-12-08 18:45:57.616663 Batch 90 \n",
      "2024-12-08 18:46:00.677891 Batch 105 \n",
      "2024-12-08 18:46:03.916694 Batch 120 \n",
      "2024-12-08 18:46:07.036665 Batch 135 \n",
      "[Train] Accuracy: 42.9401%, Loss per batch: 1.6338\n",
      "2024-12-08 18:46:25.271749 Batch 15 \n",
      "2024-12-08 18:46:27.187395 Batch 30 \n",
      "2024-12-08 18:46:29.489407 Batch 45 \n",
      "2024-12-08 18:46:31.384844 Batch 60 \n",
      "[Val] Accuracy: 48.586%, loss per batch: 1.5243\n",
      "Epoch 8: SGD lr 0.0048 -> 0.0043\n",
      "2024-12-08 18:46:32.497868 Epoch 9 \n",
      "2024-12-08 18:46:47.797300 Batch 15 \n",
      "2024-12-08 18:46:50.625518 Batch 30 \n",
      "2024-12-08 18:46:53.476325 Batch 45 \n",
      "2024-12-08 18:46:56.686345 Batch 60 \n",
      "2024-12-08 18:47:00.030090 Batch 75 \n",
      "2024-12-08 18:47:02.962699 Batch 90 \n",
      "2024-12-08 18:47:05.825482 Batch 105 \n",
      "2024-12-08 18:47:08.665911 Batch 120 \n",
      "2024-12-08 18:47:11.619292 Batch 135 \n",
      "[Train] Accuracy: 45.1051%, Loss per batch: 1.5741\n",
      "2024-12-08 18:47:30.393555 Batch 15 \n",
      "2024-12-08 18:47:32.224650 Batch 30 \n",
      "2024-12-08 18:47:34.409210 Batch 45 \n",
      "2024-12-08 18:47:36.223809 Batch 60 \n",
      "[Val] Accuracy: 46.9045%, loss per batch: 1.5619\n",
      "Epoch 9: SGD lr 0.0043 -> 0.0039\n",
      "2024-12-08 18:47:37.285366 Epoch 10 \n",
      "2024-12-08 18:47:51.149267 Batch 15 \n",
      "2024-12-08 18:47:54.446605 Batch 30 \n",
      "2024-12-08 18:47:57.860535 Batch 45 \n",
      "2024-12-08 18:48:01.296320 Batch 60 \n",
      "2024-12-08 18:48:04.560217 Batch 75 \n",
      "2024-12-08 18:48:07.460660 Batch 90 \n",
      "2024-12-08 18:48:10.227864 Batch 105 \n",
      "2024-12-08 18:48:13.058552 Batch 120 \n",
      "2024-12-08 18:48:15.864510 Batch 135 \n",
      "[Train] Accuracy: 48.0832%, Loss per batch: 1.5084\n",
      "2024-12-08 18:48:32.268745 Batch 15 \n",
      "2024-12-08 18:48:33.954937 Batch 30 \n",
      "2024-12-08 18:48:36.038187 Batch 45 \n",
      "2024-12-08 18:48:37.836821 Batch 60 \n",
      "[Val] Accuracy: 53.5796%, loss per batch: 1.4072\n",
      "Epoch 10: SGD lr 0.0039 -> 0.0035\n"
     ]
    }
   ],
   "source": [
    "history, gradient_stats = train(epochs, train_loader, val_loader, model, optimizer, loss_fn, scheduler, outputs_path='../../log/YOLOv8cls-version-1/training/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fa7035e-ebd2-42ab-86e4-9e5e198f6f84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Me\\PJAIT\\Thesis\\Code\\yolov2-to-yolov8\\Backbone experiments\\YOLOv8s-cls Path\\Version 1\\../../..\\train.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(os.path.join(outputs_path, f\"state.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-08 18:48:39.041176 Epoch 11 \n",
      "2024-12-08 18:48:53.024690 Batch 15 \n",
      "2024-12-08 18:48:55.700808 Batch 30 \n",
      "2024-12-08 18:48:58.419483 Batch 45 \n",
      "2024-12-08 18:49:01.140800 Batch 60 \n",
      "2024-12-08 18:49:03.872071 Batch 75 \n",
      "2024-12-08 18:49:06.635061 Batch 90 \n",
      "2024-12-08 18:49:09.408585 Batch 105 \n",
      "2024-12-08 18:49:12.193805 Batch 120 \n",
      "2024-12-08 18:49:15.002414 Batch 135 \n",
      "[Train] Accuracy: 49.002%, Loss per batch: 1.486\n",
      "2024-12-08 18:49:31.133624 Batch 15 \n",
      "2024-12-08 18:49:32.796981 Batch 30 \n",
      "2024-12-08 18:49:34.786163 Batch 45 \n",
      "2024-12-08 18:49:36.584807 Batch 60 \n",
      "[Val] Accuracy: 51.2866%, loss per batch: 1.443\n",
      "Epoch 11: SGD lr 0.0035 -> 0.0031\n",
      "2024-12-08 18:49:37.567437 Epoch 12 \n",
      "2024-12-08 18:49:51.592443 Batch 15 \n",
      "2024-12-08 18:49:54.313291 Batch 30 \n",
      "2024-12-08 18:49:57.065709 Batch 45 \n",
      "2024-12-08 18:49:59.800605 Batch 60 \n",
      "2024-12-08 18:50:02.506326 Batch 75 \n",
      "2024-12-08 18:50:05.339605 Batch 90 \n",
      "2024-12-08 18:50:08.081546 Batch 105 \n",
      "2024-12-08 18:50:10.855335 Batch 120 \n",
      "2024-12-08 18:50:13.818082 Batch 135 \n",
      "[Train] Accuracy: 50.7657%, Loss per batch: 1.4296\n",
      "2024-12-08 18:50:30.163862 Batch 15 \n",
      "2024-12-08 18:50:31.901762 Batch 30 \n",
      "2024-12-08 18:50:34.115431 Batch 45 \n",
      "2024-12-08 18:50:35.891631 Batch 60 \n",
      "[Val] Accuracy: 56.2293%, loss per batch: 1.3334\n",
      "Epoch 12: SGD lr 0.0031 -> 0.0028\n",
      "2024-12-08 18:50:36.877008 Epoch 13 \n",
      "2024-12-08 18:50:50.755716 Batch 15 \n",
      "2024-12-08 18:50:53.500708 Batch 30 \n",
      "2024-12-08 18:50:56.401354 Batch 45 \n",
      "2024-12-08 18:50:59.213368 Batch 60 \n",
      "2024-12-08 18:51:02.090713 Batch 75 \n",
      "2024-12-08 18:51:04.894221 Batch 90 \n",
      "2024-12-08 18:51:07.822875 Batch 105 \n",
      "2024-12-08 18:51:10.583363 Batch 120 \n",
      "2024-12-08 18:51:13.790174 Batch 135 \n",
      "[Train] Accuracy: 53.3636%, Loss per batch: 1.3735\n",
      "2024-12-08 18:51:30.098398 Batch 15 \n",
      "2024-12-08 18:51:31.827610 Batch 30 \n",
      "2024-12-08 18:51:33.856034 Batch 45 \n",
      "2024-12-08 18:51:35.624445 Batch 60 \n",
      "[Val] Accuracy: 58.8025%, loss per batch: 1.2107\n",
      "Epoch 13: SGD lr 0.0028 -> 0.0025\n",
      "2024-12-08 18:51:36.617835 Epoch 14 \n",
      "2024-12-08 18:51:50.814351 Batch 15 \n",
      "2024-12-08 18:51:53.484741 Batch 30 \n",
      "2024-12-08 18:51:56.166546 Batch 45 \n",
      "2024-12-08 18:51:58.937284 Batch 60 \n",
      "2024-12-08 18:52:01.674001 Batch 75 \n",
      "2024-12-08 18:52:04.672661 Batch 90 \n",
      "2024-12-08 18:52:07.497212 Batch 105 \n",
      "2024-12-08 18:52:10.352591 Batch 120 \n",
      "2024-12-08 18:52:13.120035 Batch 135 \n",
      "[Train] Accuracy: 55.254%, Loss per batch: 1.3245\n",
      "2024-12-08 18:52:29.291405 Batch 15 \n",
      "2024-12-08 18:52:30.984282 Batch 30 \n",
      "2024-12-08 18:52:33.213186 Batch 45 \n",
      "2024-12-08 18:52:34.868872 Batch 60 \n",
      "[Val] Accuracy: 60.0%, loss per batch: 1.2188\n",
      "Epoch 14: SGD lr 0.0025 -> 0.0023\n",
      "2024-12-08 18:52:35.868304 Epoch 15 \n",
      "2024-12-08 18:52:49.731562 Batch 15 \n",
      "2024-12-08 18:52:52.714483 Batch 30 \n",
      "2024-12-08 18:52:55.520933 Batch 45 \n",
      "2024-12-08 18:52:58.225868 Batch 60 \n",
      "2024-12-08 18:53:00.923587 Batch 75 \n",
      "2024-12-08 18:53:03.669454 Batch 90 \n",
      "2024-12-08 18:53:06.419111 Batch 105 \n",
      "2024-12-08 18:53:09.178797 Batch 120 \n",
      "2024-12-08 18:53:11.942137 Batch 135 \n",
      "[Train] Accuracy: 56.0883%, Loss per batch: 1.3064\n",
      "2024-12-08 18:53:28.297903 Batch 15 \n",
      "2024-12-08 18:53:30.088843 Batch 30 \n",
      "2024-12-08 18:53:32.164603 Batch 45 \n",
      "2024-12-08 18:53:33.903258 Batch 60 \n",
      "[Val] Accuracy: 61.0191%, loss per batch: 1.174\n",
      "Epoch 15: SGD lr 0.0023 -> 0.0021\n",
      "2024-12-08 18:53:34.905270 Epoch 16 \n",
      "2024-12-08 18:53:49.107135 Batch 15 \n",
      "2024-12-08 18:53:51.796439 Batch 30 \n",
      "2024-12-08 18:53:54.476145 Batch 45 \n",
      "2024-12-08 18:53:57.365122 Batch 60 \n",
      "2024-12-08 18:54:00.084473 Batch 75 \n",
      "2024-12-08 18:54:02.812377 Batch 90 \n",
      "2024-12-08 18:54:05.563955 Batch 105 \n",
      "2024-12-08 18:54:08.338698 Batch 120 \n",
      "2024-12-08 18:54:11.103298 Batch 135 \n",
      "[Train] Accuracy: 57.3767%, Loss per batch: 1.265\n",
      "2024-12-08 18:54:27.133829 Batch 15 \n",
      "2024-12-08 18:54:28.779575 Batch 30 \n",
      "2024-12-08 18:54:30.911668 Batch 45 \n",
      "2024-12-08 18:54:32.683563 Batch 60 \n",
      "[Val] Accuracy: 62.7261%, loss per batch: 1.1181\n",
      "Epoch 16: SGD lr 0.0021 -> 0.0019\n",
      "2024-12-08 18:54:33.714447 Epoch 17 \n",
      "2024-12-08 18:54:47.839773 Batch 15 \n",
      "2024-12-08 18:54:50.528477 Batch 30 \n",
      "2024-12-08 18:54:53.302543 Batch 45 \n",
      "2024-12-08 18:54:56.015789 Batch 60 \n",
      "2024-12-08 18:54:58.711421 Batch 75 \n",
      "2024-12-08 18:55:01.442886 Batch 90 \n",
      "2024-12-08 18:55:04.183266 Batch 105 \n",
      "2024-12-08 18:55:06.931169 Batch 120 \n",
      "2024-12-08 18:55:09.694415 Batch 135 \n",
      "[Train] Accuracy: 58.644%, Loss per batch: 1.2324\n",
      "2024-12-08 18:55:25.604490 Batch 15 \n",
      "2024-12-08 18:55:27.315061 Batch 30 \n",
      "2024-12-08 18:55:29.544897 Batch 45 \n",
      "2024-12-08 18:55:31.444740 Batch 60 \n",
      "[Val] Accuracy: 61.707%, loss per batch: 1.1374\n",
      "Epoch 17: SGD lr 0.0019 -> 0.0017\n",
      "2024-12-08 18:55:32.385494 Epoch 18 \n",
      "2024-12-08 18:55:46.055379 Batch 15 \n",
      "2024-12-08 18:55:48.806036 Batch 30 \n",
      "2024-12-08 18:55:51.483430 Batch 45 \n",
      "2024-12-08 18:55:54.215093 Batch 60 \n",
      "2024-12-08 18:55:56.925208 Batch 75 \n",
      "2024-12-08 18:55:59.633275 Batch 90 \n",
      "2024-12-08 18:56:02.694923 Batch 105 \n",
      "2024-12-08 18:56:05.457793 Batch 120 \n",
      "2024-12-08 18:56:08.256174 Batch 135 \n",
      "[Train] Accuracy: 59.1298%, Loss per batch: 1.2049\n",
      "2024-12-08 18:56:24.114113 Batch 15 \n",
      "2024-12-08 18:56:25.878447 Batch 30 \n",
      "2024-12-08 18:56:27.846187 Batch 45 \n",
      "2024-12-08 18:56:29.567172 Batch 60 \n",
      "[Val] Accuracy: 65.3503%, loss per batch: 1.0668\n",
      "Epoch 18: SGD lr 0.0017 -> 0.0015\n",
      "2024-12-08 18:56:30.493069 Epoch 19 \n",
      "2024-12-08 18:56:44.430401 Batch 15 \n",
      "2024-12-08 18:56:47.120194 Batch 30 \n",
      "2024-12-08 18:56:49.794922 Batch 45 \n",
      "2024-12-08 18:56:52.492336 Batch 60 \n",
      "2024-12-08 18:56:55.200302 Batch 75 \n",
      "2024-12-08 18:56:57.931400 Batch 90 \n",
      "2024-12-08 18:57:00.727215 Batch 105 \n",
      "2024-12-08 18:57:03.984969 Batch 120 \n",
      "2024-12-08 18:57:06.811262 Batch 135 \n",
      "[Train] Accuracy: 60.4076%, Loss per batch: 1.1834\n",
      "2024-12-08 18:57:22.870858 Batch 15 \n",
      "2024-12-08 18:57:24.563990 Batch 30 \n",
      "2024-12-08 18:57:26.671427 Batch 45 \n",
      "2024-12-08 18:57:28.342358 Batch 60 \n",
      "[Val] Accuracy: 67.8217%, loss per batch: 1.0032\n",
      "Epoch 19: SGD lr 0.0015 -> 0.0014\n",
      "2024-12-08 18:57:29.314602 Epoch 20 \n",
      "2024-12-08 18:57:43.052213 Batch 15 \n",
      "2024-12-08 18:57:45.714753 Batch 30 \n",
      "2024-12-08 18:57:48.424525 Batch 45 \n",
      "2024-12-08 18:57:51.139113 Batch 60 \n",
      "2024-12-08 18:57:53.892454 Batch 75 \n",
      "2024-12-08 18:57:57.101885 Batch 90 \n",
      "2024-12-08 18:57:59.937468 Batch 105 \n",
      "2024-12-08 18:58:02.690118 Batch 120 \n",
      "2024-12-08 18:58:05.542422 Batch 135 \n",
      "[Train] Accuracy: 61.3159%, Loss per batch: 1.1575\n",
      "2024-12-08 18:58:21.544576 Batch 15 \n",
      "2024-12-08 18:58:23.291336 Batch 30 \n",
      "2024-12-08 18:58:25.464457 Batch 45 \n",
      "2024-12-08 18:58:27.296146 Batch 60 \n",
      "[Val] Accuracy: 66.6752%, loss per batch: 1.0347\n",
      "Epoch 20: SGD lr 0.0014 -> 0.0012\n"
     ]
    }
   ],
   "source": [
    "history, gradient_stats = train(epochs, train_loader, val_loader, model, optimizer, \n",
    "                                loss_fn, scheduler, outputs_path='../../log/YOLOv8cls-version-1/training/', resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a71c4c5-6b0c-41a1-8669-0ae12ee90ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
