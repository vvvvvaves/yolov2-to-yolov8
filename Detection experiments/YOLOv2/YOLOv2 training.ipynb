{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512e4cb2-1f99-40fb-ba8d-f0663195ec90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Vstanovlene\\Anaconda Distribution\\envs\\new_gpu_env\\lib\\site-packages\\albumentations\\__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.23). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"../../\")\n",
    "sys.path.insert(1, \"../../Models/\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from yolov2 import YOLOv2D19 as YOLOv2\n",
    "from detection_datasets import VOCDatasetV2\n",
    "from torch import optim\n",
    "from loss import YOLOv2Loss\n",
    "from train import *\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from data_preprocessing import get_norms\n",
    "import pickle\n",
    "with open('../../Models/anchors_VOC0712trainval.pickle', 'rb') as handle:\n",
    "    anchors = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1532df2e-63db-4870-8e33-66a09981579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "dtype=torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f5be9e7-fd4d-484d-9feb-0d6007fda614",
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = get_norms('../../../datasets/VOCdevkit/trainval_norms.json')\n",
    "means = norms['means']\n",
    "stds = norms['stds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb80c18f-6c05-4344-9d04-c7b347020414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 0.00 MB\n",
      "Allocated memory: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "import gc\n",
    "\n",
    "# Invoke garbage collector\n",
    "gc.collect()\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4971daae-6ce2-4f48-80a5-1d294e02b528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ../../../datasets/VOCdevkit/VOC2012\\ImageSets\\Main\\trainval.txt\n",
      "True ../../../datasets/VOCdevkit/VOC2007\\ImageSets\\Main\\val.txt\n"
     ]
    }
   ],
   "source": [
    "transforms = A.Compose([\n",
    "    A.Resize(width=416, height=416),\n",
    "    # A.VerticalFlip(p=1.0),\n",
    "    A.Normalize(mean=means, std=stds),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc'))\n",
    "train_set = VOCDatasetV2(devkit_path = '../../../datasets/VOCdevkit/', \n",
    "                         subsets = [('VOC2012', 'trainval')],\n",
    "                         scales=[13], anchors=anchors, transforms=transforms, \n",
    "                         dtype=dtype, device=device)\n",
    "val_set = VOCDatasetV2(devkit_path = '../../../datasets/VOCdevkit/', \n",
    "                       subsets = [('VOC2007', 'val')],\n",
    "                       scales=[13], anchors=anchors, transforms=transforms, \n",
    "                       dtype=dtype, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fd6ac77-14a7-4918-bb26-68303cd9dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Subset(train_set, list(range(0, 100)))\n",
    "val_set = Subset(val_set, list(range(1, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6678c8fe-8e98-47d8-9ec0-879cc36a4571",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8350ff7-f3c8-42cb-bcd7-6f735a8d12fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = YoloLoss(anchors=torch.tensor(anchors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "066fb3de-2511-4b39-bedd-9ca3241ddeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Me\\PJAIT\\Thesis\\Code\\yolov2-to-yolov8\\Detection experiments\\YOLOv2\\../../Models\\yolov2.py:138: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path, map_location=self.device)\n"
     ]
    }
   ],
   "source": [
    "model = YOLOv2(state_dict_path='../../Models/darknet19_72.96.pth', device=device, dtype=dtype, num_anchors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f700978-3cb3-478b-91a0-e910995633ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6002a881-de16-4ba7-8c62-d3b828b36f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5fde5e16-3c79-464a-8e2e-1c5e396000ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.amp.GradScaler(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3515b88c-7625-4444-b0ae-2ac199615fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.32882816561531697, 0.6554346360225188],\n",
       " [0.07522471620262354, 0.1241032298491792],\n",
       " [0.7862514755140577, 0.8175396064741282],\n",
       " [0.32882816561531697, 0.6554346360225188],\n",
       " [0.07522471620262354, 0.1241032298491792],\n",
       " [0.7862514755140577, 0.8175396064741282],\n",
       " [0.32882816561531697, 0.6554346360225188],\n",
       " [0.07522471620262354, 0.1241032298491792],\n",
       " [0.7862514755140577, 0.8175396064741282]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3d09e4c-3d6e-4910-9af1-7fcf3e6f601e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[0.3288, 0.6554]]],\n",
       "\n",
       "\n",
       "         [[[0.0752, 0.1241]]],\n",
       "\n",
       "\n",
       "         [[[0.7863, 0.8175]]]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(anchors).reshape(1, 3, 1, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "072cdb69-8956-45bf-ad03-90ab1fe12f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-23 21:42:15.563512 Epoch 1 \n",
      "scaler used\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history, gradient_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                \u001b[49m\u001b[43msave_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../log/YOLOv2/training/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Me\\PJAIT\\Thesis\\Code\\yolov2-to-yolov8\\Detection experiments\\YOLOv2\\../..\\train.py:42\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epochs, train_loader, val_loader, model, optimizer, loss_fn, scheduler, scaler, outputs_path, save_at, save_grad, resume)\u001b[0m\n\u001b[0;32m     40\u001b[0m _datetime \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_datetime\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m train_map, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m val_map, val_loss \u001b[38;5;241m=\u001b[39m validation_loop(model, val_loader, loss_fn)\n\u001b[0;32m     45\u001b[0m _gradient_stats \u001b[38;5;241m=\u001b[39m get_gradient_stats(model)\n",
      "File \u001b[1;32mC:\\Me\\PJAIT\\Thesis\\Code\\yolov2-to-yolov8\\Detection experiments\\YOLOv2\\../..\\loop.py:27\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(optimizer, model, loss_fn, train_loader, scaler)\u001b[0m\n\u001b[0;32m     23\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     25\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(imgs)\n\u001b[1;32m---> 27\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     31\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n",
      "File \u001b[1;32mC:\\Vstanovlene\\Anaconda Distribution\\envs\\new_gpu_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Vstanovlene\\Anaconda Distribution\\envs\\new_gpu_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mC:\\Me\\PJAIT\\Thesis\\Code\\yolov2-to-yolov8\\Detection experiments\\YOLOv2\\alladins_loss.py:46\u001b[0m, in \u001b[0;36mYoloLoss.forward\u001b[1;34m(self, predictions, target)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# ==================== #\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m#   FOR OBJECT LOSS    #\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# ==================== #\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manchors\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m box_preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(predictions[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m3\u001b[39m]), \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manchors\u001b[49m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     47\u001b[0m ious \u001b[38;5;241m=\u001b[39m intersection_over_union(box_preds[obj], target[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m5\u001b[39m][obj])\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m     48\u001b[0m object_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(predictions[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m][obj]), ious \u001b[38;5;241m*\u001b[39m target[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m][obj])\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "history, gradient_stats = train(epochs, train_loader, val_loader, model, optimizer, loss_fn, scheduler, scaler, \n",
    "                                save_grad=False, outputs_path='../log/YOLOv2/training/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ff112-1059-4f5f-b614-84b9a8db530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history, gradient_stats = train(epochs, train_loader, val_loader, model, optimizer, loss_fn, scheduler, scaler, \n",
    "                                save_grad=False, outputs_path='../log/YOLOv2/training/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ffad1b-94d9-4b1a-a675-3d82df2a6c99",
   "metadata": {},
   "source": [
    "Results when loss_fn is MSE, 13x13x125, full sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd78dbf2-6d1d-4387-a7ad-141364343c3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-23 08:42:34.189888 Epoch 1 \n",
      "2024-12-23 08:42:50.410643 Batch 30 \n",
      "2024-12-23 08:43:07.046975 Batch 60 \n",
      "2024-12-23 08:43:23.138869 Batch 90 \n",
      "2024-12-23 08:43:39.060661 Batch 120 \n",
      "2024-12-23 08:43:55.065185 Batch 150 \n",
      "2024-12-23 08:44:11.294661 Batch 180 \n",
      "2024-12-23 08:44:27.271376 Batch 210 \n",
      "2024-12-23 08:44:43.124076 Batch 240 \n",
      "2024-12-23 08:44:59.231072 Batch 270 \n",
      "2024-12-23 08:45:15.154746 Batch 300 \n",
      "2024-12-23 08:45:31.198184 Batch 330 \n",
      "2024-12-23 08:45:47.260832 Batch 360 \n",
      "[Train] Loss per batch: 0.0364\n",
      "2024-12-23 08:45:58.658664 Batch 30 \n",
      "2024-12-23 08:46:09.537877 Batch 60 \n",
      "[Val] loss per batch: 0.0248\n",
      "Epoch 1: SGD lr 0.0100 -> 0.0100\n",
      "2024-12-23 08:46:16.661041 Epoch 2 \n",
      "2024-12-23 08:46:32.511914 Batch 30 \n",
      "2024-12-23 08:46:48.611562 Batch 60 \n",
      "2024-12-23 08:47:04.673063 Batch 90 \n",
      "2024-12-23 08:47:20.756701 Batch 120 \n",
      "2024-12-23 08:47:37.109335 Batch 150 \n",
      "2024-12-23 08:47:53.084386 Batch 180 \n",
      "2024-12-23 08:48:08.983728 Batch 210 \n",
      "2024-12-23 08:48:24.952682 Batch 240 \n",
      "2024-12-23 08:48:40.876569 Batch 270 \n",
      "2024-12-23 08:48:56.823425 Batch 300 \n",
      "2024-12-23 08:49:12.749448 Batch 330 \n",
      "2024-12-23 08:49:28.909541 Batch 360 \n",
      "[Train] Loss per batch: 0.0247\n",
      "2024-12-23 08:49:40.447538 Batch 30 \n",
      "2024-12-23 08:49:51.458455 Batch 60 \n",
      "[Val] loss per batch: 0.0246\n",
      "Epoch 2: SGD lr 0.0100 -> 0.0100\n",
      "2024-12-23 08:49:58.660747 Epoch 3 \n",
      "2024-12-23 08:50:14.487345 Batch 30 \n",
      "2024-12-23 08:50:30.535232 Batch 60 \n",
      "2024-12-23 08:50:46.873548 Batch 90 \n",
      "2024-12-23 08:51:04.015649 Batch 120 \n",
      "2024-12-23 08:51:21.185457 Batch 150 \n",
      "2024-12-23 08:51:38.569076 Batch 180 \n",
      "2024-12-23 08:51:55.925180 Batch 210 \n",
      "2024-12-23 08:52:13.195040 Batch 240 \n",
      "2024-12-23 08:52:29.921410 Batch 270 \n",
      "2024-12-23 08:52:46.514252 Batch 300 \n",
      "2024-12-23 08:53:02.591057 Batch 330 \n",
      "2024-12-23 08:53:18.498513 Batch 360 \n",
      "[Train] Loss per batch: 0.0246\n",
      "2024-12-23 08:53:29.891897 Batch 30 \n",
      "2024-12-23 08:53:40.672194 Batch 60 \n",
      "[Val] loss per batch: 0.0246\n",
      "Epoch 3: SGD lr 0.0100 -> 0.0100\n",
      "2024-12-23 08:53:47.911134 Epoch 4 \n",
      "2024-12-23 08:54:03.572967 Batch 30 \n",
      "2024-12-23 08:54:19.563259 Batch 60 \n",
      "2024-12-23 08:54:35.628084 Batch 90 \n",
      "2024-12-23 08:54:51.369372 Batch 120 \n",
      "2024-12-23 08:55:07.364246 Batch 150 \n",
      "2024-12-23 08:55:23.638159 Batch 180 \n",
      "2024-12-23 08:55:39.619941 Batch 210 \n",
      "2024-12-23 08:55:55.604741 Batch 240 \n",
      "2024-12-23 08:56:11.482775 Batch 270 \n",
      "2024-12-23 08:56:27.449203 Batch 300 \n",
      "2024-12-23 08:56:43.694803 Batch 330 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history, gradient_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                \u001b[49m\u001b[43msave_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../log/YOLOv2/training/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Me\\PJAIT\\Thesis\\Code\\yolov2-to-yolov8\\Detection experiments\\YOLOv2\\../..\\train.py:42\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epochs, train_loader, val_loader, model, optimizer, loss_fn, scheduler, scaler, outputs_path, save_at, save_grad, resume)\u001b[0m\n\u001b[0;32m     40\u001b[0m _datetime \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_datetime\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m train_map, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m val_map, val_loss \u001b[38;5;241m=\u001b[39m validation_loop(model, val_loader, loss_fn)\n\u001b[0;32m     45\u001b[0m _gradient_stats \u001b[38;5;241m=\u001b[39m get_gradient_stats(model)\n",
      "File \u001b[1;32mC:\\Me\\PJAIT\\Thesis\\Code\\yolov2-to-yolov8\\Detection experiments\\YOLOv2\\../..\\loop.py:12\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(optimizer, model, loss_fn, train_loader, scaler)\u001b[0m\n\u001b[0;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     11\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (imgs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m30\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     14\u001b[0m         _datetime \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[1;32mC:\\Vstanovlene\\Anaconda Distribution\\envs\\new_gpu_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mC:\\Vstanovlene\\Anaconda Distribution\\envs\\new_gpu_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mC:\\Vstanovlene\\Anaconda Distribution\\envs\\new_gpu_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mC:\\Vstanovlene\\Anaconda Distribution\\envs\\new_gpu_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mC:\\Me\\PJAIT\\Thesis\\Code\\yolov2-to-yolov8\\Detection experiments\\YOLOv2\\../../Models\\detection_datasets.py:87\u001b[0m, in \u001b[0;36mVOCDatasetV2.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     84\u001b[0m     bboxes\u001b[38;5;241m.\u001b[39mappend([xmin, ymin, xmax, ymax, class_label])\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 87\u001b[0m     np_img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mPIL_img\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     88\u001b[0m     transformed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms(image\u001b[38;5;241m=\u001b[39mnp_img, bboxes\u001b[38;5;241m=\u001b[39mbboxes)\n\u001b[0;32m     89\u001b[0m     image \u001b[38;5;241m=\u001b[39m transformed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mC:\\Vstanovlene\\Anaconda Distribution\\envs\\new_gpu_env\\lib\\site-packages\\PIL\\Image.py:993\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    991\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m--> 993\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    995\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Vstanovlene\\Anaconda Distribution\\envs\\new_gpu_env\\lib\\site-packages\\PIL\\ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history, gradient_stats = train(epochs, train_loader, val_loader, model, optimizer, loss_fn, scheduler, scaler, \n",
    "                                save_grad=False, outputs_path='../log/YOLOv2/training/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf20a814-c37b-433c-8dc7-b27d43a40b29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "New Python (GPU)",
   "language": "python",
   "name": "new_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
